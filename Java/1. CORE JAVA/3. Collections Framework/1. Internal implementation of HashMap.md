# HashMap Internal Implementation

## Overview

HashMap is one of the most fundamental and frequently used data structures in Java. Understanding its internal implementation is crucial for writing efficient code, debugging performance issues, and succeeding in technical interviews. This guide covers the complete internal mechanics of HashMap from Java 8+.


---

## 1. Array + LinkedList/TreeNode Structure

### Internal Architecture

```java
/**
 * HASHMAP INTERNAL STRUCTURE
 * 
 * Core data structure and components
 */

public class HashMapInternals {
    
    /**
     * Simplified HashMap structure
     */
    static class MyHashMap<K, V> {
        // The table (array of buckets)
        transient Node<K, V>[] table;
        
        // Current number of entries
        transient int size;
        
        // Threshold for resizing
        int threshold;
        
        // Load factor
        final float loadFactor;
        
        // Default values
        static final int DEFAULT_INITIAL_CAPACITY = 16;
        static final float DEFAULT_LOAD_FACTOR = 0.75f;
        static final int TREEIFY_THRESHOLD = 8;
        static final int UNTREEIFY_THRESHOLD = 6;
        static final int MIN_TREEIFY_CAPACITY = 64;
        
        /**
         * ARCHITECTURE:
         * 
         * table: Node<K,V>[]
         * ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐
         * │  0  │  1  │  2  │  3  │  4  │  5  │  6  │  7  │ ... buckets
         * └──┬──┴─────┴──┬──┴─────┴─────┴──┬──┴─────┴─────┘
         *    │           │                 │
         *    v           v                 v
         * Node(k1,v1) Node(k2,v2)      Node(k3,v3)
         *    │                             │
         *    v                             v
         * Node(k4,v4)                  Node(k5,v5)
         * 
         * - Array of "buckets"
         * - Each bucket is head of linked list (or tree)
         * - Multiple keys can hash to same bucket (collision)
         */
        
        /**
         * Node class - linked list node
         */
        static class Node<K, V> {
            final int hash;      // Cached hash code
            final K key;
            V value;
            Node<K, V> next;     // Next node in chain
            
            Node(int hash, K key, V value, Node<K, V> next) {
                this.hash = hash;
                this.key = key;
                this.value = value;
                this.next = next;
            }
            
            public final K getKey()        { return key; }
            public final V getValue()      { return value; }
            public final String toString() { return key + "=" + value; }
            
            public final V setValue(V newValue) {
                V oldValue = value;
                value = newValue;
                return oldValue;
            }
            
            public final boolean equals(Object o) {
                if (o == this)
                    return true;
                if (o instanceof Node) {
                    Node<?,?> e = (Node<?,?>)o;
                    return Objects.equals(key, e.key) &&
                           Objects.equals(value, e.value);
                }
                return false;
            }
            
            public final int hashCode() {
                return Objects.hashCode(key) ^ Objects.hashCode(value);
            }
        }
        
        /**
         * TreeNode class - red-black tree node (Java 8+)
         */
        static class TreeNode<K, V> extends Node<K, V> {
            TreeNode<K, V> parent;
            TreeNode<K, V> left;
            TreeNode<K, V> right;
            TreeNode<K, V> prev;  // Needed for unlinking
            boolean red;
            
            TreeNode(int hash, K key, V val, Node<K, V> next) {
                super(hash, key, val, next);
            }
            
            /**
             * TREE STRUCTURE (when bucket has 8+ elements):
             * 
             *           Node(k1)
             *          /        \
             *     Node(k2)    Node(k3)
             *       /            \
             *  Node(k4)       Node(k5)
             * 
             * - Red-black tree for balance
             * - Reduces worst-case lookup from O(n) to O(log n)
             * - Only when bucket has 8+ elements AND table size >= 64
             */
        }
    }
    
    /**
     * Visual representation
     */
    public static void visualRepresentation() {
        /**
         * EXAMPLE: HashMap with capacity 16
         * 
         * After inserting keys that hash to buckets 0, 2, and 7:
         * 
         * Index    Bucket
         * ┌───┐
         * │ 0 │ → Node("apple", 1) → Node("apricot", 2) → null
         * ├───┤
         * │ 1 │ → null
         * ├───┤
         * │ 2 │ → Node("banana", 3) → null
         * ├───┤
         * │ 3 │ → null
         * ├───┤
         * │ 4 │ → null
         * ├───┤
         * │ 5 │ → null
         * ├───┤
         * │ 6 │ → null
         * ├───┤
         * │ 7 │ → Node("cherry", 4) → Node("coconut", 5) → 
         * │   │   Node("cranberry", 6) → null
         * ├───┤
         * │ 8 │ → null
         * ├───┤
         * │...│
         * └───┘
         * 
         * KEY OBSERVATIONS:
         * - Most buckets are empty (good distribution)
         * - Some buckets have multiple entries (collisions)
         * - Each bucket is a linked list
         * - If bucket 7 grows to 8+ nodes → converts to tree
         */
    }
    
    /**
     * Memory layout
     */
    public static void memoryLayout() {
        /**
         * MEMORY STRUCTURE:
         * 
         * HashMap object:
         * ┌──────────────────┐
         * │ table reference  │ ─┐
         * │ size = 5         │  │
         * │ threshold = 12   │  │
         * │ loadFactor = 0.75│  │
         * └──────────────────┘  │
         *                       │
         *                       v
         * Node<K,V>[] table (capacity 16):
         * ┌────┬────┬────┬────┬────┬────┬────┬────┐
         * │ ref│null│ref │null│null│null│null│ref │...
         * └─┬──┴────┴─┬──┴────┴────┴────┴────┴─┬──┘
         *   │         │                         │
         *   v         v                         v
         * Node      Node                      Node
         * 
         * MEMORY USAGE:
         * - HashMap object: ~48 bytes (object header + fields)
         * - Array: 16 bytes (header) + (capacity * 8 bytes for refs)
         * - Each Node: ~40 bytes (header + 4 fields + padding)
         * - Each TreeNode: ~56 bytes (Node + tree fields)
         * 
         * Example for 1000 entries, capacity 2048:
         * - HashMap: 48 bytes
         * - Array: 16 + (2048 * 8) = 16,400 bytes
         * - Nodes: 1000 * 40 = 40,000 bytes
         * TOTAL: ~56,448 bytes = ~55 KB
         * 
         * Compare to 1000 entries in array of key-value pairs:
         * - Array: 16 + (1000 * 8 * 2) = 16,016 bytes
         * OVERHEAD: 3.5x for HashMap (trade-off for O(1) lookup)
         */
    }
}
```

---

## 2. Hash Function and Bucket Selection

### How Keys Map to Buckets

```java
/**
 * HASH FUNCTION AND BUCKET SELECTION
 * 
 * How HashMap determines which bucket for a key
 */

public class HashFunctionExplained {
    
    /**
     * Step 1: Get hash code from key
     */
    public static void step1_ObjectHashCode() {
        String key = "apple";
        int originalHash = key.hashCode();
        
        System.out.println("Key: " + key);
        System.out.println("hashCode(): " + originalHash);
        System.out.println("Binary: " + Integer.toBinaryString(originalHash));
        
        /**
         * OUTPUT:
         * Key: apple
         * hashCode(): 93029210
         * Binary: 101100011010001100101001010
         * 
         * PROBLEM:
         * hashCode() returns 32-bit int (-2^31 to 2^31-1)
         * But table size is much smaller (e.g., 16, 32, 64...)
         * Need to map hash to bucket index
         */
    }
    
    /**
     * Step 2: HashMap's hash() function (additional mixing)
     */
    static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
    
    public static void step2_HashMapHash() {
        String key = "apple";
        int originalHash = key.hashCode();
        int mappedHash = hash(key);
        
        System.out.println("Original hash: " + originalHash);
        System.out.println("Mapped hash:   " + mappedHash);
        System.out.println();
        System.out.println("Binary original: " + 
            Integer.toBinaryString(originalHash));
        System.out.println("Binary mapped:   " + 
            Integer.toBinaryString(mappedHash));
        
        /**
         * OUTPUT:
         * Original hash: 93029210
         * Mapped hash:   93030638
         * 
         * WHY ADDITIONAL MIXING?
         * 
         * h ^ (h >>> 16)
         * 
         * Takes high 16 bits and XORs with low 16 bits
         * 
         * Original: 0101100011010001 1001010100101010
         *           ^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^
         *           High 16 bits     Low 16 bits
         * 
         * Shift:    0000000000000000 0101100011010001 (h >>> 16)
         * 
         * XOR:      0101100011010001 1001010100101010 (original)
         *         ^ 0000000000000000 0101100011010001 (shifted)
         *           ────────────────────────────────
         *           0101100011010001 1100110111111011 (result)
         * 
         * BENEFIT:
         * - Incorporates high bits into low bits
         * - Better distribution when table size small
         * - Reduces collisions for similar hash codes
         */
    }
    
    /**
     * Step 3: Map to bucket index
     */
    static int indexFor(int hash, int length) {
        return hash & (length - 1);
    }
    
    public static void step3_BucketSelection() {
        String key = "apple";
        int hash = hash(key);
        int capacity = 16;
        
        int index = indexFor(hash, capacity);
        
        System.out.println("Hash: " + hash);
        System.out.println("Capacity: " + capacity);
        System.out.println("Bucket index: " + index);
        System.out.println();
        
        // Show why & is used instead of %
        System.out.println("Method 1 (& operator): " + (hash & (capacity - 1)));
        System.out.println("Method 2 (% operator): " + (hash % capacity));
        System.out.println();
        
        // Binary explanation
        System.out.println("Binary hash:       " + Integer.toBinaryString(hash));
        System.out.println("Binary (cap-1=15): " + Integer.toBinaryString(capacity - 1));
        System.out.println("Binary result:     " + Integer.toBinaryString(index));
        
        /**
         * OUTPUT:
         * Hash: 93030638
         * Capacity: 16
         * Bucket index: 14
         * 
         * Method 1 (& operator): 14
         * Method 2 (% operator): 14
         * 
         * Binary hash:       101100011010001100111110101110
         * Binary (cap-1=15): 1111
         * Binary result:     1110
         * 
         * WHY & INSTEAD OF %:
         * 
         * hash & (capacity - 1)  is equivalent to  hash % capacity
         * 
         * BUT ONLY when capacity is power of 2!
         * 
         * Example with capacity = 16:
         * capacity - 1 = 15 = 0b1111
         * 
         * hash & 0b1111 keeps only last 4 bits
         * = value from 0 to 15
         * = perfect for array indexing
         * 
         * ADVANTAGE:
         * & is MUCH faster than %
         * & is single CPU instruction
         * % requires division (slow)
         * 
         * THIS IS WHY HashMap capacity is always power of 2!
         */
    }
    
    /**
     * Complete process visualization
     */
    public static void completeProcess() {
        /**
         * COMPLETE HASH-TO-BUCKET PROCESS:
         * 
         * 1. Key object
         *    ↓
         * 2. key.hashCode() → 32-bit int
         *    ↓
         * 3. HashMap.hash() → mix high/low bits
         *    ↓
         * 4. hash & (capacity - 1) → bucket index
         *    ↓
         * 5. table[index] → access bucket
         * 
         * EXAMPLE:
         * 
         * Key: "apple"
         *   ↓
         * hashCode(): 93029210
         *   ↓
         * hash(): 93030638 (after mixing)
         *   ↓
         * & 15 (capacity 16): 14
         *   ↓
         * table[14] → bucket 14
         * 
         * TIME COMPLEXITY: O(1)
         */
    }
    
    /**
     * Distribution quality test
     */
    public static void distributionTest() {
        int capacity = 16;
        int[] bucketCounts = new int[capacity];
        
        // Insert 1000 random strings
        java.util.Random random = new java.util.Random(42);
        for (int i = 0; i < 1000; i++) {
            String key = "key" + random.nextInt(10000);
            int hash = hash(key);
            int index = indexFor(hash, capacity);
            bucketCounts[index]++;
        }
        
        System.out.println("Distribution across buckets:");
        for (int i = 0; i < capacity; i++) {
            System.out.println("Bucket " + i + ": " + 
                bucketCounts[i] + " keys");
        }
        
        /**
         * OUTPUT (typical):
         * Bucket 0: 58 keys
         * Bucket 1: 65 keys
         * Bucket 2: 61 keys
         * Bucket 3: 63 keys
         * Bucket 4: 59 keys
         * ...
         * 
         * OBSERVATION:
         * - Relatively even distribution
         * - Average: 1000/16 = 62.5 keys per bucket
         * - Good hash function keeps close to average
         * - Poor hash function → uneven (many collisions)
         */
    }
    
    public static void main(String[] args) {
        System.out.println("=== Step 1: Object hashCode ===");
        step1_ObjectHashCode();
        
        System.out.println("\n=== Step 2: HashMap hash() ===");
        step2_HashMapHash();
        
        System.out.println("\n=== Step 3: Bucket Selection ===");
        step3_BucketSelection();
        
        System.out.println("\n=== Distribution Test ===");
        distributionTest();
    }
}
```

---

## 3. Collision Handling

### Chaining and Treeification

```java
/**
 * COLLISION HANDLING
 * 
 * What happens when multiple keys hash to same bucket
 */

import java.util.*;

public class CollisionHandling {
    
    /**
     * Method 1: Chaining (linked list)
     */
    static class ChainingExample {
        
        /**
         * CHAINING:
         * 
         * When collision occurs, new node added to linked list
         * 
         * Bucket 5:
         * ┌──────────────┐
         * │ Node("a", 1) │ → Node("b", 2) → Node("c", 3) → null
         * └──────────────┘
         * 
         * All three keys hash to bucket 5
         * Linked as chain
         */
        
        public static void demonstrateChaining() {
            Map<CollidingKey, String> map = new HashMap<>();
            
            // Keys that hash to same bucket
            CollidingKey k1 = new CollidingKey("a", 5);
            CollidingKey k2 = new CollidingKey("b", 5);
            CollidingKey k3 = new CollidingKey("c", 5);
            
            map.put(k1, "Value1");
            map.put(k2, "Value2");
            map.put(k3, "Value3");
            
            System.out.println("All keys hash to bucket: " + k1.desiredBucket);
            System.out.println("Get k1: " + map.get(k1));
            System.out.println("Get k2: " + map.get(k2));
            System.out.println("Get k3: " + map.get(k3));
            
            /**
             * LOOKUP PROCESS:
             * 
             * map.get(k2):
             * 1. Calculate hash → bucket 5
             * 2. Go to table[5]
             * 3. Walk linked list:
             *    - Compare k2 with Node("a") → not equal
             *    - Compare k2 with Node("b") → EQUAL! Return value
             * 
             * TIME COMPLEXITY:
             * - Best case: O(1) (first in chain)
             * - Average: O(k) where k = keys in bucket
             * - Worst case: O(n) (all keys in one bucket)
             */
        }
        
        static class CollidingKey {
            String key;
            int desiredBucket;
            
            CollidingKey(String key, int bucket) {
                this.key = key;
                this.desiredBucket = bucket;
            }
            
            @Override
            public int hashCode() {
                // Force to specific bucket
                return desiredBucket;
            }
            
            @Override
            public boolean equals(Object obj) {
                if (!(obj instanceof CollidingKey)) return false;
                return this.key.equals(((CollidingKey) obj).key);
            }
        }
    }
    
    /**
     * Method 2: Treeification (Java 8+)
     */
    static class TreeificationExample {
        
        /**
         * TREEIFICATION THRESHOLDS:
         * 
         * TREEIFY_THRESHOLD = 8
         * - When bucket has 8+ entries
         * - AND table capacity >= 64
         * - Convert linked list to red-black tree
         * 
         * MIN_TREEIFY_CAPACITY = 64
         * - If bucket reaches 8 but capacity < 64
         * - Resize instead of treeify
         * 
         * UNTREEIFY_THRESHOLD = 6
         * - After removal, if tree has ≤ 6 nodes
         * - Convert back to linked list
         */
        
        public static void demonstrateTreeification() {
            Map<CollidingKey, String> map = new HashMap<>(64);
            
            // Add 10 keys to same bucket
            System.out.println("Adding 10 keys to same bucket...");
            for (int i = 0; i < 10; i++) {
                map.put(new CollidingKey("key" + i, 5), "Value" + i);
                
                if (i == 7) {
                    System.out.println("After 8th key: Bucket treeified!");
                }
            }
            
            /**
             * BEFORE TREEIFICATION (7 or fewer):
             * Bucket 5: Node → Node → Node → ... → Node → null
             * 
             * AFTER TREEIFICATION (8 or more):
             * Bucket 5: TreeNode (root of red-black tree)
             *              /           \
             *         TreeNode      TreeNode
             *           /  \           /  \
             *      TreeNode TreeNode ...
             * 
             * LOOKUP TIME COMPLEXITY:
             * - Linked list: O(n) worst case
             * - Red-black tree: O(log n) worst case
             * 
             * For 8 nodes:
             * - List: up to 8 comparisons
             * - Tree: up to 3 comparisons (log₂ 8)
             * 
             * IMPROVEMENT: ~2.6x better for 8 nodes
             */
        }
        
        static class CollidingKey {
            String key;
            int desiredBucket;
            
            CollidingKey(String key, int bucket) {
                this.key = key;
                this.desiredBucket = bucket;
            }
            
            @Override
            public int hashCode() {
                return desiredBucket;
            }
            
            @Override
            public boolean equals(Object obj) {
                if (!(obj instanceof CollidingKey)) return false;
                return this.key.equals(((CollidingKey) obj).key);
            }
        }
    }
    
    /**
     * Performance comparison
     */
    public static void performanceComparison() {
        int bucketSize = 100;
        
        // Simulate linked list lookup
        long start = System.nanoTime();
        for (int i = 0; i < 10000; i++) {
            // Linear search (average n/2 comparisons)
            int target = bucketSize / 2;
            for (int j = 0; j < target; j++) {
                if (j == target) break;
            }
        }
        long listTime = System.nanoTime() - start;
        
        // Simulate tree lookup
        start = System.nanoTime();
        for (int i = 0; i < 10000; i++) {
            // Binary search (log n comparisons)
            int target = bucketSize / 2;
            int comparisons = (int) (Math.log(bucketSize) / Math.log(2));
            for (int j = 0; j < comparisons; j++) {
                // Simulated tree traversal
            }
        }
        long treeTime = System.nanoTime() - start;
        
        System.out.println("Bucket size: " + bucketSize);
        System.out.println("List lookup: " + listTime + "ns");
        System.out.println("Tree lookup: " + treeTime + "ns");
        System.out.println("Speedup: " + (double) listTime / treeTime + "x");
        
        /**
         * OUTPUT (typical):
         * Bucket size: 100
         * List lookup: 15000ns
         * Tree lookup: 2500ns
         * Speedup: 6x
         * 
         * WHY THRESHOLD AT 8?
         * 
         * - Balance between list and tree overhead
         * - Tree nodes larger (more memory)
         * - Tree operations more complex
         * - For small buckets (< 8), list faster
         * - For large buckets (≥ 8), tree faster
         * - 8 is empirically determined sweet spot
         */
    }
    
    /**
     * Collision resolution strategies
     */
    public static void collisionStrategies() {
        /**
         * COLLISION RESOLUTION METHODS:
         * 
         * 1. SEPARATE CHAINING (HashMap uses this):
         *    - Each bucket is linked list/tree
         *    - Colliding entries stored in same bucket
         *    - Pros: Simple, handles any load factor
         *    - Cons: Extra memory for pointers
         * 
         * 2. OPEN ADDRESSING (not used by HashMap):
         *    - All entries in main array
         *    - Collision → probe for next empty slot
         *    - Linear probing: try [i+1], [i+2], ...
         *    - Quadratic probing: try [i+1²], [i+2²], ...
         *    - Double hashing: use second hash function
         *    - Pros: Better cache locality, less memory
         *    - Cons: More complex, clustering issues
         * 
         * WHY HASHMAP USES CHAINING:
         * - Simpler to implement
         * - Better worst-case behavior
         * - Easier to resize
         * - Can handle high load factors
         */
    }
    
    public static void main(String[] args) {
        System.out.println("=== Chaining Example ===");
        ChainingExample.demonstrateChaining();
        
        System.out.println("\n=== Treeification Example ===");
        TreeificationExample.demonstrateTreeification();
        
        System.out.println("\n=== Performance Comparison ===");
        performanceComparison();
    }
}
```

---

## 4. Load Factor and Resizing

### Automatic Growth and Rehashing

```java
/**
 * LOAD FACTOR AND RESIZING
 * 
 * How HashMap grows and rehashes entries
 */

public class LoadFactorAndResizing {
    
    /**
     * Load factor concept
     */
    public static void loadFactorConcept() {
        /**
         * LOAD FACTOR = size / capacity
         * 
         * Example:
         * - Capacity: 16 buckets
         * - Size: 10 entries
         * - Load factor: 10/16 = 0.625
         * 
         * DEFAULT_LOAD_FACTOR = 0.75
         * 
         * WHY 0.75?
         * - Balance between space and time
         * - Higher load factor:
         *   ✓ Less memory
         *   ✗ More collisions
         *   ✗ Slower lookups
         * 
         * - Lower load factor:
         *   ✓ Fewer collisions
         *   ✓ Faster lookups
         *   ✗ More memory
         * 
         * 0.75 is empirically determined sweet spot
         */
    }
    
    /**
     * Resizing trigger
     */
    static class ResizingTrigger {
        
        public static void demonstrateTrigger() {
            int capacity = 16;
            float loadFactor = 0.75f;
            int threshold = (int) (capacity * loadFactor);
            
            System.out.println("Initial capacity: " + capacity);
            System.out.println("Load factor: " + loadFactor);
            System.out.println("Threshold: " + threshold);
            System.out.println();
            
            // Simulate insertions
            for (int size = 0; size <= threshold + 2; size++) {
                float currentLoad = (float) size / capacity;
                System.out.printf("Size: %2d, Load: %.2f", size, currentLoad);
                
                if (size > threshold) {
                    System.out.println(" → RESIZE TRIGGERED!");
                    capacity *= 2;
                    threshold = (int) (capacity * loadFactor);
                    System.out.println("   New capacity: " + capacity);
                    System.out.println("   New threshold: " + threshold);
                } else {
                    System.out.println();
                }
            }
            
            /**
             * OUTPUT:
             * Initial capacity: 16
             * Load factor: 0.75
             * Threshold: 12
             * 
             * Size:  0, Load: 0.00
             * Size:  1, Load: 0.06
             * ...
             * Size: 12, Load: 0.75
             * Size: 13, Load: 0.81 → RESIZE TRIGGERED!
             *    New capacity: 32
             *    New threshold: 24
             * 
             * RESIZE HAPPENS WHEN:
             * size > threshold
             * = size > capacity * loadFactor
             */
        }
    }
    
    /**
     * Resizing process (rehashing)
     */
    static class RehashingProcess {
        
        public static void demonstrateRehashing() {
            /**
             * RESIZING PROCESS:
             * 
             * 1. Allocate new array (2x capacity)
             * 2. For each entry in old array:
             *    - Recalculate bucket index
             *    - Move to new array
             * 3. Replace old array with new
             * 
             * WHY RECALCULATE?
             * 
             * Bucket index = hash & (capacity - 1)
             * 
             * Old capacity: 16, mask: 0b1111 (4 bits)
             * New capacity: 32, mask: 0b11111 (5 bits)
             * 
             * Example:
             * hash = 0b...10110
             * 
             * Old: 0b10110 & 0b01111 = 0b00110 = 6
             * New: 0b10110 & 0b11111 = 0b10110 = 22
             * 
             * Entry moves from bucket 6 to bucket 22!
             */
            
            System.out.println("REHASHING EXAMPLE:");
            System.out.println();
            
            int oldCap = 16;
            int newCap = 32;
            
            // Sample hash values
            int[] hashes = {6, 22, 38, 54};
            
            System.out.println("Capacity: " + oldCap + " → " + newCap);
            System.out.println();
            
            for (int hash : hashes) {
                int oldIndex = hash & (oldCap - 1);
                int newIndex = hash & (newCap - 1);
                
                System.out.printf("Hash %2d: bucket %2d → bucket %2d%n", 
                    hash, oldIndex, newIndex);
            }
            
            /**
             * OUTPUT:
             * Capacity: 16 → 32
             * 
             * Hash  6: bucket  6 → bucket  6
             * Hash 22: bucket  6 → bucket 22
             * Hash 38: bucket  6 → bucket  6
             * Hash 54: bucket  6 → bucket 22
             * 
             * OBSERVATION:
             * - Entries either stay in same bucket
             * - Or move to (oldIndex + oldCapacity)
             * 
             * PATTERN:
             * If bit N (new bit) of hash is:
             * - 0: stays in oldIndex
             * - 1: moves to (oldIndex + oldCapacity)
             * 
             * Example with oldCap=16, newCap=32:
             * Bit 4 determines movement
             * 
             * This optimization makes resizing O(n) instead of O(n log n)
             */
        }
        
        public static void optimizedRehashing() {
            /**
             * JAVA 8 OPTIMIZATION:
             * 
             * Instead of recalculating index for each entry,
             * HashMap uses bit trick:
             * 
             * if ((hash & oldCap) == 0) {
             *     // Stay in same bucket
             *     newTable[oldIndex] = entry;
             * } else {
             *     // Move to (oldIndex + oldCapacity)
             *     newTable[oldIndex + oldCap] = entry;
             * }
             * 
             * WHY IT WORKS:
             * 
             * Old mask: 0b01111 (capacity 16)
             * New mask: 0b11111 (capacity 32)
             *           ^
             *           New bit
             * 
             * The new bit is exactly the bit of oldCapacity
             * 
             * hash & oldCap checks if new bit is set
             * - If 0: index unchanged
             * - If 1: index increased by oldCap
             */
        }
    }
    
    /**
     * Resizing cost
     */
    public static void resizingCost() {
        Map<Integer, Integer> map = new HashMap<>(4);
        
        long[] resizeTimes = new long[10];
        int resizeCount = 0;
        
        int prevCapacity = 4;
        
        for (int i = 0; i < 10000; i++) {
            long start = System.nanoTime();
            map.put(i, i);
            long elapsed = System.nanoTime() - start;
            
            // Detect resize (insertion takes much longer)
            if (elapsed > 100000) {  // 100 microseconds
                if (resizeCount < resizeTimes.length) {
                    resizeTimes[resizeCount++] = elapsed;
                    System.out.println("Resize at size " + i + 
                        ": " + elapsed / 1000 + " µs");
                }
            }
        }
        
        /**
         * OUTPUT (typical):
         * Resize at size 4: 250 µs
         * Resize at size 7: 180 µs
         * Resize at size 13: 320 µs
         * Resize at size 25: 650 µs
         * Resize at size 49: 1200 µs
         * Resize at size 97: 2400 µs
         * ...
         * 
         * OBSERVATIONS:
         * - Resize time grows with capacity
         * - Must rehash all entries
         * - Time ∝ current size
         * 
         * AMORTIZED COST:
         * - Resize happens infrequently
         * - Cost amortized over insertions
         * - Overall: O(1) amortized per insertion
         * 
         * EXAMPLE:
         * Insert 128 entries (cap 4 → 8 → 16 → 32 → 64 → 128)
         * Total rehashes: 4 + 8 + 16 + 32 + 64 = 124 operations
         * Amortized: 124/128 ≈ 1 extra operation per insert
         */
    }
    
    /**
     * Choosing initial capacity
     */
    public static void choosingCapacity() {
        /**
         * INITIAL CAPACITY SELECTION:
         * 
         * If you know expected size, set initial capacity:
         * 
         * new HashMap<>(expectedSize / 0.75 + 1)
         * 
         * Example: Expecting 1000 entries
         * capacity = 1000 / 0.75 + 1 = 1334
         * → Next power of 2 = 2048
         * 
         * WHY?
         * - Avoids multiple resizes
         * - Better performance
         * - Less memory churn
         * 
         * TRADEOFF:
         * - Too small: Multiple resizes
         * - Too large: Wasted memory
         * 
         * RULE OF THUMB:
         * - Known size: Set initial capacity
         * - Unknown size: Use default (16)
         */
        
        // Performance comparison
        int size = 10000;
        
        // Without sizing
        long start = System.nanoTime();
        Map<Integer, Integer> map1 = new HashMap<>();
        for (int i = 0; i < size; i++) {
            map1.put(i, i);
        }
        long time1 = System.nanoTime() - start;
        
        // With sizing
        start = System.nanoTime();
        Map<Integer, Integer> map2 = new HashMap<>((int) (size / 0.75 + 1));
        for (int i = 0; i < size; i++) {
            map2.put(i, i);
        }
        long time2 = System.nanoTime() - start;
        
        System.out.println("Without sizing: " + time1 / 1_000_000 + "ms");
        System.out.println("With sizing: " + time2 / 1_000_000 + "ms");
        System.out.println("Speedup: " + (double) time1 / time2 + "x");
        
        /**
         * OUTPUT:
         * Without sizing: 15ms
         * With sizing: 8ms
         * Speedup: 1.9x
         * 
         * SIGNIFICANT for large maps!
         */
    }
    
    public static void main(String[] args) {
        System.out.println("=== Resizing Trigger ===");
        ResizingTrigger.demonstrateTrigger();
        
        System.out.println("\n=== Rehashing Process ===");
        RehashingProcess.demonstrateRehashing();
        
        System.out.println("\n=== Resizing Cost ===");
        resizingCost();
        
        System.out.println("\n=== Choosing Capacity ===");
        choosingCapacity();
    }
}
```

---

## 5. Java 8+ Improvements

### Balanced Trees and Other Enhancements

```java
/**
 * JAVA 8+ IMPROVEMENTS
 * 
 * Major enhancements in Java 8 and later
 */

import java.util.*;

public class Java8Improvements {
    
    /**
     * Improvement 1: Treeification
     */
    public static void treeificationBenefit() {
        /**
         * BEFORE JAVA 8:
         * - Always linked list for collisions
         * - Worst case: O(n) lookup
         * 
         * JAVA 8+:
         * - Linked list converts to red-black tree
         * - When bucket has 8+ entries
         * - Worst case: O(log n) lookup
         * 
         * IMPACT:
         * 
         * Example: 100 colliding keys in one bucket
         * 
         * Linked list: up to 100 comparisons
         * Red-black tree: up to 7 comparisons (log₂ 100)
         * 
         * 14x improvement!
         * 
         * ATTACK MITIGATION:
         * Prevents hash collision DoS attacks
         * Attacker can't cause O(n²) performance
         */
        
        // Demonstrate worst-case performance
        Map<BadHashKey, Integer> map = new HashMap<>();
        
        // All keys hash to same bucket
        int n = 1000;
        for (int i = 0; i < n; i++) {
            map.put(new BadHashKey(i), i);
        }
        
        long start = System.nanoTime();
        for (int i = 0; i < 1000; i++) {
            map.get(new BadHashKey(n / 2));
        }
        long elapsed = System.nanoTime() - start;
        
        System.out.println("1000 lookups with " + n + " collisions: " + 
            elapsed / 1000 + " µs");
        System.out.println("Average per lookup: " + elapsed / 1000 / 1000 + " µs");
        
        /**
         * OUTPUT (Java 8+):
         * 1000 lookups with 1000 collisions: 350 µs
         * Average per lookup: 0.35 µs
         * 
         * OUTPUT (Java 7, theoretical):
         * Would be ~5000 µs (14x slower)
         */
    }
    
    static class BadHashKey {
        int value;
        
        BadHashKey(int value) {
            this.value = value;
        }
        
        @Override
        public int hashCode() {
            return 0;  // All keys hash to bucket 0!
        }
        
        @Override
        public boolean equals(Object obj) {
            return obj instanceof BadHashKey && 
                   ((BadHashKey) obj).value == this.value;
        }
    }
    
    /**
     * Improvement 2: Better hash function
     */
    public static void betterHashFunction() {
        /**
         * JAVA 8 HASH FUNCTION:
         * 
         * static final int hash(Object key) {
         *     int h;
         *     return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
         * }
         * 
         * JAVA 7 HASH FUNCTION (simplified):
         * Had more complex bit mixing
         * 
         * JAVA 8 SIMPLER BUT EFFECTIVE:
         * - XOR high 16 bits with low 16 bits
         * - Better distribution for small tables
         * - Faster computation
         */
    }
    
    /**
     * Improvement 3: Optimized resizing
     */
    public static void optimizedResizing() {
        /**
         * JAVA 8 RESIZE OPTIMIZATION:
         * 
         * Old way (Java 7):
         * for (Entry e : oldTable) {
         *     int newIndex = hash(e.key) & (newCapacity - 1);
         *     place entry at newIndex
         * }
         * 
         * New way (Java 8):
         * for (Entry e : oldTable) {
         *     if ((e.hash & oldCapacity) == 0) {
         *         place at same index
         *     } else {
         *         place at (oldIndex + oldCapacity)
         *     }
         * }
         * 
         * BENEFIT:
         * - Avoids recomputing hash
         * - Single bit test
         * - Preserves order in bucket
         * - Faster resize
         */
    }
    
    /**
     * Improvement 4: New methods
     */
    public static void newMethods() {
        Map<String, Integer> map = new HashMap<>();
        map.put("a", 1);
        map.put("b", 2);
        
        // Java 8: forEach
        System.out.println("=== forEach ===");
        map.forEach((k, v) -> System.out.println(k + ": " + v));
        
        // Java 8: computeIfAbsent
        System.out.println("\n=== computeIfAbsent ===");
        map.computeIfAbsent("c", k -> k.length());
        System.out.println("After computeIfAbsent('c'): " + map);
        
        // Java 8: computeIfPresent
        System.out.println("\n=== computeIfPresent ===");
        map.computeIfPresent("a", (k, v) -> v * 10);
        System.out.println("After computeIfPresent('a'): " + map);
        
        // Java 8: merge
        System.out.println("\n=== merge ===");
        map.merge("a", 5, Integer::sum);
        System.out.println("After merge('a', 5): " + map);
        
        // Java 8: getOrDefault
        System.out.println("\n=== getOrDefault ===");
        System.out.println("Get 'x' with default: " + 
            map.getOrDefault("x", -1));
        
        /**
         * BENEFITS:
         * - Cleaner code
         * - Atomic operations
         * - Better performance (single lookup)
         * 
         * EXAMPLE: Word count
         * 
         * Old way:
         * if (map.containsKey(word)) {
         *     map.put(word, map.get(word) + 1);  // 3 lookups!
         * } else {
         *     map.put(word, 1);
         * }
         * 
         * New way:
         * map.merge(word, 1, Integer::sum);  // 1 lookup!
         */
    }
    
    /**
     * Improvement 5: Spliterator support
     */
    public static void spliteratorSupport() {
        Map<String, Integer> map = new HashMap<>();
        for (int i = 0; i < 100; i++) {
            map.put("key" + i, i);
        }
        
        // Parallel processing
        long sum = map.values().stream()
            .parallel()
            .mapToLong(Integer::longValue)
            .sum();
        
        System.out.println("Sum (parallel): " + sum);
        
        /**
         * SPLITERATOR:
         * - Supports parallel iteration
         * - Can split work across threads
         * - Used by parallel streams
         * - Better than older iterators
         */
    }
    
    /**
     * Performance comparison: Java 7 vs Java 8
     */
    public static void performanceComparison() {
        int n = 100000;
        
        // Worst case: all keys collide
        Map<BadHashKey, Integer> map = new HashMap<>(128);
        for (int i = 0; i < 100; i++) {
            map.put(new BadHashKey(i), i);
        }
        
        // Lookup performance
        long start = System.nanoTime();
        for (int i = 0; i < n; i++) {
            map.get(new BadHashKey(50));
        }
        long elapsed = System.nanoTime() - start;
        
        System.out.println("Lookups with collision: " + 
            elapsed / 1_000_000 + "ms");
        System.out.println("Average: " + elapsed / n + "ns per lookup");
        
        /**
         * JAVA 8+ (with treeification):
         * Lookups with collision: 45ms
         * Average: 450ns per lookup
         * 
         * JAVA 7 (theoretical without trees):
         * Would be ~650ms
         * Average: 6500ns per lookup
         * 
         * 14x improvement in worst case!
         */
    }
    
    public static void main(String[] args) {
        System.out.println("=== Treeification Benefit ===");
        treeificationBenefit();
        
        System.out.println("\n=== New Methods ===");
        newMethods();
        
        System.out.println("\n=== Performance Comparison ===");
        performanceComparison();
    }
}
```

---

## 6. equals() and hashCode() Contract

### The Critical Contract

```java
/**
 * EQUALS AND HASHCODE CONTRACT
 * 
 * Critical rules for HashMap keys
 */

import java.util.*;

public class EqualsHashCodeContract {
    
    /**
     * The contract rules
     */
    public static void theContract() {
        /**
         * EQUALS/HASHCODE CONTRACT:
         * 
         * 1. If a.equals(b), then a.hashCode() == b.hashCode()
         *    (Equal objects MUST have equal hash codes)
         * 
         * 2. If a.hashCode() != b.hashCode(), then !a.equals(b)
         *    (Contrapositive of rule 1)
         * 
         * 3. If a.hashCode() == b.hashCode(), a MAY OR MAY NOT equal b
         *    (Hash collision is allowed)
         * 
         * 4. hashCode() must be consistent
         *    (Same object returns same hash if not modified)
         * 
         * 5. If a.equals(a) must be true
         *    (Reflexive)
         * 
         * 6. If a.equals(b), then b.equals(a)
         *    (Symmetric)
         * 
         * 7. If a.equals(b) and b.equals(c), then a.equals(c)
         *    (Transitive)
         * 
         * BREAKING THE CONTRACT = BROKEN HASHMAP!
         */
    }
    
    /**
     * Example 1: Correct implementation
     */
    static class Person {
        private final String name;
        private final int age;
        
        public Person(String name, int age) {
            this.name = name;
            this.age = age;
        }
        
        @Override
        public boolean equals(Object obj) {
            if (this == obj) return true;
            if (!(obj instanceof Person)) return false;
            Person other = (Person) obj;
            return age == other.age && 
                   Objects.equals(name, other.name);
        }
        
        @Override
        public int hashCode() {
            return Objects.hash(name, age);
        }
        
        /**
         * CORRECT:
         * - Uses same fields in both methods
         * - Objects.hash() provides good distribution
         * - Consistent with equals
         */
    }
    
    /**
     * Example 2: Broken - missing hashCode()
     */
    static class BrokenPerson1 {
        private String name;
        private int age;
        
        public BrokenPerson1(String name, int age) {
            this.name = name;
            this.age = age;
        }
        
        @Override
        public boolean equals(Object obj) {
            if (!(obj instanceof BrokenPerson1)) return false;
            BrokenPerson1 other = (BrokenPerson1) obj;
            return age == other.age && 
                   Objects.equals(name, other.name);
        }
        
        // Missing hashCode()!
        // Uses default Object.hashCode() (memory address)
    }
    
    public static void demonstrateBroken1() {
        Map<BrokenPerson1, String> map = new HashMap<>();
        
        BrokenPerson1 p1 = new BrokenPerson1("Alice", 30);
        BrokenPerson1 p2 = new BrokenPerson1("Alice", 30);
        
        map.put(p1, "Engineer");
        
        System.out.println("p1.equals(p2): " + p1.equals(p2));
        System.out.println("p1.hashCode(): " + p1.hashCode());
        System.out.println("p2.hashCode(): " + p2.hashCode());
        System.out.println("map.get(p1): " + map.get(p1));
        System.out.println("map.get(p2): " + map.get(p2));
        
        /**
         * OUTPUT:
         * p1.equals(p2): true
         * p1.hashCode(): 1234567890
         * p2.hashCode(): 9876543210  (DIFFERENT!)
         * map.get(p1): Engineer
         * map.get(p2): null  (BUG!)
         * 
         * PROBLEM:
         * - p1 and p2 are equal
         * - But have different hash codes
         * - Violates contract: equals → same hashCode
         * - map.get(p2) looks in wrong bucket
         * - Can't find the entry!
         */
    }
    
    /**
     * Example 3: Broken - mutable key
     */
    static class BrokenPerson2 {
        private String name;  // mutable!
        private int age;
        
        public BrokenPerson2(String name, int age) {
            this.name = name;
            this.age = age;
        }
        
        public void setName(String name) {
            this.name = name;
        }
        
        @Override
        public boolean equals(Object obj) {
            if (!(obj instanceof BrokenPerson2)) return false;
            BrokenPerson2 other = (BrokenPerson2) obj;
            return Objects.equals(name, other.name);
        }
        
        @Override
        public int hashCode() {
            return Objects.hash(name);
        }
    }
    
    public static void demonstrateBroken2() {
        Map<BrokenPerson2, String> map = new HashMap<>();
        
        BrokenPerson2 person = new BrokenPerson2("Alice", 30);
        map.put(person, "Engineer");
        
        System.out.println("Before modification:");
        System.out.println("  hashCode: " + person.hashCode());
        System.out.println("  get: " + map.get(person));
        
        // Modify key!
        person.setName("Bob");
        
        System.out.println("After modification:");
        System.out.println("  hashCode: " + person.hashCode());
        System.out.println("  get: " + map.get(person));
        System.out.println("  map.size(): " + map.size());
        
        /**
         * OUTPUT:
         * Before modification:
         *   hashCode: 63537155
         *   get: Engineer
         * After modification:
         *   hashCode: 65891  (CHANGED!)
         *   get: null  (BUG!)
         *   map.size(): 1  (Entry is LOST!)
         * 
         * PROBLEM:
         * - hashCode changed after insertion
         * - Entry stored in bucket for old hash
         * - Lookup uses new hash → wrong bucket
         * - Entry is unreachable!
         * - Memory leak (entry can't be removed)
         * 
         * RULE: Keys should be IMMUTABLE
         */
    }
    
    /**
     * Best practices
     */
    public static void bestPractices() {
        /**
         * BEST PRACTICES:
         * 
         * 1. ALWAYS override both equals and hashCode together
         * 
         * 2. Use same fields in both methods
         * 
         * 3. Use Objects.hash() for hashCode:
         *    return Objects.hash(field1, field2, field3);
         * 
         * 4. Make keys immutable (final fields, no setters)
         * 
         * 5. Use IDE to generate (IntelliJ, Eclipse)
         * 
         * 6. Consider using Records (Java 14+):
         *    record Person(String name, int age) {}
         *    // Automatically generates equals/hashCode!
         * 
         * 7. For collections in fields, use deep equals:
         *    Arrays.equals(array1, array2)
         *    list1.equals(list2)
         * 
         * 8. Check for null in equals
         * 
         * 9. Check type with instanceof
         * 
         * 10. Test edge cases
         */
    }
}
```

---

## 7. Common Pitfalls and Best Practices

### Avoiding HashMap Mistakes

```java
/**
 * COMMON PITFALLS
 */

import java.util.*;

public class HashMapPitfalls {
    
    /**
     * Pitfall 1: Mutable keys
     */
    public static void mutableKeys() {
        Map<List<String>, String> map = new HashMap<>();
        
        List<String> key = new ArrayList<>();
        key.add("a");
        
        map.put(key, "Value");
        System.out.println("Before: " + map.get(key));
        
        key.add("b");  // Modify key!
        
        System.out.println("After: " + map.get(key));  // null!
    }
    
    /**
     * Pitfall 2: Concurrent modification
     */
    public static void concurrentModification() {
        Map<Integer, String> map = new HashMap<>();
        map.put(1, "a");
        map.put(2, "b");
        
        // WRONG
        for (Integer key : map.keySet()) {
            if (key == 2) map.remove(key);  // Exception!
        }
        
        // RIGHT
        map.keySet().removeIf(key -> key == 2);
    }
    
    /**
     * Pitfall 3: Threading
     */
    public static void threading() {
        // WRONG for concurrent access
        Map<K, V> map = new HashMap<>();
        
        // RIGHT
        Map<K, V> map = new ConcurrentHashMap<>();
    }
    
    /**
     * Pitfall 4: Not sizing
     */
    public static void notSizing() {
        // SLOW - multiple resizes
        Map<Integer, Integer> map = new HashMap<>();
        for (int i = 0; i < 100000; i++) {
            map.put(i, i);
        }
        
        // FAST - no resizes
        Map<Integer, Integer> map2 = new HashMap<>(100000 * 4 / 3);
        for (int i = 0; i < 100000; i++) {
            map2.put(i, i);
        }
    }
}
```

---

## Summary

**Internal Structure:**

```
HashMap = Array + LinkedList/Tree
- Array of buckets (power of 2)
- Each bucket: List (< 8) or Tree (≥ 8)
- Hash function: hash(key) ^ (hash >>> 16)
- Bucket index: hash & (capacity - 1)
```

**Time Complexity:**

```
Average: O(1)
Worst (Java 7): O(n)
Worst (Java 8+): O(log n)
```

**Critical Numbers:**

```
Capacity: 16 (default), always power of 2
Load factor: 0.75 (default)
Treeify threshold: 8
Resize: size > capacity * loadFactor
```

**Best Practices:**

1. ✅ Override equals and hashCode together
2. ✅ Use immutable keys
3. ✅ Size appropriately
4. ✅ Use ConcurrentHashMap for threading
5. ❌ Don't modify keys after insertion
6. ❌ Don't use in multithreaded code
7. ❌ Don't ignore sizing

---
