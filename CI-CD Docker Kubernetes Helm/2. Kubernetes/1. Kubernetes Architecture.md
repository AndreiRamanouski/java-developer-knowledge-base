
## Overview

Kubernetes (K8s) is an open-source container orchestration platform that automates deployment, scaling, and management of containerized applications.

**Key Concepts:**

- **Cluster**: A set of machines (nodes) running Kubernetes
- **Control Plane**: The brain of Kubernetes (manages the cluster)
- **Worker Nodes**: Machines that run containerized applications
- **Pod**: The smallest deployable unit (one or more containers)

**Why Kubernetes?**

- ✅ Automated container deployment and scaling
- ✅ Self-healing (restarts failed containers)
- ✅ Service discovery and load balancing
- ✅ Automated rollouts and rollbacks
- ✅ Secret and configuration management
- ✅ Storage orchestration
- ✅ Horizontal scaling

---

## Kubernetes Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                        CONTROL PLANE                             │
│  ┌────────────┐  ┌────────────┐  ┌──────────────────────────┐  │
│  │ API Server │  │    etcd    │  │  Controller Manager      │  │
│  │            │  │            │  │  - Node Controller       │  │
│  │  Gateway   │◄─┤ Data Store │  │  - Replication Ctrl      │  │
│  │  for all   │  │ (key-value)│  │  - Endpoints Controller  │  │
│  │  operations│  └────────────┘  │  - ServiceAccount Ctrl   │  │
│  └────────────┘                  └──────────────────────────┘  │
│         ▲                                     ▲                 │
│         │                                     │                 │
│  ┌──────┴───────┐              ┌─────────────┴──────────┐      │
│  │  Scheduler   │              │ Cloud Controller Mgr   │      │
│  │              │              │ (optional)             │      │
│  │ Pod → Node   │              │                        │      │
│  │ placement    │              │ - Node Controller      │      │
│  └──────────────┘              │ - Route Controller     │      │
│                                │ - Service Controller   │      │
│                                └────────────────────────┘      │
└─────────────────────────────────────────────────────────────────┘
                           │
                           │ (communicates via API Server)
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│                        WORKER NODES                              │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Node 1                                                    │  │
│  │  ┌──────────┐  ┌────────────┐  ┌───────────────────┐   │  │
│  │  │ kubelet  │  │ kube-proxy │  │ Container Runtime │   │  │
│  │  │          │  │            │  │ (containerd/CRI-O)│   │  │
│  │  │ Node     │  │ Network    │  │                   │   │  │
│  │  │ Agent    │  │ Proxy      │  │                   │   │  │
│  │  └──────────┘  └────────────┘  └───────────────────┘   │  │
│  │       │              │                    │              │  │
│  │  ┌────▼──────────────▼────────────────────▼──────────┐  │  │
│  │  │              PODS (Containers)                    │  │  │
│  │  │  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐         │  │  │
│  │  │  │ Pod1 │  │ Pod2 │  │ Pod3 │  │ Pod4 │         │  │  │
│  │  │  └──────┘  └──────┘  └──────┘  └──────┘         │  │  │
│  │  └────────────────────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Node 2, Node 3, Node N... (same structure)               │  │
│  └───────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

**Key Principle: Declarative Configuration**

- You declare desired state (e.g., "run 3 replicas of this app")
- Kubernetes continuously works to maintain that state
- Controllers watch for changes and take corrective action

---

## 1. Control Plane Components

The control plane makes global decisions about the cluster and responds to cluster events.

### API Server (kube-apiserver)

The **API Server** is the front-end for the Kubernetes control plane. ALL communication goes through it.

#### What API Server Does

**Core Responsibilities:**

1. **Gateway**: Single entry point for all cluster operations
2. **Authentication**: Verifies who you are
3. **Authorization**: Verifies what you can do
4. **Admission Control**: Validates and modifies requests
5. **Persistence**: Saves data to etcd
6. **Watches**: Streams changes to clients

**Communication Flow:**

```
kubectl                  Controller Manager
   │                            │
   ▼                            ▼
┌──────────────────────────────────┐
│        API Server                │
│  ┌──────────────────────────┐   │
│  │ 1. Authentication        │   │
│  │ 2. Authorization         │   │
│  │ 3. Admission Control     │   │
│  │ 4. Validation            │   │
│  │ 5. Persistence (etcd)    │   │
│  │ 6. Response              │   │
│  └──────────────────────────┘   │
└──────────────────────────────────┘
           │
           ▼
        etcd
```

#### How API Server Works

**Request Processing:**

```
1. Client sends request (kubectl, controller, kubelet)
   Example: kubectl create deployment nginx --image=nginx

2. API Server receives HTTP/HTTPS request
   POST /apis/apps/v1/namespaces/default/deployments

3. Authentication
   - Validates client certificate
   - OR validates bearer token
   - OR validates service account token

4. Authorization
   - RBAC: Does user have permission?
   - Check: Can this user create deployments in default namespace?

5. Admission Control
   - Mutating Admission: Modify request (add defaults, inject sidecars)
   - Validating Admission: Validate request (security policies)

6. Validation
   - Schema validation (is this a valid deployment spec?)
   - Business logic validation

7. Persistence
   - Save to etcd: /registry/deployments/default/nginx

8. Response
   - Return created object to client
   - Notify watchers (controllers, kubectl wait)
```

**Example API Server Request:**

```bash
# What kubectl does internally:
curl -X POST https://kubernetes:6443/apis/apps/v1/namespaces/default/deployments \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "apiVersion": "apps/v1",
    "kind": "Deployment",
    "metadata": {"name": "nginx"},
    "spec": {
      "replicas": 3,
      "selector": {"matchLabels": {"app": "nginx"}},
      "template": {
        "metadata": {"labels": {"app": "nginx"}},
        "spec": {
          "containers": [{
            "name": "nginx",
            "image": "nginx:1.25"
          }]
        }
      }
    }
  }'
```

#### API Server Features

**1. Watch Mechanism:**

```bash
# Controllers watch for changes
# Equivalent to: kubectl get pods --watch

GET /api/v1/namespaces/default/pods?watch=true

# API Server keeps connection open
# Sends events as they occur:
{
  "type": "ADDED",
  "object": { "kind": "Pod", "metadata": { "name": "nginx-abc123" }}
}
{
  "type": "MODIFIED",
  "object": { "kind": "Pod", "metadata": { "name": "nginx-abc123" }}
}
{
  "type": "DELETED",
  "object": { "kind": "Pod", "metadata": { "name": "nginx-abc123" }}
}
```

**2. Resource Versions (Optimistic Concurrency):**

```yaml
# Every object has a resourceVersion
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  resourceVersion: "12345"  # Incremented on every change

# Update requires matching resourceVersion
# Prevents conflicting updates
```

**3. Aggregation Layer:**

```
API Server can aggregate multiple API servers:
- Core APIs (pods, services, etc.)
- Custom Resource Definitions (CRDs)
- API Extensions (metrics server)

kubectl get --raw /apis | jq
# Shows all available API groups
```

#### API Server Configuration

**Key Flags:**

```bash
kube-apiserver \
  --etcd-servers=https://10.0.0.1:2379 \
  --service-cluster-ip-range=10.96.0.0/12 \
  --authorization-mode=Node,RBAC \
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,PodSecurityPolicy \
  --tls-cert-file=/etc/kubernetes/pki/apiserver.crt \
  --tls-private-key-file=/etc/kubernetes/pki/apiserver.key \
  --client-ca-file=/etc/kubernetes/pki/ca.crt \
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
```

---

### etcd (Distributed Key-Value Store)

**etcd** is Kubernetes' database. It stores the entire cluster state.

#### What etcd Stores

**All cluster data:**

```
/registry/
├── pods/
│   ├── default/
│   │   ├── nginx-abc123
│   │   └── redis-def456
│   └── kube-system/
│       └── coredns-ghi789
├── services/
│   └── default/
│       └── kubernetes
├── deployments/
│   └── default/
│       └── nginx
├── configmaps/
├── secrets/
├── nodes/
└── ... (everything else)
```

**Example data in etcd:**

```bash
# View what's in etcd (if you have access)
ETCDCTL_API=3 etcdctl get /registry/pods/default/nginx-abc123 \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Returns binary protobuf data (Kubernetes object)
```

#### How etcd Works

**Raft Consensus Algorithm:**

```
etcd uses Raft for distributed consensus

1. Leader Election:
   - One node is leader, others are followers
   - Leader handles all writes
   - Followers replicate data

2. Write Process:
   ┌──────────┐         ┌──────────┐         ┌──────────┐
   │  Leader  │────────▶│Follower 1│         │Follower 2│
   │  (Write) │         │ (Ack)    │         │ (Ack)    │
   └──────────┘         └──────────┘         └──────────┘
        │                      │                     │
        └──────────────────────┴─────────────────────┘
                    Committed when quorum reached
                    (majority of nodes acknowledge)

3. Read Process:
   - Reads can come from any node
   - Linearizable reads (consistent)
   - Or serializable reads (faster, may be stale)
```

**Quorum Requirement:**

```
Cluster Size | Quorum | Tolerated Failures
-------------|--------|-------------------
     1       |   1    |   0
     3       |   2    |   1  (recommended minimum)
     5       |   3    |   2  (recommended production)
     7       |   4    |   3
     9       |   5    |   4
```

**Why odd numbers?**

- 4 nodes: quorum = 3, failures tolerated = 1 (same as 3 nodes!)
- 5 nodes: quorum = 3, failures tolerated = 2 (better!)
- More nodes = more overhead, diminishing returns after 7

#### etcd Best Practices

**1. Always use 3 or 5 nodes in production:**

```yaml
# etcd cluster setup
etcd-1: 10.0.0.1
etcd-2: 10.0.0.2
etcd-3: 10.0.0.3

# Each node configured with:
--initial-cluster=etcd-1=https://10.0.0.1:2380,etcd-2=https://10.0.0.2:2380,etcd-3=https://10.0.0.3:2380
```

**2. Use fast SSD storage:**

```bash
# etcd is latency-sensitive
# Requirements:
- SSD storage (not HDD!)
- <10ms disk latency
- High IOPS

# Monitor disk performance:
etcdctl check perf
```

**3. Regular backups:**

```bash
# Backup etcd
ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Verify backup
ETCDCTL_API=3 etcdctl snapshot status snapshot.db

# Restore etcd (disaster recovery)
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir=/var/lib/etcd-restored
```

**4. Monitor etcd health:**

```bash
# Check cluster health
ETCDCTL_API=3 etcdctl endpoint health \
  --endpoints=https://10.0.0.1:2379,https://10.0.0.2:2379,https://10.0.0.3:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt

# Check member list
ETCDCTL_API=3 etcdctl member list

# Key metrics to watch:
- Leader changes (should be rare)
- Disk sync duration (should be <10ms)
- Database size (compact when large)
```

#### etcd Compaction and Defragmentation

**Why needed:**

```
etcd keeps history of all changes (for watch mechanism)
Over time, this grows and slows down performance

Solution: Compaction + Defragmentation
```

**Automatic compaction:**

```bash
kube-apiserver \
  --etcd-compaction-interval=5m
# Removes history older than 5 minutes
```

**Manual defragmentation:**

```bash
# Defragment to reclaim space
ETCDCTL_API=3 etcdctl defrag \
  --endpoints=https://10.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt

# Check database size
ETCDCTL_API=3 etcdctl endpoint status --write-out=table
```

---

### Scheduler (kube-scheduler)

The **Scheduler** decides which node each pod should run on.

#### What Scheduler Does

**Core Responsibility:** Watch for newly created pods with no assigned node, and assign them to a node based on:

- Resource requirements (CPU, memory)
- Hardware constraints (node labels, node affinity)
- Software constraints (pod affinity, anti-affinity)
- Data locality
- Resource availability
- Taints and tolerations

#### Scheduling Process

**Two-Phase Process:**

**Phase 1: Filtering (Predicates)**

```
Filter out nodes that can't run the pod

Checks:
1. Does node have enough CPU? (pod requests 2 CPU, node has 1 available) ❌
2. Does node have enough memory? (pod requests 4Gi, node has 8Gi available) ✅
3. Does pod tolerate node taints? (node tainted NoSchedule) ❌
4. Does pod selector match node labels? (pod wants gpu=true) ❌
5. Are required ports available? (pod uses port 80, already in use) ❌

Result: List of feasible nodes
Example: nodes [node-1, node-3, node-5] can run this pod
```

**Phase 2: Scoring (Priorities)**

```
Rank feasible nodes to find the best one

Scoring plugins:
1. LeastRequestedPriority: Prefer nodes with more available resources
   - node-1: 20% CPU used → score: 80
   - node-3: 50% CPU used → score: 50
   - node-5: 10% CPU used → score: 90 ✅ (highest)

2. BalancedResourceAllocation: Prefer balanced CPU/memory usage
   - node-1: 30% CPU, 60% memory → imbalanced → score: 50
   - node-5: 40% CPU, 40% memory → balanced → score: 90 ✅

3. ImageLocalityPriority: Prefer nodes with image already pulled
   - node-5: has nginx:1.25 image → score: +10

4. InterPodAffinityPriority: Prefer nodes near related pods

Final scores:
- node-1: 60
- node-3: 55
- node-5: 95 ✅ (winner!)

Scheduler assigns pod to node-5
```

#### Scheduling Example

```yaml
# Pod definition
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    resources:
      requests:
        cpu: "500m"
        memory: "512Mi"
      limits:
        cpu: "1"
        memory: "1Gi"
  nodeSelector:
    disktype: ssd
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: In
            values:
            - us-west-1a
            - us-west-1b
```

**What scheduler does:**

```
1. Watch API Server for pods with no nodeName

2. Filter nodes:
   ✓ node-1: 2 CPU available, 4Gi memory, disktype=ssd, zone=us-west-1a
   ✗ node-2: 0.3 CPU available (not enough!)
   ✓ node-3: 1 CPU available, 2Gi memory, disktype=ssd, zone=us-west-1b
   ✗ node-4: disktype=hdd (doesn't match!)

3. Score nodes:
   node-1: score 85
   node-3: score 78

4. Bind pod to node-1:
   PATCH /api/v1/namespaces/default/pods/nginx
   {
     "spec": {
       "nodeName": "node-1"
     }
   }

5. API Server persists to etcd
6. kubelet on node-1 notices pod assigned to it
7. kubelet starts containers
```

#### Advanced Scheduling

**1. Node Affinity:**

```yaml
# Prefer nodes with SSD, require specific zones
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: topology.kubernetes.io/zone
          operator: In
          values: ["us-west-1a", "us-west-1b"]
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 80
      preference:
        matchExpressions:
        - key: disktype
          operator: In
          values: ["ssd"]
```

**2. Pod Affinity/Anti-Affinity:**

```yaml
# Run near other nginx pods (for performance)
affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values: ["nginx"]
      topologyKey: kubernetes.io/hostname

# Don't run near other database pods (for reliability)
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchExpressions:
        - key: app
          operator: In
          values: ["database"]
      topologyKey: topology.kubernetes.io/zone
```

**3. Taints and Tolerations:**

```bash
# Taint node (prevent scheduling unless tolerated)
kubectl taint nodes node-1 gpu=true:NoSchedule

# Only pods with matching toleration can schedule
tolerations:
- key: "gpu"
  operator: "Equal"
  value: "true"
  effect: "NoSchedule"
```

**4. Priority and Preemption:**

```yaml
# High priority pod can evict low priority pods if needed
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  # If no resources available, evict lower priority pods

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "For critical applications"
```

---

### Controller Manager (kube-controller-manager)

The **Controller Manager** runs controllers that maintain desired state.

#### What is a Controller?

**Control Loop Pattern:**

```
┌─────────────────────────────────────────┐
│                                         │
│  1. Watch current state (via API)      │
│         │                               │
│         ▼                               │
│  2. Compare with desired state          │
│         │                               │
│         ▼                               │
│  3. If different, take action           │
│         │                               │
│         ▼                               │
│  4. Update current state                │
│         │                               │
│         └──────────┐                    │
│                    │                    │
└────────────────────┼────────────────────┘
                     │
                     └─── Loop forever (reconciliation)
```

**Example: Deployment Controller**

```
Desired State: 3 replicas of nginx
Current State: 2 replicas running

Controller Action:
1. Detects difference (3 desired, 2 current)
2. Creates 1 new pod
3. Waits for pod to be running
4. Repeat forever (continuous reconciliation)
```

#### Built-in Controllers

**1. Node Controller:**

```
Responsibilities:
- Monitors node health
- Evicts pods from unhealthy nodes
- Updates node status

Process:
1. Checks node heartbeat every 5 seconds
2. If node hasn't reported for 40 seconds → ConditionUnknown
3. If ConditionUnknown for 5 minutes → Start eviction
4. Moves pods to other healthy nodes
```

**2. Replication Controller / ReplicaSet Controller:**

```
Responsibilities:
- Maintains desired number of pod replicas
- Creates new pods if too few
- Deletes pods if too many

Example:
Desired: 5 replicas
Current: 3 replicas
Action: Create 2 new pods

Desired: 5 replicas
Current: 7 replicas
Action: Delete 2 pods (oldest first)
```

**3. Deployment Controller:**

```
Responsibilities:
- Manages ReplicaSets
- Handles rolling updates
- Handles rollbacks

Process:
1. User updates deployment: nginx:1.24 → nginx:1.25
2. Controller creates new ReplicaSet with nginx:1.25
3. Scales up new ReplicaSet: 0 → 1 → 2 → 3
4. Scales down old ReplicaSet: 3 → 2 → 1 → 0
5. Keeps old ReplicaSet for rollback
```

**4. StatefulSet Controller:**

```
Responsibilities:
- Manages stateful applications
- Provides stable pod identities
- Ordered deployment and scaling

Example:
Pods: database-0, database-1, database-2
- Created in order: 0 → 1 → 2
- Deleted in reverse: 2 → 1 → 0
- Each has persistent storage
- Each has stable DNS name
```

**5. DaemonSet Controller:**

```
Responsibilities:
- Ensures pod runs on all (or selected) nodes
- Adds pod when new node joins
- Removes pod when node is removed

Use cases:
- Log collectors (run on every node)
- Monitoring agents (run on every node)
- Network plugins (run on every node)
```

**6. Job Controller:**

```
Responsibilities:
- Runs pods to completion
- Retries failed pods
- Tracks completion status

Example:
Job: Run database backup
- Creates pod
- Pod runs backup script
- Pod exits successfully
- Job marked as complete
```

**7. CronJob Controller:**

```
Responsibilities:
- Schedules jobs on cron schedule
- Creates job at specified time

Example:
CronJob: "0 2 * * *" (daily at 2 AM)
- At 2:00 AM: Creates Job
- Job runs backup
- Next day at 2:00 AM: Creates new Job
```

**8. Service Controller:**

```
Responsibilities:
- Manages service endpoints
- Configures load balancers (cloud environments)
- Updates endpoint list when pods change

Example:
Service: nginx
- Watches pods with label app=nginx
- Finds: 3 pods with IPs 10.1.1.1, 10.1.1.2, 10.1.1.3
- Creates endpoints list
- Updates load balancer to route to these IPs
```

**9. Namespace Controller:**

```
Responsibilities:
- Deletes all resources in namespace when namespace deleted
- Prevents deletion until all resources removed

Process:
1. User deletes namespace: kubectl delete ns myapp
2. Namespace marked for deletion (finalizers)
3. Controller deletes all resources in namespace
4. After all resources deleted, namespace removed
```

**10. ServiceAccount Controller:**

```
Responsibilities:
- Creates default service account in each namespace
- Creates tokens for service accounts

Example:
New namespace "myapp" created:
1. Controller detects new namespace
2. Creates service account: default
3. Creates secret with token for default service account
4. All pods in namespace can use this service account
```

**11. Endpoints Controller:**

```
Responsibilities:
- Updates endpoints when pods change
- Ensures service points to correct pods

Example:
Service: api-service (selector: app=api)
Pods: api-1 (10.1.1.1), api-2 (10.1.1.2), api-3 (10.1.1.3)

Endpoints object:
- 10.1.1.1:8080
- 10.1.1.2:8080
- 10.1.1.3:8080

When api-2 dies:
- Controller removes 10.1.1.2 from endpoints
- Service routes only to api-1 and api-3
```

#### Controller Configuration

```bash
kube-controller-manager \
  --node-monitor-period=5s \
  --node-monitor-grace-period=40s \
  --pod-eviction-timeout=5m \
  --controllers=*,bootstrapsigner,tokencleaner \
  --leader-elect=true \
  --use-service-account-credentials=true
```

---

### Cloud Controller Manager

The **Cloud Controller Manager** integrates with cloud provider APIs.

#### What Cloud Controller Manager Does

**Separates cloud-specific logic from Kubernetes core:**

```
Old way (pre-1.6):
Kubernetes core code contained:
- AWS-specific code
- GCP-specific code
- Azure-specific code
→ Hard to maintain, slow to add new clouds

New way (1.6+):
Cloud Controller Manager:
- Separate binary per cloud provider
- Pluggable architecture
- Easier to maintain
```

#### Cloud Controllers

**1. Node Controller:**

```
Responsibilities:
- Creates Kubernetes node objects for cloud VMs
- Adds cloud-specific labels (zone, region, instance-type)
- Deletes nodes when cloud VM deleted

Example (AWS):
- EC2 instance created: i-abc123
- Cloud controller creates node object
- Adds labels:
  topology.kubernetes.io/zone: us-east-1a
  node.kubernetes.io/instance-type: t3.medium
  topology.kubernetes.io/region: us-east-1
```

**2. Route Controller:**

```
Responsibilities:
- Configures routes in cloud network
- Enables pod-to-pod communication across nodes

Example (GCP):
- Pod on node-1 (10.244.1.0/24) needs to reach pod on node-2 (10.244.2.0/24)
- Cloud controller creates VPC route:
  Destination: 10.244.2.0/24
  Next hop: node-2's internal IP
```

**3. Service Controller:**

```
Responsibilities:
- Provisions cloud load balancers for LoadBalancer services
- Configures load balancer backend pools
- Returns load balancer IP to service

Example (AWS):
Service:
  type: LoadBalancer
  selector:
    app: nginx

Cloud controller:
1. Creates ELB (Elastic Load Balancer)
2. Adds EC2 instances (nodes) to target group
3. Configures health checks
4. Returns ELB DNS name
5. Updates service status with LoadBalancer IP
```

**4. Volume Controller:**

```
Responsibilities:
- Attaches/detaches cloud volumes to/from nodes
- Creates cloud disks for PersistentVolumes

Example (Azure):
PersistentVolumeClaim: 10Gi disk
1. Cloud controller creates Azure Managed Disk
2. Attaches disk to VM (node)
3. kubelet mounts disk into pod
```

#### Cloud Provider Examples

**AWS Cloud Provider:**

```yaml
# kube-apiserver flag
--cloud-provider=aws

# Cloud Controller Manager deployment
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: aws-cloud-controller-manager
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: aws-cloud-controller-manager
  template:
    metadata:
      labels:
        k8s-app: aws-cloud-controller-manager
    spec:
      containers:
      - name: aws-cloud-controller-manager
        image: gcr.io/k8s-staging-provider-aws/cloud-controller-manager:latest
        command:
        - /bin/aws-cloud-controller-manager
        - --cloud-provider=aws
        - --configure-cloud-routes=true
```

**GCP Cloud Provider:**

```yaml
# Similar structure, different provider
command:
- /gce-cloud-controller-manager
- --cloud-provider=gce
- --configure-cloud-routes=true
```

**Azure Cloud Provider:**

```yaml
command:
- /azure-cloud-controller-manager
- --cloud-provider=azure
- --configure-cloud-routes=true
```

---

## 2. Node Components

### kubelet (Node Agent)

The **kubelet** is the primary node agent that runs on each worker node.

#### What kubelet Does

**Core Responsibilities:**

1. **Registers node** with API server
2. **Watches** for pods assigned to its node
3. **Starts containers** via container runtime
4. **Monitors** pod and container health
5. **Reports** pod status back to API server
6. **Executes** pod lifecycle hooks
7. **Manages** volumes (mounts/unmounts)

#### How kubelet Works

```
┌─────────────────────────────────────────────┐
│              kubelet Process                 │
├─────────────────────────────────────────────┤
│                                             │
│  1. Watch API Server for pods assigned     │
│     to this node                            │
│                                             │
│  2. Pull container images                  │
│                                             │
│  3. Call Container Runtime (containerd)    │
│     to start containers                     │
│                                             │
│  4. Monitor container status               │
│                                             │
│  5. Report status to API Server            │
│     (pod phase, container state)            │
│                                             │
│  6. Run health checks (liveness, readiness)│
│                                             │
│  7. Restart failed containers              │
│                                             │
└─────────────────────────────────────────────┘
         │                          ▲
         ▼                          │
┌──────────────────┐     ┌──────────────────┐
│ Container Runtime│     │   API Server     │
│  (containerd)    │     │                  │
└──────────────────┘     └──────────────────┘
```

#### kubelet Process Flow

**Complete Pod Lifecycle:**

```
1. API Server assigns pod to node
   Pod: nginx
   Node: worker-1
   nodeName: worker-1

2. kubelet on worker-1 notices new pod
   (via watch on API server)

3. kubelet pulls container image
   docker pull nginx:1.25
   (via Container Runtime Interface)

4. kubelet creates pod sandbox (pause container)
   Network namespace created
   IP address assigned: 10.244.1.5

5. kubelet starts init containers (if any)
   Init container runs to completion

6. kubelet starts main containers
   Container: nginx
   Command: nginx -g 'daemon off;'

7. kubelet reports pod status
   Phase: Running
   Container state: Running
   Started at: 2024-11-14T10:30:00Z

8. kubelet monitors container
   Every 10s: Check if container running
   Every 10s: Run liveness probe
   Every 10s: Run readiness probe

9. If container crashes:
   kubelet restarts container automatically
   Reports status: CrashLoopBackOff

10. When pod deleted:
    kubelet stops containers
    Removes pod sandbox
    Reports status: Terminated
```

#### kubelet Configuration

**Key flags:**

```bash
kubelet \
  --config=/var/lib/kubelet/config.yaml \
  --kubeconfig=/etc/kubernetes/kubelet.conf \
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \
  --pod-manifest-path=/etc/kubernetes/manifests \
  --node-ip=10.0.0.10 \
  --cluster-dns=10.96.0.10 \
  --cluster-domain=cluster.local \
  --cgroup-driver=systemd
```

**kubelet config file:**

```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
authorization:
  mode: Webhook
clusterDomain: cluster.local
clusterDNS:
  - 10.96.0.10
podCIDR: 10.244.1.0/24
containerLogMaxSize: 10Mi
containerLogMaxFiles: 5
maxPods: 110
cgroupDriver: systemd
```

#### Static Pods

**kubelet can run pods without API server:**

```bash
# Place manifest in /etc/kubernetes/manifests/
# kubelet automatically creates and manages pod

cat > /etc/kubernetes/manifests/nginx.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx-static
spec:
  containers:
  - name: nginx
    image: nginx:1.25
EOF

# kubelet creates pod immediately
# Pod appears in kubectl but cannot be deleted via kubectl
# (Must delete manifest file to stop pod)
```

**Use case: Control plane components**

```bash
# Control plane components run as static pods:
/etc/kubernetes/manifests/
├── kube-apiserver.yaml
├── kube-controller-manager.yaml
├── kube-scheduler.yaml
└── etcd.yaml

# This allows control plane to run even if API server is down
```

---

### kube-proxy (Network Proxy)

The **kube-proxy** maintains network rules on nodes for service communication.

#### What kube-proxy Does

**Core Responsibility:** Implements Kubernetes Service abstraction by maintaining network rules (iptables/IPVS).

**Service Concept:**

```
Problem:
- Pods have dynamic IPs (change when recreated)
- How do clients find backends?

Solution: Service
- Stable virtual IP (ClusterIP)
- DNS name
- kube-proxy routes traffic to backend pods
```

#### How kube-proxy Works

**Three Modes:**

**1. iptables mode (default):**

```
┌──────────────────────────────────────────┐
│            kube-proxy                     │
│                                           │
│  1. Watch services & endpoints           │
│                                           │
│  2. Generate iptables rules              │
│                                           │
│  3. Program iptables on node             │
│                                           │
└──────────────────────────────────────────┘
         │
         ▼
┌──────────────────────────────────────────┐
│         iptables rules                    │
│                                           │
│  Rule 1: Traffic to 10.96.0.1:80        │
│          → Randomly select:               │
│            - 10.244.1.5:8080 (33%)       │
│            - 10.244.2.6:8080 (33%)       │
│            - 10.244.3.7:8080 (33%)       │
│                                           │
└──────────────────────────────────────────┘
```

**Example iptables rules:**

```bash
# Service: nginx (ClusterIP: 10.96.0.1)
# Endpoints: 10.244.1.5, 10.244.2.6, 10.244.3.7

iptables -t nat -A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-NGINX

iptables -t nat -A KUBE-SVC-NGINX -m statistic --mode random --probability 0.33333 -j KUBE-SEP-1
iptables -t nat -A KUBE-SVC-NGINX -m statistic --mode random --probability 0.50000 -j KUBE-SEP-2
iptables -t nat -A KUBE-SVC-NGINX -j KUBE-SEP-3

iptables -t nat -A KUBE-SEP-1 -p tcp -m tcp -j DNAT --to-destination 10.244.1.5:8080
iptables -t nat -A KUBE-SEP-2 -p tcp -m tcp -j DNAT --to-destination 10.244.2.6:8080
iptables -t nat -A KUBE-SEP-3 -p tcp -m tcp -j DNAT --to-destination 10.244.3.7:8080
```

**Traffic flow:**

```
1. Application connects to service: curl http://10.96.0.1:80
2. iptables intercepts packet
3. Randomly selects backend (round-robin)
4. DNATs packet to backend pod
5. Response returns directly to client
```

**2. IPVS mode (more scalable):**

```
Uses Linux IPVS (IP Virtual Server) instead of iptables
Better performance with many services (>1000)

Benefits:
- More efficient (uses hash table vs linear search)
- More load balancing algorithms:
  - Round Robin
  - Least Connection
  - Source Hashing
  - Shortest Expected Delay

Configuration:
--proxy-mode=ipvs
```

**3. userspace mode (legacy, not recommended):**

```
kube-proxy itself proxies traffic (not kernel)
Very slow, high overhead
Only for old systems
```

#### kube-proxy Configuration

```yaml
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "iptables"  # or "ipvs"
clusterCIDR: "10.244.0.0/16"
ipvs:
  scheduler: "rr"  # round-robin
  # Other options: lc (least connection), sh (source hashing)
iptables:
  masqueradeAll: false
  masqueradeBit: 14
  minSyncPeriod: 1s
  syncPeriod: 30s
```

---

### Container Runtime (containerd, CRI-O)

The **container runtime** actually runs containers on the node.

#### Container Runtime Interface (CRI)

**Standardized API between kubelet and container runtime:**

```
┌──────────────┐         CRI          ┌──────────────┐
│   kubelet    │ ◄───────────────────►│  Container   │
│              │      (gRPC API)      │   Runtime    │
└──────────────┘                      └──────────────┘
                                             │
                                             ▼
                                      ┌──────────────┐
                                      │  Containers  │
                                      └──────────────┘
```

**CRI operations:**

```protobuf
service RuntimeService {
  // Sandbox (pod) management
  rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse);
  rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse);
  rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse);

  // Container management
  rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse);
  rpc StartContainer(StartContainerRequest) returns (StartContainerResponse);
  rpc StopContainer(StopContainerRequest) returns (StopContainerResponse);
  rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse);

  // Container inspection
  rpc ListContainers(ListContainersRequest) returns (ListContainersResponse);
  rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse);
}
```

#### containerd

**Most popular container runtime (graduated CNCF project):**

**Architecture:**

```
┌──────────────────────────────────────────┐
│           kubelet (via CRI)              │
└────────────────┬─────────────────────────┘
                 │ gRPC
                 ▼
┌──────────────────────────────────────────┐
│           containerd                      │
│  ┌────────────────────────────────────┐  │
│  │   containerd-shim-runc-v2          │  │
│  └────────────────────────────────────┘  │
└────────────────┬─────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────┐
│              runc                         │
│  (creates and runs containers)           │
└────────────────┬─────────────────────────┘
                 │
                 ▼
┌──────────────────────────────────────────┐
│         Linux Kernel                      │
│  (namespaces, cgroups)                   │
└──────────────────────────────────────────┘
```

**Configuration:**

```bash
# kubelet points to containerd socket
kubelet --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock

# containerd config
cat /etc/containerd/config.toml
version = 2

[plugins."io.containerd.grpc.v1.cri"]
  [plugins."io.containerd.grpc.v1.cri".containerd]
    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      runtime_type = "io.containerd.runc.v2"
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true
```

**crictl (CLI tool for debugging):**

```bash
# List pods
crictl pods

# List containers
crictl ps

# Inspect container
crictl inspect <container-id>

# View logs
crictl logs <container-id>

# Execute command
crictl exec -it <container-id> sh
```

#### CRI-O

**Lightweight alternative to containerd:**

**Features:**

- Built specifically for Kubernetes
- Minimal overhead
- OCI-compliant
- Used by OpenShift

**Configuration:**

```bash
# kubelet points to CRI-O socket
kubelet --container-runtime-endpoint=unix:///var/run/crio/crio.sock

# CRI-O config
cat /etc/crio/crio.conf
[crio.runtime]
default_runtime = "runc"
conmon = "/usr/bin/conmon"
cgroup_manager = "systemd"
```

#### Runtime Comparison

|Feature|containerd|CRI-O|Docker (deprecated)|
|---|---|---|---|
|**Primary use**|General purpose|Kubernetes-specific|General purpose|
|**Footprint**|Medium|Small|Large|
|**Maturity**|High (CNCF graduated)|High|High|
|**k8s support**|Native CRI|Native CRI|Via dockershim (removed in 1.24)|
|**CLI tool**|crictl|crictl|docker|
|**OCI-compliant**|Yes|Yes|Yes|

---

## 3. How They Work Together

### Request Flow (kubectl → API server → etcd → scheduler → kubelet)

**Complete flow for creating a deployment:**

```
┌──────────────────────────────────────────────────────────────────┐
│                    STEP-BY-STEP FLOW                              │
└──────────────────────────────────────────────────────────────────┘

1. User: kubectl create deployment nginx --image=nginx:1.25 --replicas=3
   ├─ kubectl converts to API call
   └─ POST /apis/apps/v1/namespaces/default/deployments

2. API Server:
   ├─ Authenticates user (certificate, token)
   ├─ Authorizes request (RBAC: can user create deployments?)
   ├─ Runs admission controllers (mutating + validating)
   ├─ Validates deployment spec
   ├─ Persists to etcd: /registry/deployments/default/nginx
   └─ Returns HTTP 201 Created

3. Deployment Controller (watching API server):
   ├─ Notices new deployment
   ├─ Reads: replicas=3, image=nginx:1.25
   ├─ Creates ReplicaSet
   └─ POST /apis/apps/v1/namespaces/default/replicasets

4. API Server:
   ├─ Validates ReplicaSet
   ├─ Persists to etcd: /registry/replicasets/default/nginx-abc123
   └─ Returns HTTP 201 Created

5. ReplicaSet Controller (watching API server):
   ├─ Notices new ReplicaSet
   ├─ Reads: replicas=3
   ├─ Current replicas: 0
   ├─ Needs to create: 3 pods
   ├─ Creates 3 pod objects (no nodeName yet)
   └─ POST /api/v1/namespaces/default/pods (x3)

6. API Server:
   ├─ Validates each pod spec
   ├─ Persists to etcd:
   │  ├─ /registry/pods/default/nginx-abc123-pod1
   │  ├─ /registry/pods/default/nginx-abc123-pod2
   │  └─ /registry/pods/default/nginx-abc123-pod3
   └─ Returns HTTP 201 Created (x3)

7. Scheduler (watching API server):
   ├─ Notices 3 pods with no nodeName
   ├─ For each pod:
   │  ├─ Filters nodes (enough resources? correct labels?)
   │  ├─ Scores nodes (which is best?)
   │  ├─ Selects node-1 for pod1
   │  ├─ Selects node-2 for pod2
   │  └─ Selects node-1 for pod3
   └─ PATCH each pod with nodeName

8. API Server:
   ├─ Updates pods with assigned nodes
   └─ Persists to etcd (pod.spec.nodeName = "node-1")

9. kubelet on node-1 (watching API server):
   ├─ Notices 2 pods assigned to node-1
   ├─ For each pod:
   │  ├─ Pulls image: docker pull nginx:1.25
   │  ├─ Creates pod sandbox (network namespace)
   │  ├─ Starts containers via CRI
   │  └─ Reports status to API server
   └─ PATCH pod status: Phase=Running

10. kubelet on node-2 (watching API server):
    ├─ Notices 1 pod assigned to node-2
    ├─ Pulls image
    ├─ Creates pod sandbox
    ├─ Starts container
    └─ Reports status: Phase=Running

11. API Server:
    ├─ Updates pod statuses in etcd
    └─ /registry/pods/default/nginx-abc123-pod1 → Running

12. ReplicaSet Controller (watching API server):
    ├─ Notices all 3 pods are Running
    ├─ Updates ReplicaSet status: readyReplicas=3
    └─ PATCH /apis/apps/v1/namespaces/default/replicasets/nginx-abc123/status

13. Deployment Controller (watching API server):
    ├─ Notices ReplicaSet has 3 ready replicas
    ├─ Updates Deployment status: availableReplicas=3
    └─ PATCH /apis/apps/v1/namespaces/default/deployments/nginx/status

14. kubectl get deployment nginx
    ├─ Queries API server
    ├─ Shows: READY 3/3, UP-TO-DATE 3, AVAILABLE 3
    └─ Done!
```

**Timeline:**

```
t=0s:    User runs kubectl
t=0.1s:  API server persists deployment
t=0.2s:  Deployment controller creates ReplicaSet
t=0.3s:  ReplicaSet controller creates pods
t=0.4s:  Scheduler assigns pods to nodes
t=0.5s:  kubelets notice pods
t=1s:    Image pull starts
t=3s:    Containers starting
t=5s:    Pods Running
t=5.1s:  Status propagates back through controllers
```

---

### Reconciliation Loops

**Core concept: Continuous monitoring and correction**

#### What is Reconciliation?

```
Desired State (etcd):    replicas: 3
Current State (reality): replicas: 2

Reconciliation: Take action to match reality to desired state
Action: Create 1 more pod
```

**Reconciliation is continuous:**

```
while true {
  desiredState = getFromEtcd()
  currentState = observeReality()
  
  if desiredState != currentState {
    takeCorrectiveAction()
  }
  
  sleep(syncPeriod)  // e.g., 30 seconds
}
```

#### Example: Deployment Reconciliation

```yaml
# Desired state
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
```

**Reconciliation scenarios:**

**Scenario 1: Pod deleted manually**

```
Time: t=0
State: 3 pods running (desired: 3)
Status: ✓ OK

Time: t=1
Event: kubectl delete pod nginx-abc123
State: 2 pods running (desired: 3)
Status: ✗ Mismatch

Time: t=2
ReplicaSet Controller reconciliation:
1. Observes: only 2 pods with label app=nginx
2. Compares: desired 3, actual 2
3. Action: Create 1 new pod
4. Result: 3 pods running again

Time: t=3
State: 3 pods running (desired: 3)
Status: ✓ OK (reconciled!)
```

**Scenario 2: Node failure**

```
Time: t=0
State: node-1 (pod1, pod2), node-2 (pod3)
Status: ✓ OK

Time: t=1
Event: node-1 crashes
State: node-1 (unreachable), node-2 (pod3 running)
Status: ✗ Mismatch

Time: t=40s
Node Controller:
1. Detects node-1 hasn't reported heartbeat
2. Marks node-1 as NotReady

Time: t=5m
Node Controller:
1. Node still NotReady after 5 minutes
2. Evicts pods from node-1
3. Deletes pod1, pod2 from API server

Time: t=5m + 1s
ReplicaSet Controller reconciliation:
1. Observes: only 1 pod running (pod3)
2. Compares: desired 3, actual 1
3. Action: Create 2 new pods
4. Scheduler assigns to healthy nodes

Time: t=5m + 10s
State: node-2 (pod3, pod4), node-3 (pod5)
Status: ✓ OK (reconciled!)
```

**Scenario 3: Scaling up**

```
Time: t=0
State: 3 pods running
Status: ✓ OK

Time: t=1
Event: kubectl scale deployment nginx --replicas=5
Action: User updates desired state

Time: t=1.1s
API Server:
1. Validates request
2. Updates deployment.spec.replicas = 5
3. Persists to etcd

Time: t=1.2s
Deployment Controller reconciliation:
1. Observes: deployment changed
2. Updates ReplicaSet: replicas=5

Time: t=1.3s
ReplicaSet Controller reconciliation:
1. Observes: ReplicaSet wants 5 replicas
2. Current: 3 replicas
3. Action: Create 2 new pods

Time: t=2s
Scheduler assigns new pods to nodes

Time: t=5s
kubelets start new containers

Time: t=8s
State: 5 pods running
Status: ✓ OK (reconciled!)
```

#### Multiple Controllers Working Together

```
User scales deployment: 3 → 5 replicas

Deployment Controller:
├─ Watches: Deployments
├─ Action: Update ReplicaSet
└─ Trigger: ReplicaSet controller

ReplicaSet Controller:
├─ Watches: ReplicaSets, Pods
├─ Action: Create 2 new pods
└─ Trigger: Scheduler

Scheduler:
├─ Watches: Pods (with no nodeName)
├─ Action: Assign nodes to pods
└─ Trigger: kubelets

kubelet (node-1):
├─ Watches: Pods (assigned to node-1)
├─ Action: Start container
└─ Result: Pod running

kubelet (node-2):
├─ Watches: Pods (assigned to node-2)
├─ Action: Start container
└─ Result: Pod running

All controllers working independently,
communicating only through API server,
continuously reconciling state!
```

---

### Leader Election

**Problem:** Control plane components are replicated for high availability. How to ensure only one instance is active?

**Solution:** Leader election

#### How Leader Election Works

**Lease-based leader election:**

```
┌─────────────────────────────────────────────────┐
│          Leader Election Process                │
├─────────────────────────────────────────────────┤
│                                                 │
│  Multiple controller-manager instances:         │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │Instance 1│  │Instance 2│  │Instance 3│     │
│  └─────┬────┘  └─────┬────┘  └─────┬────┘     │
│        │             │             │           │
│        │             │             │           │
│        └─────────────┼─────────────┘           │
│                      ▼                         │
│            ┌──────────────────┐                │
│            │   Lease Object   │                │
│            │   in API Server  │                │
│            └──────────────────┘                │
│                                                 │
│  1. All instances try to acquire lease         │
│  2. First one to acquire becomes leader        │
│  3. Leader renews lease every 10 seconds       │
│  4. If leader fails to renew, lease expires    │
│  5. Another instance acquires lease            │
│                                                 │
└─────────────────────────────────────────────────┘
```

**Lease object:**

```yaml
apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  holderIdentity: "controller-manager-1"
  leaseDurationSeconds: 15
  acquireTime: "2024-11-14T10:00:00Z"
  renewTime: "2024-11-14T10:00:10Z"
```

**Process:**

```
Time: t=0
Instance 1: Tries to create lease
Instance 2: Tries to create lease
Instance 3: Tries to create lease

Time: t=0.1s
Instance 1: Successfully creates lease (winner!)
Instance 2: Fails (lease already exists)
Instance 3: Fails (lease already exists)

Time: t=0-15s
Instance 1: Active (doing work)
Instance 2: Standby (waiting)
Instance 3: Standby (waiting)

Time: t=10s
Instance 1: Renews lease (still leader)

Time: t=20s
Instance 1: Renews lease (still leader)

Time: t=30s
Instance 1: Crashes! (no renewal)

Time: t=45s (leaseDuration expired)
Instance 2: Acquires lease (new leader!)
Instance 3: Fails to acquire

Time: t=45s onwards
Instance 2: Active (doing work)
Instance 3: Standby (waiting)
```

#### Components Using Leader Election

**1. kube-controller-manager:**

```bash
kube-controller-manager \
  --leader-elect=true \
  --leader-elect-lease-duration=15s \
  --leader-elect-renew-deadline=10s \
  --leader-elect-retry-period=2s
```

**2. kube-scheduler:**

```bash
kube-scheduler \
  --leader-elect=true \
  --leader-elect-lease-duration=15s
```

**3. Cloud controller manager:**

```bash
cloud-controller-manager \
  --leader-elect=true
```

**Why leader election?**

```
Without leader election:
- 3 controller-managers all active
- All try to reconcile same object
- Create duplicate pods!
- Conflicting operations
- Race conditions

With leader election:
- Only 1 controller-manager active
- Clean, coordinated reconciliation
- No conflicts
- Fast failover if leader dies
```

---

## Summary

### Component Overview Table

|Component|Location|Purpose|Replicas|
|---|---|---|---|
|**API Server**|Control Plane|Gateway for all operations|3+ (HA)|
|**etcd**|Control Plane|Distributed database|3 or 5 (HA)|
|**Scheduler**|Control Plane|Assigns pods to nodes|3+ (HA, 1 active)|
|**Controller Manager**|Control Plane|Maintains desired state|3+ (HA, 1 active)|
|**Cloud Controller**|Control Plane|Cloud provider integration|3+ (HA, 1 active)|
|**kubelet**|Worker Node|Node agent, runs containers|1 per node|
|**kube-proxy**|Worker Node|Network proxy, services|1 per node|
|**Container Runtime**|Worker Node|Runs containers|1 per node|

### Communication Patterns

```
kubectl → API Server
Controllers → API Server (watch + update)
Scheduler → API Server (watch + bind)
kubelet → API Server (watch + update status)
kube-proxy → API Server (watch services)

API Server → etcd (read/write)

kubelet → Container Runtime (via CRI)
Container Runtime → runc → Linux Kernel
```

### Key Takeaways

**Control Plane:**

- **API Server**: Single source of truth, all communication goes through it
- **etcd**: Critical component, requires fast storage and regular backups
- **Scheduler**: Smart placement based on resources and constraints
- **Controllers**: Continuous reconciliation, self-healing system
- **Cloud Controller**: Cloud-specific logic separated from core

**Worker Nodes:**

- **kubelet**: Watches API, runs containers, reports status
- **kube-proxy**: Implements Services via iptables/IPVS
- **Container Runtime**: Actually runs containers (containerd, CRI-O)

**Design Principles:**

1. **Declarative**: Describe desired state, not steps
2. **Reconciliation**: Continuous loops maintain state
3. **Watch**: Efficient change notification
4. **Leader Election**: High availability with single active instance
5. **API-driven**: All components communicate via API server
6. **Self-healing**: Automatically recovers from failures

**High Availability:**

- Multiple API servers (load balanced)
- etcd cluster (3 or 5 nodes)
- Multiple schedulers (1 active via leader election)
- Multiple controller managers (1 active via leader election)
- kubelets on every node (no single point of failure)
