
## 1. Pods

A **Pod** is the smallest deployable unit in Kubernetes. It represents a single instance of a running process in your cluster.

### What is a Pod?

**Key Characteristics:**

- One or more containers that share resources
- Shared network namespace (one IP address)
- Shared storage volumes
- Defined lifecycle
- Ephemeral (pods are cattle, not pets)

**Pod Analogy:**

```
Pod = A group of whales swimming together (whale pod!)
- They travel together (same node)
- They communicate easily (localhost)
- They share resources (volumes)
- If separated, create a new pod
```

**Basic Pod Manifest:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    tier: frontend
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

---

### Pod Lifecycle

Pods go through several phases during their lifetime.

#### Pod Phases

```
┌──────────────────────────────────────────────────────┐
│                  POD LIFECYCLE                        │
├──────────────────────────────────────────────────────┤
│                                                       │
│  ┌─────────┐                                         │
│  │ Pending │  Waiting for scheduling/image pull     │
│  └────┬────┘                                         │
│       │                                              │
│       ▼                                              │
│  ┌─────────┐                                         │
│  │ Running │  At least one container running        │
│  └────┬────┘                                         │
│       │                                              │
│       ├──────────────┬──────────────┐               │
│       ▼              ▼              ▼               │
│  ┌─────────┐   ┌──────────┐   ┌─────────┐          │
│  │Succeeded│   │  Failed  │   │ Unknown │          │
│  └─────────┘   └──────────┘   └─────────┘          │
│   (Completed)   (Error/OOM)    (Node lost)          │
│                                                       │
└──────────────────────────────────────────────────────┘
```

**Phase Details:**

**1. Pending:**

```yaml
status:
  phase: Pending
  conditions:
  - type: PodScheduled
    status: "False"
    reason: Unschedulable
    message: "0/3 nodes available: insufficient cpu"

# Reasons for Pending:
- Waiting for scheduler to assign node
- Pulling container image
- Insufficient resources
- Waiting for volume to attach
```

**2. Running:**

```yaml
status:
  phase: Running
  conditions:
  - type: Ready
    status: "True"
  containerStatuses:
  - name: nginx
    state:
      running:
        startedAt: "2024-11-14T10:00:00Z"
    ready: true
    restartCount: 0

# At least one container is running
# Pod has been bound to node
# All containers have been created
```

**3. Succeeded:**

```yaml
status:
  phase: Succeeded
  containerStatuses:
  - name: job-container
    state:
      terminated:
        exitCode: 0
        finishedAt: "2024-11-14T10:05:00Z"

# All containers terminated with exit code 0
# Typically for Jobs
# Pod will not restart
```

**4. Failed:**

```yaml
status:
  phase: Failed
  containerStatuses:
  - name: app
    state:
      terminated:
        exitCode: 1
        reason: Error
        message: "Application crashed"

# At least one container terminated with non-zero exit code
# OR
# Terminated by system (OOM killed)
```

**5. Unknown:**

```yaml
status:
  phase: Unknown
  message: "Node is not reachable"

# Communication lost with node
# State cannot be determined
# Usually due to node failure
```

#### Pod Conditions

More granular status information:

```yaml
status:
  conditions:
  - type: PodScheduled
    status: "True"
    lastTransitionTime: "2024-11-14T10:00:00Z"
  
  - type: Initialized
    status: "True"
    lastTransitionTime: "2024-11-14T10:00:05Z"
  
  - type: ContainersReady
    status: "True"
    lastTransitionTime: "2024-11-14T10:00:10Z"
  
  - type: Ready
    status: "True"
    lastTransitionTime: "2024-11-14T10:00:10Z"
```

**Condition Types:**

- **PodScheduled**: Pod assigned to node
- **Initialized**: Init containers completed
- **ContainersReady**: All containers ready
- **Ready**: Pod ready to serve traffic

#### Container States

```yaml
containerStatuses:
- name: nginx
  # State 1: Waiting
  state:
    waiting:
      reason: ContainerCreating
      message: "Pulling image nginx:1.25"
  
  # State 2: Running
  state:
    running:
      startedAt: "2024-11-14T10:00:10Z"
  
  # State 3: Terminated
  state:
    terminated:
      exitCode: 0
      reason: Completed
      startedAt: "2024-11-14T10:00:10Z"
      finishedAt: "2024-11-14T10:05:10Z"
```

#### Complete Lifecycle Example

```bash
# Create pod
kubectl apply -f pod.yaml

# Watch lifecycle
kubectl get pod nginx-pod --watch

# Output:
NAME        READY   STATUS              RESTARTS   AGE
nginx-pod   0/1     Pending             0          0s
nginx-pod   0/1     Pending             0          0s   # Scheduled
nginx-pod   0/1     ContainerCreating   0          1s   # Pulling image
nginx-pod   1/1     Running             0          5s   # Started!

# View detailed status
kubectl describe pod nginx-pod

# Events show lifecycle:
Events:
  Type    Reason     Age   Message
  ----    ------     ----  -------
  Normal  Scheduled  30s   Successfully assigned default/nginx-pod to node-1
  Normal  Pulling    29s   Pulling image "nginx:1.25"
  Normal  Pulled     25s   Successfully pulled image
  Normal  Created    25s   Created container nginx
  Normal  Started    24s   Started container nginx
```

---

### Init Containers

**Init containers** run before app containers and must complete successfully.

#### Why Use Init Containers?

**Use cases:**

1. Wait for services to be available
2. Clone git repository
3. Set up configuration files
4. Database migrations
5. Warm up caches
6. Security: Run privileged setup tasks

#### How Init Containers Work

```
┌────────────────────────────────────────────┐
│         POD STARTUP SEQUENCE               │
├────────────────────────────────────────────┤
│                                            │
│  1. Init Container 1 (runs to completion) │
│           ↓                                │
│  2. Init Container 2 (runs to completion) │
│           ↓                                │
│  3. Init Container 3 (runs to completion) │
│           ↓                                │
│  4. Main Container (starts)                │
│           ↓                                │
│  5. Sidecar Containers (start)             │
│                                            │
└────────────────────────────────────────────┘

Key points:
- Init containers run sequentially (one after another)
- Each must succeed before next starts
- If init container fails, pod restarts
- Main containers don't start until ALL init containers succeed
```

#### Init Container Examples

**Example 1: Wait for service availability**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  # Init container - waits for database
  initContainers:
  - name: wait-for-db
    image: busybox:1.36
    command: ['sh', '-c']
    args:
    - |
      until nslookup postgres.default.svc.cluster.local; do
        echo "Waiting for database...";
        sleep 2;
      done;
      echo "Database is ready!";
  
  # Main application container
  containers:
  - name: myapp
    image: myapp:1.0
    env:
    - name: DATABASE_HOST
      value: postgres.default.svc.cluster.local
```

**Example 2: Git clone**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  initContainers:
  - name: git-clone
    image: alpine/git
    command:
    - git
    - clone
    - https://github.com/company/webapp.git
    - /app
    volumeMounts:
    - name: app-code
      mountPath: /app
  
  containers:
  - name: nginx
    image: nginx:1.25
    volumeMounts:
    - name: app-code
      mountPath: /usr/share/nginx/html
  
  volumes:
  - name: app-code
    emptyDir: {}
```

**Example 3: Database migration**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: api-server
spec:
  initContainers:
  - name: db-migration
    image: flyway/flyway:9
    command:
    - flyway
    - migrate
    - -url=jdbc:postgresql://postgres:5432/mydb
    - -user=admin
    - -password=secret
    - -locations=filesystem:/migrations
    volumeMounts:
    - name: migrations
      mountPath: /migrations
  
  containers:
  - name: api
    image: api-server:1.0
  
  volumes:
  - name: migrations
    configMap:
      name: db-migrations
```

**Example 4: Configuration setup**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-app
spec:
  initContainers:
  - name: setup-config
    image: busybox
    command:
    - sh
    - -c
    - |
      cp /config-template/app.conf /config/app.conf
      sed -i "s/ENVIRONMENT/production/g" /config/app.conf
      echo "Config prepared!"
    volumeMounts:
    - name: config-template
      mountPath: /config-template
    - name: config
      mountPath: /config
  
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: config
      mountPath: /etc/app
  
  volumes:
  - name: config-template
    configMap:
      name: app-config-template
  - name: config
    emptyDir: {}
```

#### Init Container Behavior

```bash
# If init container fails, pod restarts
Events:
  Normal  Pulled   10s   Successfully pulled image "busybox"
  Normal  Created  10s   Created init container wait-for-db
  Normal  Started  10s   Started init container wait-for-db
  Warning BackOff  5s    Back-off restarting failed init container

# Restart policy applies to whole pod
spec:
  restartPolicy: Always  # Default: Restart if init fails
  # restartPolicy: Never   # Don't restart
  # restartPolicy: OnFailure  # Only restart if non-zero exit

# View init container logs
kubectl logs pod-name -c init-container-name
```

---

### Sidecar Containers

**Sidecar containers** run alongside main containers throughout pod lifetime.

#### Common Sidecar Patterns

**1. Logging Sidecar:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-with-logging
spec:
  containers:
  # Main application container
  - name: webapp
    image: myapp:1.0
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
  
  # Logging sidecar
  - name: log-shipper
    image: fluent/fluentd:v1.16
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
      readOnly: true
    env:
    - name: FLUENTD_CONF
      value: "fluent.conf"
  
  volumes:
  - name: logs
    emptyDir: {}
```

**2. Proxy Sidecar (Service Mesh):**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-proxy
spec:
  containers:
  # Main application
  - name: app
    image: myapp:1.0
    ports:
    - containerPort: 8080
  
  # Envoy proxy sidecar (Istio pattern)
  - name: istio-proxy
    image: istio/proxyv2:1.20
    ports:
    - containerPort: 15001  # Envoy admin
    - containerPort: 15090  # Envoy metrics
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
```

**3. Metrics Exporter Sidecar:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-metrics
spec:
  containers:
  # Main application
  - name: app
    image: myapp:1.0
  
  # Prometheus exporter sidecar
  - name: metrics-exporter
    image: prom/statsd-exporter:v0.26.0
    ports:
    - containerPort: 9102
      name: metrics
    args:
    - --statsd.mapping-config=/etc/statsd/statsd-mapping.conf
```

**4. Configuration Watcher Sidecar:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-config-watcher
spec:
  containers:
  # Main application
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: config
      mountPath: /etc/app
  
  # Config watcher sidecar
  - name: config-reloader
    image: stakater/reloader:v1.0.0
    volumeMounts:
    - name: config
      mountPath: /etc/app
    # Watches config changes and signals app to reload
  
  volumes:
  - name: config
    configMap:
      name: app-config
```

---

### Multi-Container Patterns

Three classic patterns for multi-container pods:

#### 1. Sidecar Pattern

```
┌────────────────────────────────┐
│            POD                  │
│  ┌──────────┐   ┌───────────┐ │
│  │  Main    │   │  Sidecar  │ │
│  │Container │───│ Container │ │
│  │          │   │           │ │
│  │  App     │   │  Logs     │ │
│  │  Server  │   │  Shipper  │ │
│  └──────────┘   └───────────┘ │
│       │                        │
│       └─ Shared Volume ────────│
└────────────────────────────────┘

Use case: Enhance/extend main container
Examples: Logging, monitoring, security
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-example
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: logs
      mountPath: /var/log/nginx
  
  - name: log-aggregator
    image: busybox
    command: ['sh', '-c', 'tail -f /var/log/nginx/access.log']
    volumeMounts:
    - name: logs
      mountPath: /var/log/nginx
  
  volumes:
  - name: logs
    emptyDir: {}
```

#### 2. Ambassador Pattern

```
┌────────────────────────────────┐
│            POD                  │
│  ┌──────────┐   ┌───────────┐ │
│  │  Main    │   │Ambassador│ │
│  │Container │──▶│ Container │─┼─▶ External
│  │          │   │  (Proxy)  │ │   Service
│  │  App     │   │           │ │
│  └──────────┘   └───────────┘ │
│                                │
└────────────────────────────────┘

Use case: Proxy connections to external services
Examples: DB proxy, API gateway, circuit breaker
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ambassador-example
spec:
  containers:
  # Main application
  - name: app
    image: myapp
    env:
    - name: DATABASE_HOST
      value: localhost  # Connects to ambassador
    - name: DATABASE_PORT
      value: "5432"
  
  # Ambassador: DB connection pooler
  - name: pgbouncer
    image: pgbouncer/pgbouncer:1.21
    ports:
    - containerPort: 5432
    env:
    - name: DB_HOST
      value: postgres.prod.svc.cluster.local
    - name: POOL_MODE
      value: transaction
    - name: MAX_CLIENT_CONN
      value: "1000"
```

#### 3. Adapter Pattern

```
┌────────────────────────────────┐
│            POD                  │
│  ┌──────────┐   ┌───────────┐ │
│  │  Main    │   │  Adapter  │ │
│  │Container │──▶│ Container │ │
│  │          │   │(Transform)│ │
│  │  Legacy  │   │  Metrics  │ │
│  │  App     │   │  Format   │ │
│  └──────────┘   └───────────┘ │
│                       │        │
│                       ▼        │
│               Standardized     │
│                 Output         │
└────────────────────────────────┘

Use case: Transform/adapt output to standard format
Examples: Metrics format conversion, log formatting
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: adapter-example
spec:
  containers:
  # Legacy app with non-standard metrics
  - name: legacy-app
    image: legacy-monitoring-app
    volumeMounts:
    - name: metrics
      mountPath: /var/metrics
  
  # Adapter: Converts to Prometheus format
  - name: metrics-adapter
    image: prom/statsd-exporter
    ports:
    - containerPort: 9102
      name: prometheus
    volumeMounts:
    - name: metrics
      mountPath: /var/metrics
    command:
    - /bin/statsd_exporter
    - --statsd.mapping-config=/etc/adapter/mapping.conf
  
  volumes:
  - name: metrics
    emptyDir: {}
```

---

### Pod Networking (One IP per Pod)

Every pod gets its own unique IP address in the cluster.

#### Pod Network Model

```
┌────────────────────────────────────────────────┐
│               NODE 1                            │
│                                                 │
│  ┌─────────────────────────────────────────┐  │
│  │ Pod 1 (10.244.1.5)                      │  │
│  │  ┌──────────┐  ┌──────────┐            │  │
│  │  │Container1│  │Container2│            │  │
│  │  │(nginx)   │  │(sidecar) │            │  │
│  │  └──────────┘  └──────────┘            │  │
│  │       │             │                    │  │
│  │       └──────┬──────┘                    │  │
│  │              │                           │  │
│  │         localhost                        │  │
│  │     (shared network namespace)           │  │
│  └─────────────────────────────────────────┘  │
│                                                 │
│  ┌─────────────────────────────────────────┐  │
│  │ Pod 2 (10.244.1.6)                      │  │
│  │  ┌──────────┐                           │  │
│  │  │Container │                           │  │
│  │  │(redis)   │                           │  │
│  │  └──────────┘                           │  │
│  └─────────────────────────────────────────┘  │
└────────────────────────────────────────────────┘

Key Points:
- Each pod has ONE IP address
- All containers in pod share that IP
- Containers communicate via localhost
- Different ports for different containers
```

#### Container Communication Within Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  # Container 1: Web server
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80
      name: http
  
  # Container 2: Application server  
  - name: app
    image: myapp:1.0
    ports:
    - containerPort: 8080
      name: app
    env:
    - name: NGINX_HOST
      value: "localhost"  # Can reach nginx via localhost!
    - name: NGINX_PORT
      value: "80"
```

**Communication:**

```bash
# Inside 'app' container:
curl http://localhost:80  # Reaches nginx container

# Inside 'nginx' container:
curl http://localhost:8080  # Reaches app container

# From outside pod (other pods):
curl http://10.244.1.5:80  # Reaches nginx
curl http://10.244.1.5:8080  # Reaches app
```

#### Pod Network Requirements (CNI)

Kubernetes imposes these fundamental requirements:

1. **All pods can communicate with each other** across nodes without NAT
2. **All nodes can communicate with all pods** without NAT
3. **Pod sees its own IP** the same way others see it

```
┌──────────────┐          ┌──────────────┐
│   NODE 1     │          │   NODE 2     │
│              │          │              │
│  Pod A       │          │  Pod B       │
│  10.244.1.5 ─┼──────────┼─▶10.244.2.8  │
│              │  Direct  │              │
│              │  (no NAT)│              │
└──────────────┘          └──────────────┘

Pod A can directly reach Pod B at 10.244.2.8
No port mapping, no translation
```

---

## 2. Workload Resources

### Deployments (Most Common)

**Deployment** is the most common way to run stateless applications.

#### What Deployments Do

**Key Features:**

- ✅ Declarative updates for pods
- ✅ Rolling updates (zero downtime)
- ✅ Rollback to previous versions
- ✅ Scale up/down
- ✅ Pause/resume rollouts
- ✅ Automatic self-healing

#### Deployment Structure

```
┌──────────────────────────────────────┐
│          Deployment                   │
│  (desired: 3 replicas, nginx:1.25)  │
└────────────────┬─────────────────────┘
                 │ manages
                 ▼
┌──────────────────────────────────────┐
│          ReplicaSet                   │
│     (ensures 3 pods exist)           │
└────────────────┬─────────────────────┘
                 │ creates/manages
                 ▼
┌─────────┐  ┌─────────┐  ┌─────────┐
│  Pod 1  │  │  Pod 2  │  │  Pod 3  │
│ (nginx) │  │ (nginx) │  │ (nginx) │
└─────────┘  └─────────┘  └─────────┘
```

#### Creating a Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "250m"
          limits:
            memory: "128Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

```bash
# Create deployment
kubectl apply -f deployment.yaml

# Check status
kubectl get deployments
kubectl get replicasets
kubectl get pods

# Output:
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30s

# Describe deployment
kubectl describe deployment nginx-deployment
```

#### Rolling Updates

```bash
# Update image (triggers rolling update)
kubectl set image deployment/nginx-deployment nginx=nginx:1.26

# OR update YAML and apply
kubectl apply -f deployment.yaml

# Watch rollout
kubectl rollout status deployment/nginx-deployment

# Output:
Waiting for deployment "nginx-deployment" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 1 old replicas are pending termination...
deployment "nginx-deployment" successfully rolled out
```

**What happens during rolling update:**

```
Initial state: 3 pods running nginx:1.25

Time: t=0
- Create 1 pod with nginx:1.26
- Pods: 3 old + 1 new = 4 total (maxSurge)

Time: t=10s
- New pod ready
- Terminate 1 old pod
- Pods: 2 old + 1 new = 3 total

Time: t=20s
- Create 1 pod with nginx:1.26
- Pods: 2 old + 2 new = 4 total

Time: t=30s
- New pod ready
- Terminate 1 old pod
- Pods: 1 old + 2 new = 3 total

Time: t=40s
- Create 1 pod with nginx:1.26
- Pods: 1 old + 3 new = 4 total

Time: t=50s
- New pod ready
- Terminate last old pod
- Pods: 3 new = 3 total

Final state: 3 pods running nginx:1.26
```

#### Rolling Update Strategy

```yaml
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Max pods above desired count
      maxUnavailable: 1  # Max pods unavailable during update
  
  # Example scenarios:
  # replicas=3, maxSurge=1, maxUnavailable=1
  # Can have: 2-4 pods during rollout
  
  # replicas=10, maxSurge=2, maxUnavailable=0
  # Can have: 10-12 pods, never drops below 10 (zero downtime!)
  
  # replicas=10, maxSurge=0, maxUnavailable=2
  # Can have: 8-10 pods, never exceeds 10 (resource-constrained)
```

#### Rollback

```bash
# View rollout history
kubectl rollout history deployment/nginx-deployment

# Output:
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/nginx-deployment nginx=nginx:1.26
3         kubectl set image deployment/nginx-deployment nginx=nginx:1.27

# Rollback to previous version
kubectl rollout undo deployment/nginx-deployment

# Rollback to specific revision
kubectl rollout undo deployment/nginx-deployment --to-revision=2

# Pause rollout (useful for canary testing)
kubectl rollout pause deployment/nginx-deployment

# Resume rollout
kubectl rollout resume deployment/nginx-deployment
```

#### Scaling

```bash
# Scale up/down
kubectl scale deployment/nginx-deployment --replicas=5

# Autoscaling (HPA - Horizontal Pod Autoscaler)
kubectl autoscale deployment/nginx-deployment --min=2 --max=10 --cpu-percent=80

# View HPA
kubectl get hpa
```

---

### ReplicaSets (Managed by Deployments)

**ReplicaSet** ensures a specified number of pod replicas are running.

**You typically don't create ReplicaSets directly** - Deployments manage them for you.

#### ReplicaSet Example

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
```

#### How ReplicaSets Work

```
Desired State: 3 replicas
Current State: 2 replicas

ReplicaSet Controller:
1. Observes: Only 2 pods with label app=nginx
2. Compares: Desired 3, Current 2
3. Action: Create 1 new pod
4. Result: 3 pods running

If a pod dies:
1. ReplicaSet detects: 2 pods running
2. Creates replacement pod
3. Self-healing!
```

```bash
# View ReplicaSets
kubectl get replicasets

# When you update a Deployment, you get multiple ReplicaSets:
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-7d64b8d9c    3         3         3       5m   # Old version
nginx-deployment-5b9c8f6d7    3         3         3       2m   # New version (active)
nginx-deployment-8f6c4b9a1    0         0         0       1m   # Previous version (scaled to 0)

# Old ReplicaSets kept for rollback!
```

---

### StatefulSets (For Stateful Apps)

**StatefulSet** manages stateful applications that need:

- Stable network identities
- Stable persistent storage
- Ordered deployment and scaling
- Ordered rolling updates

#### StatefulSet Use Cases

**When to use:**

- Databases (PostgreSQL, MySQL, MongoDB)
- Message queues (Kafka, RabbitMQ)
- Distributed systems (Zookeeper, etcd, Consul)
- Any application requiring stable identity

**When NOT to use:**

- Stateless applications (use Deployment instead)
- Applications that don't need stable network identity
- Applications where pods are interchangeable

#### StatefulSet Features

```
┌───────────────────────────────────────────────┐
│           StatefulSet: database               │
├───────────────────────────────────────────────┤
│                                               │
│  Pods created in order:                       │
│  ┌──────────────┐  ┌──────────────┐         │
│  │ database-0   │  │ database-1   │         │
│  │ (Primary)    │  │ (Replica)    │         │
│  │              │  │              │         │
│  │ PVC: data-0  │  │ PVC: data-1  │         │
│  │ DNS: db-0... │  │ DNS: db-1... │         │
│  └──────────────┘  └──────────────┘         │
│         │                  │                  │
│         ▼                  ▼                  │
│  ┌──────────────┐  ┌──────────────┐         │
│  │   PV vol-0   │  │   PV vol-1   │         │
│  │  (Persistent)│  │  (Persistent)│         │
│  └──────────────┘  └──────────────┘         │
│                                               │
│  Characteristics:                             │
│  - Stable names: database-{0..N}             │
│  - Stable DNS: database-0.database.svc...    │
│  - Stable storage: Persists across restarts  │
│  - Ordered: Created 0→1→2, Deleted 2→1→0    │
└───────────────────────────────────────────────┘
```

#### StatefulSet Example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    name: postgres
  clusterIP: None  # Headless service (important!)
  selector:
    app: postgres

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: "postgres"  # Must match headless service
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_PASSWORD
          value: secret
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  
  # Volume Claim Template - creates PVC per pod
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

```bash
# Deploy StatefulSet
kubectl apply -f statefulset.yaml

# Watch pods created in order
kubectl get pods -w

# Output:
NAME         READY   STATUS              AGE
postgres-0   0/1     ContainerCreating   0s
postgres-0   1/1     Running             30s
postgres-1   0/1     ContainerCreating   0s
postgres-1   1/1     Running             30s
postgres-2   0/1     ContainerCreating   0s
postgres-2   1/1     Running             30s

# View PVCs (one per pod)
kubectl get pvc

# Output:
NAME                        STATUS   VOLUME    CAPACITY
postgres-storage-postgres-0   Bound    pv-001    10Gi
postgres-storage-postgres-1   Bound    pv-002    10Gi
postgres-storage-postgres-2   Bound    pv-003    10Gi

# DNS names (from any pod):
nslookup postgres-0.postgres.default.svc.cluster.local
nslookup postgres-1.postgres.default.svc.cluster.local
nslookup postgres-2.postgres.default.svc.cluster.local
```

#### StatefulSet Scaling

```bash
# Scale up (creates postgres-3)
kubectl scale statefulset postgres --replicas=4

# Scaling happens in order:
# Scale up: 0 → 1 → 2 → 3
# Each waits for previous to be Ready

# Scale down (deletes postgres-3, then postgres-2)
kubectl scale statefulset postgres --replicas=2

# Scaling down: 3 → 2
# Happens in reverse order
# PVCs are NOT deleted automatically (data preserved!)
```

#### StatefulSet Update Strategy

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  
  # partition=2 means: Only update pods >= 2
  # Useful for canary testing:
  # 1. Set partition=2, update statefulset
  # 2. Only postgres-2 updates (postgres-0, postgres-1 unchanged)
  # 3. Test postgres-2
  # 4. If OK, set partition=0 to update all
```

---

### DaemonSets (One Pod Per Node)

**DaemonSet** ensures a pod runs on every node (or selected nodes).

#### DaemonSet Use Cases

**Common uses:**

- Log collection (Fluentd, Logstash)
- Monitoring agents (Prometheus Node Exporter, Datadog)
- Network plugins (Calico, Weave)
- Storage daemons (Ceph, GlusterFS)
- Security agents (Falco, Aqua Security)

#### DaemonSet Example

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      # Allow scheduling on master nodes
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.16
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

```bash
# Deploy DaemonSet
kubectl apply -f daemonset.yaml

# View DaemonSet
kubectl get daemonset -n kube-system

# Output:
NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE
fluentd   3         3         3       3            3

# One pod per node:
kubectl get pods -n kube-system -o wide | grep fluentd

# Output:
NAME            READY   STATUS    NODE
fluentd-abc123  1/1     Running   node-1
fluentd-def456  1/1     Running   node-2
fluentd-ghi789  1/1     Running   node-3
```

#### Node Selection

```yaml
spec:
  template:
    spec:
      # Run only on nodes with SSD
      nodeSelector:
        disktype: ssd
      
      # OR use node affinity
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - worker
                - compute
```

#### DaemonSet Updates

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  
  # Updates one node at a time
  # Ensures at least N-1 nodes have running pod
```

---

### Jobs and CronJobs (Batch Workloads)

#### Jobs

**Job** runs pods to completion (not long-running).

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: database-backup
spec:
  # Retry policy
  backoffLimit: 3  # Retry 3 times if failed
  
  # Completion requirements
  completions: 1  # How many successful completions needed
  parallelism: 1  # How many pods to run in parallel
  
  template:
    spec:
      restartPolicy: Never  # or OnFailure
      containers:
      - name: backup
        image: postgres:15
        command:
        - /bin/bash
        - -c
        - |
          pg_dump -h postgres -U admin mydb > /backup/db-$(date +%Y%m%d).sql
          echo "Backup completed!"
        volumeMounts:
        - name: backup-volume
          mountPath: /backup
      volumes:
      - name: backup-volume
        persistentVolumeClaim:
          claimName: backup-pvc
```

```bash
# Create job
kubectl apply -f job.yaml

# Watch job
kubectl get job -w

# Output:
NAME              COMPLETIONS   DURATION   AGE
database-backup   0/1           0s         0s
database-backup   1/1           30s        30s

# View pods created by job
kubectl get pods

# Job creates pod with unique name:
NAME                    READY   STATUS      RESTARTS
database-backup-abc123  0/1     Completed   0

# View logs
kubectl logs database-backup-abc123

# Delete job (and its pods)
kubectl delete job database-backup
```

**Parallel Jobs:**

```yaml
spec:
  completions: 10  # Need 10 successful completions
  parallelism: 3   # Run 3 pods at a time
  
  # Creates 10 pods total, 3 at a time
  # If pod fails, creates replacement
  # Job complete when 10 succeed
```

#### CronJobs

**CronJob** runs jobs on a schedule.

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: daily-backup
spec:
  # Cron schedule (same as Linux cron)
  schedule: "0 2 * * *"  # Daily at 2:00 AM
  
  # Concurrency policy
  concurrencyPolicy: Forbid  # Don't start if previous still running
  # concurrencyPolicy: Allow   # Allow concurrent jobs
  # concurrencyPolicy: Replace # Cancel old, start new
  
  # Job history
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: backup
            image: postgres:15
            command:
            - /bin/bash
            - -c
            - |
              DATE=$(date +%Y%m%d_%H%M%S)
              pg_dump -h postgres -U admin mydb > /backup/db-${DATE}.sql
              echo "Backup completed: db-${DATE}.sql"
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: password
```

**Cron Schedule Format:**

```
┌───────────── minute (0 - 59)
│ ┌───────────── hour (0 - 23)
│ │ ┌───────────── day of month (1 - 31)
│ │ │ ┌───────────── month (1 - 12)
│ │ │ │ ┌───────────── day of week (0 - 6) (Sunday=0)
│ │ │ │ │
│ │ │ │ │
* * * * *

Examples:
"0 2 * * *"       # Daily at 2:00 AM
"*/15 * * * *"    # Every 15 minutes
"0 */6 * * *"     # Every 6 hours
"0 0 * * 0"       # Weekly on Sunday at midnight
"0 0 1 * *"       # Monthly on 1st at midnight
"0 9 * * 1-5"     # Weekdays at 9:00 AM
```

```bash
# Create CronJob
kubectl apply -f cronjob.yaml

# View CronJobs
kubectl get cronjob

# Output:
NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
daily-backup   0 2 * * *     False     0        12h             5d

# View jobs created by CronJob
kubectl get jobs

# Output:
NAME                      COMPLETIONS   DURATION   AGE
daily-backup-28404320     1/1           45s        12h
daily-backup-28404200     1/1           42s        36h
daily-backup-28404080     1/1           48s        60h

# Manually trigger job
kubectl create job --from=cronjob/daily-backup manual-backup-1

# Suspend CronJob
kubectl patch cronjob daily-backup -p '{"spec":{"suspend":true}}'
```

---

## 3. Services

**Service** provides stable network endpoint for a set of pods.

### Why Services?

**Problem:**

- Pods are ephemeral (come and go)
- Pod IPs change when recreated
- How do clients find backends?

**Solution: Service**

- Stable virtual IP (doesn't change)
- DNS name
- Load balances across pods
- Service discovery

### Service Types

#### 1. ClusterIP (Internal)

**ClusterIP** exposes service on internal IP within cluster.

**Use case:** Internal microservice communication

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP  # Default type
  selector:
    app: backend
  ports:
  - port: 80        # Service port
    targetPort: 8080  # Container port
    protocol: TCP
```

```
┌────────────────────────────────────────┐
│            Kubernetes Cluster           │
│                                         │
│  ┌──────────────────────────────────┐  │
│  │  Service: backend-service        │  │
│  │  ClusterIP: 10.96.0.100:80      │  │
│  └───────────────┬──────────────────┘  │
│                  │ Routes to            │
│                  ▼                      │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐  │
│  │ Pod 1   │ │ Pod 2   │ │ Pod 3   │  │
│  │:8080    │ │:8080    │ │:8080    │  │
│  │10.1.1.5 │ │10.1.1.6 │ │10.1.1.7 │  │
│  └─────────┘ └─────────┘ └─────────┘  │
│                                         │
│  Any pod can access via:                │
│  - http://backend-service              │
│  - http://backend-service.default      │
│  - http://backend-service.default.svc  │
│  - http://10.96.0.100                  │
└────────────────────────────────────────┘
```

```bash
# Create service
kubectl apply -f service.yaml

# View service
kubectl get service backend-service

# Output:
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)
backend-service   ClusterIP   10.96.0.100    <none>        80/TCP

# View endpoints (actual pod IPs)
kubectl get endpoints backend-service

# Output:
NAME              ENDPOINTS
backend-service   10.1.1.5:8080,10.1.1.6:8080,10.1.1.7:8080

# Test from another pod:
kubectl run test --rm -it --image=busybox -- sh
/ # wget -qO- http://backend-service
```

#### 2. NodePort (External, Simple)

**NodePort** exposes service on each node's IP at a static port.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  type: NodePort
  selector:
    app: frontend
  ports:
  - port: 80         # Service port (ClusterIP)
    targetPort: 8080   # Container port
    nodePort: 30080   # Port on each node (30000-32767)
    protocol: TCP
```

```
┌────────────────────────────────────────────────┐
│        External Traffic (Internet)              │
│                     │                           │
│  ┌──────────────────┼────────────────────────┐ │
│  │       http://<NodeIP>:30080              │ │
│  │                  │                        │ │
│  │  ┌───────────────▼──────────────┐        │ │
│  │  │ Any Node (IP: 192.168.1.10) │        │ │
│  │  │ NodePort: 30080              │        │ │
│  │  └───────────────┬──────────────┘        │ │
│  │                  │                        │ │
│  │  ┌───────────────▼─────────────────────┐ │ │
│  │  │ Service: frontend-service          │ │ │
│  │  │ ClusterIP: 10.96.0.200:80         │ │ │
│  │  └───────────────┬─────────────────────┘ │ │
│  │                  │ Routes to              │ │
│  │                  ▼                        │ │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐    │ │
│  │  │ Pod 1   │ │ Pod 2   │ │ Pod 3   │    │ │
│  │  │:8080    │ │:8080    │ │:8080    │    │ │
│  │  └─────────┘ └─────────┘ └─────────┘    │ │
└────────────────────────────────────────────────┘

Access from outside cluster:
- http://192.168.1.10:30080
- http://192.168.1.11:30080  (any node IP works!)
- http://192.168.1.12:30080
```

```bash
# Create NodePort service
kubectl apply -f nodeport-service.yaml

# View service
kubectl get service frontend-service

# Output:
NAME               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)
frontend-service   NodePort   10.96.0.200    <none>        80:30080/TCP

# Access from outside cluster:
curl http://<any-node-ip>:30080

# Find node IPs:
kubectl get nodes -o wide
```

**Pros/Cons:**

- ✅ Simple, no cloud provider needed
- ✅ Works on bare metal, on-premise
- ❌ Uses non-standard ports (30000-32767)
- ❌ No load balancer (must manage yourself)
- ❌ Exposes nodes directly

#### 3. LoadBalancer (Cloud Provider)

**LoadBalancer** provisions cloud load balancer.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
```

```
┌────────────────────────────────────────────────┐
│              Internet                           │
│                  │                              │
│  ┌───────────────▼──────────────────────────┐  │
│  │  Cloud Load Balancer                     │  │
│  │  (AWS ELB / GCP LB / Azure LB)          │  │
│  │  External IP: 203.0.113.10              │  │
│  └───────────────┬──────────────────────────┘  │
│                  │                              │
│  ┌───────────────▼──────────────────────────┐  │
│  │  Kubernetes Cluster                      │  │
│  │  ┌────────────────────────────────────┐  │  │
│  │  │ Service: web-service               │  │  │
│  │  │ Type: LoadBalancer                 │  │  │
│  │  │ ClusterIP: 10.96.0.50:80          │  │  │
│  │  └────────────┬───────────────────────┘  │  │
│  │               │ Routes to                 │  │
│  │               ▼                           │  │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐   │  │
│  │  │ Pod 1   │ │ Pod 2   │ │ Pod 3   │   │  │
│  │  │:8080    │ │:8080    │ │:8080    │   │  │
│  │  └─────────┘ └─────────┘ └─────────┘   │  │
│  └──────────────────────────────────────────┘  │
└────────────────────────────────────────────────┘

Access: http://203.0.113.10
```

```bash
# Create LoadBalancer service
kubectl apply -f loadbalancer-service.yaml

# View service (wait for EXTERNAL-IP)
kubectl get service web-service -w

# Output:
NAME          TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)
web-service   LoadBalancer   10.96.0.50   <pending>       80:31234/TCP
web-service   LoadBalancer   10.96.0.50   203.0.113.10    80:31234/TCP

# Access from internet:
curl http://203.0.113.10
```

**Cloud-specific annotations:**

```yaml
# AWS
metadata:
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-internal: "true"

# GCP
metadata:
  annotations:
    cloud.google.com/load-balancer-type: "Internal"

# Azure
metadata:
  annotations:
    service.beta.kubernetes.io/azure-load-balancer-internal: "true"
```

#### 4. ExternalName

**ExternalName** maps service to external DNS name.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-database
spec:
  type: ExternalName
  externalName: database.example.com
```

```bash
# Pods can access external service via internal name:
mysql -h external-database.default.svc.cluster.local

# DNS resolves to: database.example.com
# Then resolves to actual IP

# Use case: Gradually migrate external service into cluster
# 1. Start: externalName: old-database.external.com
# 2. Migrate: Deploy database in cluster
# 3. Switch: Change service to ClusterIP pointing to new pods
# 4. Apps unchanged (still use same service name)
```

#### 5. Headless Services (For StatefulSets)

**Headless service** (clusterIP: None) returns pod IPs directly instead of service IP.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: database
spec:
  clusterIP: None  # Headless!
  selector:
    app: postgres
  ports:
  - port: 5432
```

```bash
# Normal service DNS:
nslookup my-service.default.svc.cluster.local
# Returns: 10.96.0.100 (service ClusterIP)

# Headless service DNS:
nslookup database.default.svc.cluster.local
# Returns:  10.1.1.5 (pod 1 IP)
#           10.1.1.6 (pod 2 IP)
#           10.1.1.7 (pod 3 IP)

# Individual pod DNS (StatefulSet):
nslookup database-0.database.default.svc.cluster.local
# Returns: 10.1.1.5 (specific pod IP)

# Use case: StatefulSets need stable DNS per pod
```

---

### Service Discovery (DNS)

Kubernetes automatically configures DNS for service discovery.

#### DNS Names

```
<service-name>.<namespace>.svc.<cluster-domain>

Examples:
backend-service.default.svc.cluster.local
postgres.production.svc.cluster.local
api.staging.svc.cluster.local

Short forms (same namespace):
backend-service
backend-service.default

Cross-namespace:
postgres.production
```

#### DNS Resolution Example

```bash
# From a pod in 'default' namespace:

# Same namespace - short name works:
curl http://backend-service

# Different namespace - must specify:
curl http://postgres.production

# Fully qualified (always works):
curl http://backend-service.default.svc.cluster.local

# DNS queries handled by CoreDNS:
kubectl get pods -n kube-system | grep coredns
```

#### Service Environment Variables

```bash
# Kubernetes also injects environment variables:

# For service: backend-service in default namespace
BACKEND_SERVICE_SERVICE_HOST=10.96.0.100
BACKEND_SERVICE_SERVICE_PORT=80
BACKEND_SERVICE_PORT=tcp://10.96.0.100:80
BACKEND_SERVICE_PORT_80_TCP=tcp://10.96.0.100:80
BACKEND_SERVICE_PORT_80_TCP_PROTO=tcp
BACKEND_SERVICE_PORT_80_TCP_PORT=80
BACKEND_SERVICE_PORT_80_TCP_ADDR=10.96.0.100

# View from a pod:
kubectl exec -it mypod -- env | grep SERVICE
```

---

## 4. Networking Deep Dive

### CNI Plugins (Calico, Flannel, Cilium)

**CNI (Container Network Interface)** plugins implement Kubernetes networking requirements.

#### What CNI Plugins Do

**Three core responsibilities:**

1. Assign IP addresses to pods
2. Ensure pod-to-pod communication across nodes
3. Enforce network policies

#### Popular CNI Plugins

**1. Calico**

```
Features:
- L3 networking (BGP routing)
- Network policies (firewalling)
- High performance
- Scales to large clusters (1000+ nodes)
- eBPF dataplane option

Best for: Production, enterprise, security-focused

Installation:
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

Network Policies: Yes (full support)
Encryption: Yes (WireGuard)
```

**2. Flannel**

```
Features:
- Simple overlay network (VXLAN)
- Easy to set up
- Lightweight
- Good for small-medium clusters

Best for: Development, simple deployments

Installation:
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

Network Policies: No (needs Calico for policies)
Encryption: No
```

**3. Cilium**

```
Features:
- eBPF-based (Linux kernel technology)
- Very high performance
- Advanced load balancing
- API-aware network security (HTTP, gRPC aware)
- Observability (Hubble)

Best for: Modern microservices, high performance

Installation:
helm install cilium cilium/cilium

Network Policies: Yes (extended API-aware policies)
Encryption: Yes (transparent)
```

**4. Weave Net**

```
Features:
- Automatic mesh network
- Built-in DNS
- Encryption
- Simple setup

Best for: Quick setup, development

Installation:
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

Network Policies: Yes
Encryption: Yes (automatic)
```

#### CNI Plugin Comparison

|Feature|Calico|Flannel|Cilium|Weave|
|---|---|---|---|---|
|**Performance**|High|Good|Excellent|Good|
|**Complexity**|Medium|Low|High|Low|
|**Network Policies**|Yes|No|Yes (advanced)|Yes|
|**Encryption**|Yes|No|Yes|Yes|
|**Scalability**|Excellent|Good|Excellent|Good|
|**Best for**|Production|Simple|Modern apps|Quick setup|

---

### Pod-to-Pod Communication

All pods can communicate with each other directly (no NAT).

#### Same Node Communication

```
┌────────────────────────────────────────┐
│             NODE 1                      │
│                                         │
│  ┌─────────────────────────────────┐   │
│  │   Pod A (10.244.1.5)            │   │
│  │   Container: nginx              │   │
│  └───────────────┬─────────────────┘   │
│                  │                      │
│            Linux Bridge                 │
│          (cbr0 / cni0)                  │
│                  │                      │
│  ┌───────────────▼─────────────────┐   │
│  │   Pod B (10.244.1.6)            │   │
│  │   Container: redis              │   │
│  └─────────────────────────────────┘   │
│                                         │
│  Pod A → Pod B: 10.244.1.5 → 10.244.1.6│
│  Direct communication via bridge        │
└────────────────────────────────────────┘
```

#### Cross-Node Communication

```
┌──────────────────────┐        ┌──────────────────────┐
│      NODE 1          │        │      NODE 2          │
│  192.168.1.10        │        │  192.168.1.11        │
│                      │        │                      │
│  Pod A               │        │  Pod B               │
│  10.244.1.5          │        │  10.244.2.8          │
│         │            │        │         ▲            │
│         └────────────┼────────┼─────────┘            │
│                      │        │                      │
│  Packet flow:        │        │                      │
│  1. Pod A sends to   │        │                      │
│     10.244.2.8       │        │                      │
│  2. Routing table:   │        │                      │
│     10.244.2.0/24    │        │                      │
│     via node-2       │        │                      │
│  3. Encapsulate      │        │                      │
│     (VXLAN/IPIP)     │        │                      │
│  4. Send to node-2 ──┼───────▶│                      │
│                      │        │  5. Decapsulate      │
│                      │        │  6. Deliver to Pod B │
└──────────────────────┘        └──────────────────────┘

CNI plugin handles routing:
- Calico: BGP routing
- Flannel: VXLAN overlay
- Cilium: eBPF routing
```

---

### Service Networking

How services route traffic to pods.

#### iptables Mode (Default)

```bash
# kube-proxy creates iptables rules

# Service: web-service (10.96.0.100:80)
# Backends: 10.244.1.5:8080, 10.244.2.6:8080, 10.244.3.7:8080

# iptables rules (simplified):
-A KUBE-SERVICES -d 10.96.0.100/32 -p tcp --dport 80 -j KUBE-SVC-WEB

-A KUBE-SVC-WEB -m statistic --mode random --probability 0.33 -j KUBE-SEP-1
-A KUBE-SVC-WEB -m statistic --mode random --probability 0.50 -j KUBE-SEP-2
-A KUBE-SVC-WEB -j KUBE-SEP-3

-A KUBE-SEP-1 -p tcp -j DNAT --to-destination 10.244.1.5:8080
-A KUBE-SEP-2 -p tcp -j DNAT --to-destination 10.244.2.6:8080
-A KUBE-SEP-3 -p tcp -j DNAT --to-destination 10.244.3.7:8080

# Traffic flow:
# 1. App connects to 10.96.0.100:80
# 2. iptables randomly selects backend
# 3. DNAT rewrites destination to pod IP
# 4. Packet delivered to pod
```

#### IPVS Mode (More Scalable)

```bash
# kube-proxy uses IPVS instead of iptables
# Better performance with many services

# Configure:
kube-proxy --proxy-mode=ipvs

# View IPVS rules:
ipvsadm -Ln

# Output:
TCP  10.96.0.100:80 rr
  -> 10.244.1.5:8080  Masq  1  0  0
  -> 10.244.2.6:8080  Masq  1  0  0
  -> 10.244.3.7:8080  Masq  1  0  0
```

---

### Network Policies (Security)

**NetworkPolicy** controls traffic between pods (firewall rules).

#### Default Behavior (No Policies)

```
All pods can communicate with:
- All other pods
- All services
- External endpoints

This is NOT secure for production!
```

#### NetworkPolicy Example: Deny All

```yaml
# Default deny all ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: {}  # Applies to all pods
  policyTypes:
  - Ingress
  # No ingress rules = deny all
```

```yaml
# Default deny all egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
  # No egress rules = deny all
```

#### NetworkPolicy Example: Allow Specific Traffic

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
      tier: api
  
  policyTypes:
  - Ingress
  - Egress
  
  # Ingress rules
  ingress:
  # Allow from frontend pods on port 8080
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
  
  # Egress rules
  egress:
  # Allow to database pods on port 5432
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
  
  # Allow DNS (critical!)
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

#### NetworkPolicy Example: Namespace Isolation

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  # Only allow from pods in same namespace
  - from:
    - podSelector: {}
```

#### NetworkPolicy Example: Allow External

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-external-access
spec:
  podSelector:
    matchLabels:
      app: api
      public: "true"
  policyTypes:
  - Ingress
  ingress:
  # Allow from anywhere
  - from:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8  # Except private IPs
    ports:
    - protocol: TCP
      port: 443
```

```bash
# Apply policies
kubectl apply -f network-policy.yaml

# View policies
kubectl get networkpolicy -n production

# Describe policy
kubectl describe networkpolicy backend-policy

# Test connectivity (should be blocked):
kubectl exec frontend-pod -- curl http://backend-pod:8080
# Allowed!

kubectl exec other-pod -- curl http://backend-pod:8080
# Connection refused (blocked by policy)
```

---

### Ingress vs LoadBalancer vs NodePort

**Comparison of external access methods:**

#### NodePort

```
Pros:
✅ Simple, works anywhere
✅ No cloud provider needed

Cons:
❌ Non-standard ports (30000-32767)
❌ One service per port
❌ No SSL termination
❌ No path-based routing

Use case: Development, testing
```

#### LoadBalancer

```
Pros:
✅ Standard ports (80, 443)
✅ Cloud-managed
✅ Health checks

Cons:
❌ One LoadBalancer per service (expensive!)
❌ Requires cloud provider
❌ No path-based routing
❌ No SSL termination (unless cloud LB supports)

Use case: Simple production setup, single service
Cost: $18/month per service (AWS ELB)
```

#### Ingress

```
Pros:
✅ One LoadBalancer for many services!
✅ Path-based routing (example.com/api → api-service)
✅ Host-based routing (api.example.com → api-service)
✅ SSL termination
✅ URL rewriting
✅ Advanced features (auth, rate limiting)

Cons:
❌ Requires Ingress Controller
❌ More complex setup

Use case: Production (multiple services)
Cost: One LB for entire cluster (~$18/month total)
```

**Example Architectures:**

```
NodePort:
Internet → Node IP:30080 → Service → Pods
         → Node IP:30081 → Service → Pods
         → Node IP:30082 → Service → Pods

LoadBalancer:
Internet → LB1 (cost $) → Service1 → Pods
         → LB2 (cost $) → Service2 → Pods
         → LB3 (cost $) → Service3 → Pods

Ingress:
Internet → One LB (cost $) → Ingress Controller
                           ↓
         example.com/api     → api-service → Pods
         example.com/web     → web-service → Pods
         admin.example.com   → admin-service → Pods
```

**Summary Table:**

|Feature|NodePort|LoadBalancer|Ingress|
|---|---|---|---|
|**External access**|Yes|Yes|Yes|
|**Standard ports**|No|Yes|Yes|
|**SSL termination**|No|No|Yes|
|**Path routing**|No|No|Yes|
|**Cost (cloud)**|Free|$18/service|$18 total|
|**Complexity**|Low|Low|Medium|
|**Production ready**|No|Yes (single service)|Yes|

---

### Key Takeaways

**Pods:**

- Smallest unit, one IP per pod
- Init containers run sequentially before main containers
- Sidecar containers enhance main containers
- Multi-container patterns: Sidecar, Ambassador, Adapter

**Workload Resources:**

- **Deployments**: Stateless apps, rolling updates, most common
- **ReplicaSets**: Managed by Deployments, ensures replica count
- **StatefulSets**: Stateful apps, stable identities, ordered operations
- **DaemonSets**: One pod per node, for system services
- **Jobs**: Run to completion, batch processing
- **CronJobs**: Scheduled jobs

**Services:**

- **ClusterIP**: Internal communication (default)
- **NodePort**: Simple external access
- **LoadBalancer**: Cloud provider load balancer
- **ExternalName**: DNS mapping
- **Headless**: Direct pod IPs for StatefulSets

**Networking:**

- **CNI plugins**: Calico, Flannel, Cilium implement pod networking
- **Pod-to-pod**: Direct communication (no NAT)
- **Service**: Load balancing via iptables/IPVS
- **NetworkPolicy**: Firewall rules between pods
- **Ingress**: HTTP/HTTPS routing, SSL termination (production standard)
