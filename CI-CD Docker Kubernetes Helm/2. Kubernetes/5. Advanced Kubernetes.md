
## 1. Resource Management

### Requests vs Limits (CPU, Memory)

**Requests**: Minimum guaranteed resources **Limits**: Maximum allowed resources

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "256Mi"  # Guaranteed 256MB
        cpu: "500m"      # Guaranteed 0.5 CPU core
      limits:
        memory: "512Mi"  # Max 512MB (killed if exceeded)
        cpu: "1000m"     # Max 1 CPU core (throttled if exceeded)
```

**CPU Units:**

```
1 CPU = 1000m (millicores)
500m = 0.5 CPU = half a core
2000m = 2 CPUs = two cores

Examples:
100m = 10% of one CPU core
250m = 25% of one CPU core
1000m = 100% of one CPU core = 1 full core
```

**Memory Units:**

```
Ki = Kibibyte (1024 bytes)
Mi = Mebibyte (1024 Ki)
Gi = Gibibyte (1024 Mi)

128974848 = 129e6 = 129M = 123Mi (approximately)
```

**What happens when:**

```yaml
# CPU Limit exceeded:
# - Process is throttled (slowed down)
# - Pod NOT killed
# - CPU usage capped at limit

# Memory Limit exceeded:
# - Process is killed (OOMKilled)
# - Pod restarted by kubelet
# - Shows in pod status: OOMKilled

# No Requests specified:
# - Pod can be scheduled anywhere
# - No resource guarantees
# - May be evicted under pressure

# No Limits specified:
# - Pod can use all available resources
# - Can starve other pods
# - Not recommended for production
```

**Best Practices:**

```yaml
# Development (guaranteed resources)
requests:
  memory: "512Mi"
  cpu: "500m"
limits:
  memory: "512Mi"
  cpu: "500m"
# Guaranteed QoS class

# Production (allow bursting)
requests:
  memory: "512Mi"
  cpu: "500m"
limits:
  memory: "1Gi"
  cpu: "2000m"
# Burstable QoS class
```

---

### Quality of Service (QoS) Classes

Kubernetes assigns QoS class based on requests/limits:

**1. Guaranteed (Highest Priority)**

```yaml
# All containers have:
# - Memory request = Memory limit
# - CPU request = CPU limit
resources:
  requests:
    memory: "512Mi"
    cpu: "500m"
  limits:
    memory: "512Mi"
    cpu: "500m"

# Characteristics:
# ✅ Highest priority
# ✅ Last to be evicted
# ✅ Guaranteed resources
```

**2. Burstable (Medium Priority)**

```yaml
# At least one container has:
# - Request < Limit
# OR
# - Has request but no limit
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "1000m"

# Characteristics:
# ⚠️ Medium priority
# ⚠️ Evicted after BestEffort
# ✅ Can burst above requests
```

**3. BestEffort (Lowest Priority)**

```yaml
# No requests or limits specified
resources: {}

# Characteristics:
# ❌ Lowest priority
# ❌ First to be evicted
# ❌ No guarantees
```

**QoS in Action:**

```bash
# Check pod QoS class
kubectl describe pod myapp

# Output shows:
QoS Class:       Burstable

# Under resource pressure:
# 1. Kill BestEffort pods first
# 2. Kill Burstable pods second
# 3. Kill Guaranteed pods last (only if necessary)
```

---

### Resource Quotas

**ResourceQuota** limits total resources per namespace.

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    # CPU/Memory totals
    requests.cpu: "10"           # Total CPU requests
    requests.memory: "20Gi"      # Total memory requests
    limits.cpu: "20"             # Total CPU limits
    limits.memory: "40Gi"        # Total memory limits
    
    # Object counts
    pods: "50"                   # Max 50 pods
    services: "20"               # Max 20 services
    persistentvolumeclaims: "10" # Max 10 PVCs
    
    # Storage
    requests.storage: "100Gi"    # Total storage requests
```

**Multiple Quotas:**

```yaml
# Quota 1: Compute resources
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "5"
    requests.memory: "10Gi"

---
# Quota 2: Object counts
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: dev
spec:
  hard:
    pods: "20"
    services: "10"
    configmaps: "20"
    secrets: "20"
```

**Quota Scopes:**

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: priority-quota
  namespace: production
spec:
  hard:
    pods: "10"
  scopes:
  - BestEffort  # Only count BestEffort pods
  # Scopes: BestEffort, NotBestEffort, Terminating, NotTerminating
```

**Check quota usage:**

```bash
kubectl get resourcequota -n production
kubectl describe resourcequota compute-quota -n production

# Output:
Name:            compute-quota
Namespace:       production
Resource         Used   Hard
--------         ----   ----
requests.cpu     8      10
requests.memory  15Gi   20Gi
pods             35     50
```

---

### LimitRanges

**LimitRange** enforces min/max resources per pod/container.

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: production
spec:
  limits:
  # Container limits
  - type: Container
    max:
      cpu: "2"
      memory: "2Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    default:  # Default limit if not specified
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:  # Default request if not specified
      cpu: "250m"
      memory: "256Mi"
    maxLimitRequestRatio:  # Max limit/request ratio
      cpu: "4"
      memory: "4"
  
  # Pod limits (sum of all containers)
  - type: Pod
    max:
      cpu: "4"
      memory: "8Gi"
  
  # PVC limits
  - type: PersistentVolumeClaim
    max:
      storage: "10Gi"
    min:
      storage: "1Gi"
```

**How LimitRange Works:**

```yaml
# Without LimitRange:
# User creates pod with no resources
resources: {}
# Accepted! (BestEffort QoS)

# With LimitRange:
# Same pod automatically gets:
resources:
  requests:
    cpu: "250m"     # From defaultRequest
    memory: "256Mi"
  limits:
    cpu: "500m"     # From default
    memory: "512Mi"
```

---

### Horizontal Pod Autoscaler (HPA)

**HPA** automatically scales pods based on metrics.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  
  minReplicas: 2   # Minimum pods
  maxReplicas: 10  # Maximum pods
  
  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU
  
  # Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # Target 80% memory
  
  behavior:  # Scaling behavior
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60  # Scale down max 50% per minute
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15  # Scale up max 100% per 15 seconds
```

**HPA with Custom Metrics:**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-custom-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 2
  maxReplicas: 20
  
  metrics:
  # Custom metric from Prometheus
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"  # Scale at 1000 RPS per pod
  
  # External metric
  - type: External
    external:
      metric:
        name: queue_length
        selector:
          matchLabels:
            queue: "jobs"
      target:
        type: Value
        value: "30"  # Scale if queue > 30 items
```

**How HPA Works:**

```
1. HPA checks metrics every 15 seconds (default)
2. Calculates desired replicas:
   desiredReplicas = ceil(currentReplicas * currentMetric / targetMetric)
3. Example:
   - Current: 4 replicas
   - Current CPU: 90%
   - Target CPU: 70%
   - Desired: ceil(4 * 90 / 70) = ceil(5.14) = 6 replicas
4. Scales deployment to 6 replicas
```

```bash
# Check HPA status
kubectl get hpa

# Output:
NAME         REFERENCE           TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
webapp-hpa   Deployment/webapp   45%/70%   2         10        3          5m

# Describe for details
kubectl describe hpa webapp-hpa
```

---

### Vertical Pod Autoscaler (VPA)

**VPA** automatically adjusts CPU/memory requests/limits.

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: webapp-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, Off
  
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
      controlledResources: ["cpu", "memory"]
```

**VPA Update Modes:**

```yaml
# Auto: VPA updates pods automatically (recreates pods)
updateMode: "Auto"

# Recreate: VPA evicts pods to apply new resources
updateMode: "Recreate"

# Initial: Only set resources on pod creation
updateMode: "Initial"

# Off: Only provide recommendations, no changes
updateMode: "Off"
```

**VPA Recommendations:**

```bash
kubectl describe vpa webapp-vpa

# Output:
Status:
  Recommendation:
    Container Recommendations:
      Container Name:  app
      Lower Bound:     # Safe to decrease to
        Cpu:     250m
        Memory:  262144k
      Target:          # Recommended values
        Cpu:     500m
        Memory:  524288k
      Upper Bound:     # Safe to increase to
        Cpu:     2
        Memory:  2Gi
```

**HPA vs VPA:**

```
HPA (Horizontal):
- Adds/removes pods
- Good for: Handling traffic spikes
- Scales: Number of pods

VPA (Vertical):
- Adjusts pod resources
- Good for: Right-sizing resources
- Scales: Resources per pod

⚠️ Don't use HPA and VPA together on same metric!
(Can cause thrashing)

✅ Can use together:
- HPA for CPU
- VPA for memory
```

---

### Cluster Autoscaler

**Cluster Autoscaler** adds/removes nodes based on pod scheduling needs.

```yaml
# Cluster Autoscaler deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - name: cluster-autoscaler
        image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws
        - --namespace=kube-system
        - --nodes=2:10:k8s-worker-asg  # min:max:asg-name
        - --scale-down-delay-after-add=10m
        - --scale-down-unneeded-time=10m
```

**How it works:**

```
Scale Up:
1. Pod cannot be scheduled (insufficient resources)
2. Cluster Autoscaler simulates adding node
3. If pod would fit, adds node
4. Pod scheduled on new node

Scale Down:
1. Node underutilized for 10+ minutes
2. All pods can be moved to other nodes
3. Node drained and removed
```

**Three-tier autoscaling:**

```
┌────────────────────────────────────────┐
│  Cluster Autoscaler                    │
│  (Adds/removes nodes)                  │
│                                        │
│  Metrics: Unschedulable pods           │
│  Scale: Node count                     │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Horizontal Pod Autoscaler (HPA)       │
│  (Adds/removes pods)                   │
│                                        │
│  Metrics: CPU, memory, custom          │
│  Scale: Pod count                      │
└────────────────┬───────────────────────┘
                 │
┌────────────────▼───────────────────────┐
│  Vertical Pod Autoscaler (VPA)         │
│  (Adjusts pod resources)               │
│                                        │
│  Metrics: CPU/memory usage             │
│  Scale: Resources per pod              │
└────────────────────────────────────────┘

Example flow:
1. Traffic increases
2. HPA scales pods: 5 → 10
3. New pods pending (no resources)
4. Cluster Autoscaler adds node
5. Pods scheduled on new node
6. VPA adjusts resources if needed
```

---

## 2. Health Checks

### Liveness Probes

**Liveness probe** checks if container is healthy. If fails → restart container.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: liveness-demo
spec:
  containers:
  - name: app
    image: myapp
    
    livenessProbe:
      # HTTP probe
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 30  # Wait 30s after start
      periodSeconds: 10         # Check every 10s
      timeoutSeconds: 5         # Timeout after 5s
      successThreshold: 1       # 1 success = healthy
      failureThreshold: 3       # 3 failures = restart
```

**Probe Types:**

**1. HTTP GET:**

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
    scheme: HTTP  # or HTTPS
  # Success: HTTP 200-399
  # Failure: Any other code or timeout
```

**2. TCP Socket:**

```yaml
livenessProbe:
  tcpSocket:
    port: 5432
  # Success: Connection established
  # Failure: Cannot connect
```

**3. Exec Command:**

```yaml
livenessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  # Success: Exit code 0
  # Failure: Non-zero exit code
```

**4. gRPC (Kubernetes 1.24+):**

```yaml
livenessProbe:
  grpc:
    port: 9090
    service: myservice  # optional
```

---

### Readiness Probes

**Readiness probe** checks if container ready to serve traffic. If fails → remove from service.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: readiness-demo
spec:
  containers:
  - name: app
    image: myapp
    
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 5
      failureThreshold: 3
```

**Liveness vs Readiness:**

```yaml
# Liveness: Is app alive?
# - Failed → Restart container
# - Use for: Deadlock detection
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  failureThreshold: 3

# Readiness: Is app ready for traffic?
# - Failed → Remove from service
# - Use for: Startup checks, dependency checks
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  failureThreshold: 1  # Quick removal from service

# Example states:
# 1. App starting: Liveness OK, Readiness FAIL
# 2. App ready: Liveness OK, Readiness OK
# 3. App overloaded: Liveness OK, Readiness FAIL
# 4. App deadlocked: Liveness FAIL → Restart!
```

---

### Startup Probes

**Startup probe** for slow-starting apps. Disables liveness/readiness until startup succeeds.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: startup-demo
spec:
  containers:
  - name: slow-app
    image: myapp
    
    startupProbe:
      httpGet:
        path: /startup
        port: 8080
      failureThreshold: 30  # 30 failures allowed
      periodSeconds: 10     # Check every 10s
      # Total: 30 * 10 = 300s (5 minutes) to start
    
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
      periodSeconds: 10
      failureThreshold: 3
    
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
      periodSeconds: 5
```

**How it works:**

```
1. Pod starts
2. Startup probe runs (liveness/readiness disabled)
3. After 30 attempts * 10s = up to 5 minutes
4. If startup succeeds:
   - Liveness probe enabled
   - Readiness probe enabled
5. If startup fails after 5 minutes:
   - Container restarted
```

**Complete Example:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: app
        image: myapp:1.0
        ports:
        - containerPort: 8080
        
        # Startup: Give 2 minutes to start
        startupProbe:
          httpGet:
            path: /actuator/health/startup
            port: 8080
          failureThreshold: 12
          periodSeconds: 10
        
        # Liveness: Check if app is alive
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness: Check if app ready for traffic
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
```

---

## 3. Pod Scheduling

### Node Selectors

**Simplest way to assign pods to nodes.**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  nodeSelector:
    gpu: "true"
    disktype: ssd
  containers:
  - name: app
    image: myapp
```

```bash
# Label nodes
kubectl label nodes node-1 gpu=true
kubectl label nodes node-1 disktype=ssd

# Pod only scheduled on nodes with both labels
```

---

### Node Affinity

**More expressive than nodeSelector.**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: affinity-demo
spec:
  affinity:
    nodeAffinity:
      # Required: Must match
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/arch
            operator: In
            values:
            - amd64
            - arm64
      
      # Preferred: Try to match (soft requirement)
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80  # Higher weight = higher preference
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
      
      - weight: 20
        preference:
          matchExpressions:
          - key: zone
            operator: In
            values:
            - us-west-1a
  
  containers:
  - name: app
    image: myapp
```

**Operators:**

```yaml
# In: Label value in list
operator: In
values: [value1, value2]

# NotIn: Label value not in list
operator: NotIn
values: [value1, value2]

# Exists: Label key exists
operator: Exists

# DoesNotExist: Label key doesn't exist
operator: DoesNotExist

# Gt: Label value greater than
operator: Gt
values: ["5"]

# Lt: Label value less than
operator: Lt
values: ["10"]
```

---

### Pod Affinity/Anti-Affinity

**Schedule pods based on other pods.**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web
spec:
  affinity:
    # Pod Affinity: Schedule near other pods
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - cache
        topologyKey: kubernetes.io/hostname
        # Schedule on same node as cache pods
    
    # Pod Anti-Affinity: Schedule away from other pods
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - web
          topologyKey: kubernetes.io/hostname
          # Prefer different nodes than other web pods
  
  containers:
  - name: web
    image: nginx
```

**Topology Keys:**

```yaml
# Node-level: Same node
topologyKey: kubernetes.io/hostname

# Zone-level: Same availability zone
topologyKey: topology.kubernetes.io/zone

# Region-level: Same region
topologyKey: topology.kubernetes.io/region
```

**Use Cases:**

```yaml
# 1. Co-locate cache with app (affinity)
podAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: webapp
    topologyKey: kubernetes.io/hostname

# 2. Spread replicas across zones (anti-affinity)
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: database
    topologyKey: topology.kubernetes.io/zone

# 3. Separate prod from test (anti-affinity)
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        environment: production
    topologyKey: kubernetes.io/hostname
```

---

### Taints and Tolerations

**Taints** repel pods from nodes. **Tolerations** allow pods to tolerate taints.

```bash
# Taint node (repel pods)
kubectl taint nodes node-1 gpu=true:NoSchedule

# Taint effects:
# NoSchedule: Don't schedule new pods
# PreferNoSchedule: Try not to schedule
# NoExecute: Don't schedule AND evict existing pods
```

**Tolerations:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  tolerations:
  # Tolerate specific taint
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  
  # Tolerate any value
  - key: "special"
    operator: "Exists"
    effect: "NoSchedule"
  
  # Tolerate NoExecute with time
  - key: "node.kubernetes.io/unreachable"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 300  # Stay 5 min on unreachable node
  
  containers:
  - name: app
    image: myapp
```

**Common Use Cases:**

```bash
# 1. Dedicated nodes for GPU workloads
kubectl taint nodes gpu-node-1 workload=gpu:NoSchedule

# 2. Nodes for specific team
kubectl taint nodes team-a-node team=a:NoSchedule

# 3. Master nodes (prevent workload pods)
kubectl taint nodes master-1 node-role.kubernetes.io/master:NoSchedule
```

---

### Pod Priority and Preemption

**Priority** determines pod importance. **Preemption** evicts lower-priority pods.

```yaml
# Create PriorityClass
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000  # Higher = more important
globalDefault: false
description: "High priority for critical apps"

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 100000
globalDefault: false

---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1000
globalDefault: true  # Default for pods without priorityClassName
```

**Using Priority:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: critical-app
spec:
  priorityClassName: high-priority
  containers:
  - name: app
    image: myapp
```

**Preemption in Action:**

```
1. High-priority pod needs scheduling
2. No node has resources
3. Scheduler finds node with low-priority pods
4. Low-priority pods evicted (preempted)
5. High-priority pod scheduled
```

**System Priorities:**

```yaml
# System critical (highest)
system-cluster-critical  # value: 2000000000
system-node-critical     # value: 2000001000

# User workloads
high-priority    # value: 1000000
medium-priority  # value: 100000
low-priority     # value: 1000
```

---

## 4. Security

### RBAC (Role-Based Access Control)

**RBAC** controls who can do what in Kubernetes.

**Core Objects:**

- **Role/ClusterRole**: Defines permissions
- **RoleBinding/ClusterRoleBinding**: Assigns permissions to users/groups

```yaml
# Role: Namespace-scoped permissions
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: production
rules:
- apiGroups: [""]  # "" = core API group
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

---
# RoleBinding: Assign role to user
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: production
subjects:
- kind: User
  name: jane@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

**ClusterRole (Cluster-wide):**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-nodes
subjects:
- kind: Group
  name: system:monitoring
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-reader
  apiGroup: rbac.authorization.k8s.io
```

**Common Verbs:**

```yaml
verbs:
- get      # Read single resource
- list     # List resources
- watch    # Watch for changes
- create   # Create new resource
- update   # Update existing resource
- patch    # Patch resource
- delete   # Delete resource
- deletecollection  # Delete multiple resources
```

---

### Service Accounts

**ServiceAccount** provides identity for pods.

```yaml
# Create ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
  namespace: production

---
# Use in Pod
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  serviceAccountName: my-service-account
  containers:
  - name: app
    image: myapp
```

**Default Service Account:**

```yaml
# Every namespace has default ServiceAccount
# Pods use it unless specified

spec:
  serviceAccountName: default  # Automatically assigned
```

**Service Account Token:**

```bash
# Token automatically mounted in pod
/var/run/secrets/kubernetes.io/serviceaccount/
├── token        # JWT token
├── ca.crt       # CA certificate
└── namespace    # Namespace name

# Use in application:
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -H "Authorization: Bearer $TOKEN" https://kubernetes.default.svc/api/v1/pods
```

---

### Pod Security Standards

**Pod Security Standards** restrict pod security configurations (replaces PSP).

**Three levels:**

**1. Privileged (unrestricted):**

```yaml
# No restrictions
# Allows everything
```

**2. Baseline (minimally restrictive):**

```yaml
# Disallows:
- Privileged containers
- Host namespaces (hostNetwork, hostPID, hostIPC)
- Host ports
- Adding capabilities beyond defaults
```

**3. Restricted (highly restrictive):**

```yaml
# Baseline + additional restrictions:
- Must run as non-root
- Cannot use privileged escalation
- Seccomp profile required
- Read-only root filesystem recommended
```

**Enforce with labels:**

```bash
# Namespace labels
kubectl label namespace production \
  pod-security.kubernetes.io/enforce=restricted \
  pod-security.kubernetes.io/audit=restricted \
  pod-security.kubernetes.io/warn=restricted
```

---

### Security Contexts

**SecurityContext** configures pod/container security.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: security-demo
spec:
  # Pod-level security context
  securityContext:
    runAsUser: 1000      # Run as UID 1000
    runAsGroup: 3000     # Run as GID 3000
    fsGroup: 2000        # Volume ownership group
    fsGroupChangePolicy: "OnRootMismatch"
    
    seccompProfile:
      type: RuntimeDefault  # Seccomp profile
    
    seLinuxOptions:
      level: "s0:c123,c456"
  
  containers:
  - name: app
    image: myapp
    
    # Container-level security context (overrides pod-level)
    securityContext:
      runAsNonRoot: true   # Must not run as root
      runAsUser: 2000      # Override pod runAsUser
      
      allowPrivilegeEscalation: false  # No privilege escalation
      
      readOnlyRootFilesystem: true  # Read-only root FS
      
      capabilities:
        drop:
        - ALL           # Drop all capabilities
        add:
        - NET_BIND_SERVICE  # Add only necessary capabilities
      
      privileged: false  # Not privileged container
```

**Capabilities:**

```yaml
# Drop all, add only needed
capabilities:
  drop:
  - ALL
  add:
  - NET_BIND_SERVICE  # Bind ports < 1024
  - CHOWN             # Change file ownership
  - DAC_OVERRIDE      # Bypass file permissions
```

**Complete Secure Pod:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-app
spec:
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    fsGroup: 1000
    seccompProfile:
      type: RuntimeDefault
  
  containers:
  - name: app
    image: myapp:latest
    
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      capabilities:
        drop:
        - ALL
    
    volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /app/cache
  
  volumes:
  - name: tmp
    emptyDir: {}
  - name: cache
    emptyDir: {}
```

---

## Summary

### Resource Management

- **Requests/Limits**: requests = guaranteed, limits = maximum
- **QoS**: Guaranteed > Burstable > BestEffort
- **Quotas**: Limit namespace resources
- **LimitRanges**: Default/min/max per resource
- **HPA**: Scale pods based on metrics
- **VPA**: Adjust pod resources
- **Cluster Autoscaler**: Add/remove nodes

### Health Checks

- **Liveness**: Restart if fails
- **Readiness**: Remove from service if fails
- **Startup**: For slow-starting apps

### Scheduling

- **NodeSelector**: Simple node selection
- **Node Affinity**: Advanced node selection
- **Pod Affinity**: Schedule near other pods
- **Taints/Tolerations**: Repel/allow pods
- **Priority**: Evict lower-priority pods

### Security

- **RBAC**: Control who can do what
- **ServiceAccounts**: Identity for pods
- **Pod Security Standards**: Privileged/Baseline/Restricted
- **Security Context**: runAsUser, capabilities, read-only FS
