
## Overview

Kubernetes provides multiple storage options for different use cases:

**Storage Types:**

- **Ephemeral**: Data lost when pod deleted (emptyDir)
- **Node-local**: Tied to specific node (hostPath)
- **Persistent**: Data survives pod deletion (PV/PVC)
- **Network**: Accessible from any node (NFS, cloud storage)

**Key Concepts:**

- **Volume**: Storage available to pod
- **PersistentVolume (PV)**: Cluster-level storage resource
- **PersistentVolumeClaim (PVC)**: Request for storage by pod
- **StorageClass**: Defines storage types and provisioners

---

## 1. Volumes

### emptyDir (Temporary Storage)

**emptyDir** creates an empty directory when pod is assigned to node. Data is deleted when pod is removed.

#### Characteristics

```
Lifecycle: Tied to pod
Location: Node's disk or memory (tmpfs)
Shared: Between containers in same pod
Persistence: Data lost when pod deleted
Use cases: Scratch space, caching, temporary data sharing
```

#### Basic emptyDir Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: cache-app
spec:
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: cache
      mountPath: /cache
  
  - name: worker
    image: worker:1.0
    volumeMounts:
    - name: cache
      mountPath: /shared-cache
  
  volumes:
  - name: cache
    emptyDir: {}  # Default: uses node disk
```

**How it works:**

```
1. Pod scheduled to node-1
2. Kubernetes creates empty directory on node-1
3. Both containers can read/write to this directory
4. Container 'app' writes to /cache
5. Container 'worker' reads from /shared-cache
6. Same directory, shared between containers
7. Pod deleted → directory deleted
```

#### emptyDir with Size Limit

```yaml
volumes:
- name: cache
  emptyDir:
    sizeLimit: 1Gi  # Maximum size
```

```bash
# If container exceeds limit:
# - Pod evicted
# - Status: Evicted
# - Reason: Ephemeral storage limit exceeded
```

#### emptyDir in Memory (tmpfs)

```yaml
volumes:
- name: memory-cache
  emptyDir:
    medium: Memory  # Store in RAM!
    sizeLimit: 512Mi
```

**Use cases for tmpfs:**

```
✅ Very fast access (RAM speed)
✅ Sensitive data (not written to disk)
✅ Temporary computation scratch space

⚠️ Counts against container memory limit
⚠️ Data lost on pod restart
⚠️ Uses node RAM
```

#### emptyDir Use Cases

**1. Caching:**

```yaml
# Web server with cache
apiVersion: v1
kind: Pod
metadata:
  name: nginx-cache
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    volumeMounts:
    - name: cache
      mountPath: /var/cache/nginx
  
  volumes:
  - name: cache
    emptyDir:
      sizeLimit: 1Gi
```

**2. Multi-container Data Sharing:**

```yaml
# Log processor pattern
apiVersion: v1
kind: Pod
metadata:
  name: log-processor
spec:
  containers:
  # App writes logs
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
  
  # Sidecar processes logs
  - name: log-shipper
    image: fluent/fluentd:v1.16
    volumeMounts:
    - name: logs
      mountPath: /var/log/app
      readOnly: true
  
  volumes:
  - name: logs
    emptyDir: {}
```

**3. Git Clone Init Container:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  initContainers:
  # Clone repository
  - name: git-clone
    image: alpine/git
    command:
    - git
    - clone
    - https://github.com/company/webapp.git
    - /data
    volumeMounts:
    - name: git-repo
      mountPath: /data
  
  containers:
  # Serve static files
  - name: nginx
    image: nginx:1.25
    volumeMounts:
    - name: git-repo
      mountPath: /usr/share/nginx/html
      readOnly: true
  
  volumes:
  - name: git-repo
    emptyDir: {}
```

---

### hostPath (Node Storage)

**hostPath** mounts a file or directory from the host node's filesystem into the pod.

#### Characteristics

```
Lifecycle: Tied to node
Location: Node's filesystem
Shared: All pods on same node can access same path
Persistence: Survives pod deletion, tied to node
Use cases: Node monitoring, Docker socket access, local testing
```

#### ⚠️ Security Warning

```
hostPath is DANGEROUS!
- Breaks pod portability (tied to node)
- Security risk (access to host filesystem)
- Can access sensitive node files
- Should be avoided in production

Use only for:
- System-level pods (DaemonSets)
- Node monitoring/logging
- Development/testing
```

#### Basic hostPath Example

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-exporter
spec:
  containers:
  - name: node-exporter
    image: prom/node-exporter:latest
    volumeMounts:
    - name: host-root
      mountPath: /host
      readOnly: true
  
  volumes:
  - name: host-root
    hostPath:
      path: /        # Mount root of host!
      type: Directory
```

#### hostPath Types

```yaml
volumes:
- name: example
  hostPath:
    path: /data/myapp
    type: DirectoryOrCreate  # Create if doesn't exist

# Available types:
# - DirectoryOrCreate: Create directory if missing
# - Directory: Must exist, must be directory
# - FileOrCreate: Create file if missing
# - File: Must exist, must be file
# - Socket: Must be socket file
# - CharDevice: Must be character device
# - BlockDevice: Must be block device
```

#### hostPath Use Cases

**1. Access Docker Socket (DaemonSet):**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: docker-monitor
spec:
  selector:
    matchLabels:
      app: docker-monitor
  template:
    metadata:
      labels:
        app: docker-monitor
    spec:
      containers:
      - name: monitor
        image: docker-monitor:1.0
        volumeMounts:
        - name: docker-socket
          mountPath: /var/run/docker.sock
      
      volumes:
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
          type: Socket
```

**2. Node Log Collection:**

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      containers:
      - name: fluentd
        image: fluent/fluentd:v1.16
        volumeMounts:
        - name: var-log
          mountPath: /var/log
          readOnly: true
        - name: var-lib-docker
          mountPath: /var/lib/docker/containers
          readOnly: true
      
      volumes:
      - name: var-log
        hostPath:
          path: /var/log
      - name: var-lib-docker
        hostPath:
          path: /var/lib/docker/containers
```

**3. Local Development (Testing Only!):**

```yaml
# ⚠️ DO NOT USE IN PRODUCTION
apiVersion: v1
kind: Pod
metadata:
  name: dev-app
spec:
  containers:
  - name: app
    image: myapp:dev
    volumeMounts:
    - name: source-code
      mountPath: /app
  
  volumes:
  - name: source-code
    hostPath:
      path: /Users/developer/myapp  # Local laptop path
      type: Directory
```

---

### PersistentVolumes (PV)

**PersistentVolume (PV)** is a cluster-level storage resource provisioned by administrator or dynamically.

#### PV Lifecycle

```
┌────────────────────────────────────────────────┐
│          PV Lifecycle States                    │
├────────────────────────────────────────────────┤
│                                                │
│  Available → Bound → Released → Failed         │
│                                                │
│  Available: Ready for claim                    │
│  Bound: Claimed by PVC                         │
│  Released: PVC deleted, but not reclaimed yet  │
│  Failed: Automatic reclaim failed              │
└────────────────────────────────────────────────┘
```

#### Static PV Provisioning

**Step 1: Administrator creates PV:**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-1
spec:
  capacity:
    storage: 10Gi
  
  volumeMode: Filesystem
  
  accessModes:
  - ReadWriteMany  # Multiple pods can read/write
  
  persistentVolumeReclaimPolicy: Retain
  
  storageClassName: nfs-storage
  
  # NFS backend
  nfs:
    server: nfs.example.com
    path: /exports/data
```

**Access Modes:**

```
ReadWriteOnce (RWO):
- Mounted read-write by single node
- Multiple pods on same node can use it
- Most common for cloud volumes (EBS, GCE PD)

ReadOnlyMany (ROX):
- Mounted read-only by many nodes
- Good for shared configuration

ReadWriteMany (RWX):
- Mounted read-write by many nodes
- Requires network filesystem (NFS, CephFS, GlusterFS)
- NOT supported by EBS, GCE PD

ReadWriteOncePod (RWOP):
- Mounted read-write by single pod only
- Kubernetes 1.22+
```

**Reclaim Policies:**

```
Retain:
- Manual reclamation required
- PV becomes Released (not Available)
- Data preserved
- Admin must manually delete PV

Delete:
- Automatic deletion
- PV and backing storage deleted
- Default for dynamically provisioned volumes

Recycle (Deprecated):
- Basic scrub (rm -rf /volume/*)
- Don't use, deprecated
```

**Step 2: User creates PVC:**

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
spec:
  accessModes:
  - ReadWriteMany
  
  resources:
    requests:
      storage: 5Gi  # Request 5Gi (PV has 10Gi)
  
  storageClassName: nfs-storage
```

**Step 3: Kubernetes binds PVC to PV:**

```bash
# PVC finds matching PV
kubectl get pvc

NAME       STATUS   VOLUME      CAPACITY   ACCESS MODES
my-claim   Bound    pv-nfs-1    10Gi       RWX

# PV is now bound
kubectl get pv

NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM
pv-nfs-1   10Gi       RWX            Retain           Bound    default/my-claim
```

**Step 4: Pod uses PVC:**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
spec:
  containers:
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: data
      mountPath: /data
  
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: my-claim
```

---

### PersistentVolumeClaims (PVC)

**PersistentVolumeClaim (PVC)** is a user's request for storage.

#### PVC Example

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: production
spec:
  # What kind of access needed
  accessModes:
  - ReadWriteOnce  # Single node read-write
  
  # Storage class (dynamic provisioning)
  storageClassName: fast-ssd
  
  # How much storage
  resources:
    requests:
      storage: 20Gi
  
  # Optional: Select specific PV by label
  selector:
    matchLabels:
      environment: production
      tier: database
```

#### Using PVC in Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: postgres
spec:
  containers:
  - name: postgres
    image: postgres:15
    volumeMounts:
    - name: postgres-storage
      mountPath: /var/lib/postgresql/data
    env:
    - name: POSTGRES_PASSWORD
      value: secret
    - name: PGDATA
      value: /var/lib/postgresql/data/pgdata
  
  volumes:
  - name: postgres-storage
    persistentVolumeClaim:
      claimName: postgres-pvc
```

#### PVC in Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: app
        image: webapp:1.0
        volumeMounts:
        - name: shared-data
          mountPath: /data
      
      volumes:
      - name: shared-data
        persistentVolumeClaim:
          claimName: shared-storage  # Must be RWX for multiple replicas!
```

#### PVC Lifecycle

```bash
# 1. Create PVC
kubectl apply -f pvc.yaml

# 2. PVC waits for PV
kubectl get pvc
NAME            STATUS    VOLUME   CAPACITY   ACCESS MODES
postgres-pvc    Pending   -        -          -

# 3. PV bound (or dynamically provisioned)
kubectl get pvc
NAME            STATUS   VOLUME      CAPACITY   ACCESS MODES
postgres-pvc    Bound    pv-001      20Gi       RWO

# 4. Pod uses PVC
kubectl apply -f pod.yaml

# 5. Delete pod (PVC remains)
kubectl delete pod postgres
# PVC still exists, data preserved!

# 6. Delete PVC
kubectl delete pvc postgres-pvc
# PV reclaim policy determines what happens to data
```

---

## 2. Storage Classes

**StorageClass** defines types of storage and how to provision them dynamically.

### Why Storage Classes?

**Without StorageClass (Manual):**

```
1. Admin creates PV manually
2. User creates PVC
3. Kubernetes binds PVC to PV
4. Limited flexibility, manual work
```

**With StorageClass (Dynamic):**

```
1. User creates PVC with storageClassName
2. StorageClass automatically provisions PV
3. PV bound to PVC automatically
4. Automatic, scalable, easy!
```

---

### Dynamic Provisioning

#### Basic StorageClass

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iopsPerGB: "10"
  fsType: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

**Fields Explained:**

**provisioner:**

```
Determines which volume plugin to use
Examples:
- kubernetes.io/aws-ebs (AWS EBS)
- kubernetes.io/gce-pd (GCE Persistent Disk)
- kubernetes.io/azure-disk (Azure Disk)
- kubernetes.io/cinder (OpenStack)
- k8s.io/minikube-hostpath (Minikube)
- External: csi.storage.k8s.io/* (CSI drivers)
```

**parameters:**

```
Provisioner-specific parameters
AWS EBS:
- type: gp3, gp2, io1, io2, st1, sc1
- iopsPerGB: IOPS per GB (io1, io2, gp3)
- encrypted: "true" or "false"

GCE PD:
- type: pd-standard, pd-ssd, pd-balanced
- replication-type: none, regional-pd
```

**reclaimPolicy:**

```
Delete: Delete PV when PVC deleted (default)
Retain: Keep PV when PVC deleted
```

**allowVolumeExpansion:**

```
true: Allow resizing PVC
false: No resizing (default)
```

**volumeBindingMode:**

```
Immediate: Provision PV as soon as PVC created
WaitForFirstConsumer: Wait until pod using PVC is scheduled
  - Better for topology-constrained volumes
  - Ensures volume created in same zone as pod
```

#### Using StorageClass

```yaml
# PVC requesting dynamic provisioning
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-dynamic-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast-ssd  # References StorageClass
  resources:
    requests:
      storage: 10Gi
```

**What happens:**

```
1. User creates PVC with storageClassName: fast-ssd
2. Kubernetes sees StorageClass "fast-ssd"
3. Calls AWS EBS provisioner
4. Creates 10Gi gp3 EBS volume
5. Creates PV representing this volume
6. Binds PVC to PV
7. Pod can now use PVC
```

#### Default StorageClass

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
```

```yaml
# PVC without storageClassName uses default
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  # No storageClassName specified → uses default
```

---

### Different Storage Backends

#### AWS EBS (Elastic Block Store)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: aws-ebs-gp3
provisioner: ebs.csi.aws.com  # CSI driver (modern)
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
  kmsKeyId: "arn:aws:kms:us-east-1:123456789:key/abcd-1234"
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
```

**EBS Volume Types:**

```
gp3: General Purpose SSD (latest)
  - 3000 IOPS baseline, up to 16000 IOPS
  - Cost: ~$0.08/GB/month
  - Best for: Most workloads

gp2: General Purpose SSD (legacy)
  - 3 IOPS per GB, up to 16000 IOPS
  - Cost: ~$0.10/GB/month

io2: Provisioned IOPS SSD
  - Up to 64000 IOPS
  - Cost: ~$0.125/GB/month + IOPS cost
  - Best for: Databases, high performance

st1: Throughput Optimized HDD
  - Throughput: 500 MB/s
  - Cost: ~$0.045/GB/month
  - Best for: Big data, log processing

sc1: Cold HDD
  - Throughput: 250 MB/s
  - Cost: ~$0.015/GB/month
  - Best for: Infrequent access
```

#### GCE Persistent Disk

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gce-pd-ssd
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd
  replication-type: regional-pd
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

**GCE PD Types:**

```
pd-standard: Standard persistent disk (HDD)
  - Cost: ~$0.040/GB/month

pd-balanced: Balanced persistent disk (SSD)
  - Cost: ~$0.100/GB/month
  - Best for: Most workloads

pd-ssd: SSD persistent disk
  - Cost: ~$0.170/GB/month
  - Best for: High performance

pd-extreme: Extreme persistent disk
  - Cost: ~$0.300/GB/month
  - Best for: Highest performance
```

#### Azure Disk

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk-premium
provisioner: disk.csi.azure.com
parameters:
  skuName: Premium_LRS
  kind: Managed
allowVolumeExpansion: true
reclaimPolicy: Delete
```

**Azure Disk SKUs:**

```
Standard_LRS: Standard HDD
Premium_LRS: Premium SSD
StandardSSD_LRS: Standard SSD
UltraSSD_LRS: Ultra Disk
```

#### NFS (Network File System)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs
provisioner: nfs.csi.k8s.io
parameters:
  server: nfs.example.com
  share: /exports/data
mountOptions:
  - hard
  - nfsvers=4.1
```

**Static NFS PV:**

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteMany  # NFS supports RWX!
  nfs:
    server: nfs.example.com
    path: /exports/data
  storageClassName: nfs
```

#### Ceph (RBD - RADOS Block Device)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-rbd
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: <ceph-cluster-id>
  pool: kubernetes
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: ceph-secret
  csi.storage.k8s.io/provisioner-secret-namespace: default
reclaimPolicy: Delete
allowVolumeExpansion: true
```

#### CephFS (Ceph Filesystem)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: cephfs
provisioner: cephfs.csi.ceph.com
parameters:
  clusterID: <ceph-cluster-id>
  fsName: myfs
  pool: cephfs_data
  csi.storage.k8s.io/provisioner-secret-name: ceph-secret
```

#### Comparison Table

|Backend|Access Modes|Performance|Cost|Use Case|
|---|---|---|---|---|
|**AWS EBS**|RWO|High|Medium|General purpose|
|**GCE PD**|RWO|High|Medium|General purpose|
|**Azure Disk**|RWO|High|Medium|General purpose|
|**NFS**|RWX, ROX|Medium|Low|Shared storage|
|**CephFS**|RWX, ROX|High|Medium|Distributed storage|
|**GlusterFS**|RWX, ROX|Medium|Low|Distributed storage|
|**Portworx**|RWO, RWX|Very High|High|Enterprise storage|

---

### Access Modes Explained

```
┌─────────────────────────────────────────────────────┐
│              Access Modes                            │
├─────────────────────────────────────────────────────┤
│                                                      │
│  ReadWriteOnce (RWO)                                │
│  ┌─────────┐     ┌─────────┐                       │
│  │ Node 1  │     │ Node 2  │                       │
│  │ ┌─────┐ │     │         │                       │
│  │ │Pod A│ │     │         │                       │
│  │ │Pod B│ │     │         │                       │
│  │ └──┬──┘ │     │         │                       │
│  │    │    │     │         │                       │
│  │ ┌──▼──┐ │     │         │                       │
│  │ │ PV  │ │     │         │                       │
│  │ └─────┘ │     │         │                       │
│  └─────────┘     └─────────┘                       │
│  ✅ Pod A and Pod B can use (same node)            │
│  ❌ Pod on Node 2 cannot use                        │
│                                                      │
│  ReadWriteMany (RWX)                                │
│  ┌─────────┐     ┌─────────┐                       │
│  │ Node 1  │     │ Node 2  │                       │
│  │ ┌─────┐ │     │ ┌─────┐ │                       │
│  │ │Pod A│ │     │ │Pod C│ │                       │
│  │ └──┬──┘ │     │ └──┬──┘ │                       │
│  │    │    │     │    │    │                       │
│  │    └────┼─────┼────┘    │                       │
│  │         │     │         │                       │
│  └─────────┼─────┼─────────┘                       │
│            │     │                                  │
│         ┌──▼─────▼──┐                              │
│         │  NFS PV   │                              │
│         └───────────┘                              │
│  ✅ All pods can read/write                         │
│                                                      │
└─────────────────────────────────────────────────────┘
```

**Access Mode Support by Backend:**

|Backend|RWO|ROX|RWX|
|---|---|---|---|
|**AWS EBS**|✅|❌|❌|
|**GCE PD**|✅|✅|❌|
|**Azure Disk**|✅|❌|❌|
|**NFS**|✅|✅|✅|
|**CephFS**|✅|✅|✅|
|**GlusterFS**|✅|✅|✅|

---

### Volume Expansion

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: expandable
provisioner: kubernetes.io/aws-ebs
allowVolumeExpansion: true  # Allow resizing
```

**Expand PVC:**

```bash
# Original PVC: 10Gi
kubectl get pvc my-pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES
my-pvc   Bound    pv-001   10Gi       RWO

# Edit PVC to increase size
kubectl edit pvc my-pvc
# Change: storage: 10Gi → storage: 20Gi

# Expansion happens automatically
kubectl get pvc my-pvc
NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES
my-pvc   Bound    pv-001   20Gi       RWO
```

**Requirements:**

- ✅ StorageClass has `allowVolumeExpansion: true`
- ✅ Volume plugin supports expansion
- ⚠️ Can only INCREASE size (not decrease)
- ⚠️ Some volume types require pod restart

---

## 3. StatefulSets with Storage

**StatefulSet** provides stable storage for stateful applications.

### Volume Claim Templates

**Problem with Deployment + PVC:**

```yaml
# ❌ All replicas share same PVC
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: db
        image: postgres:15
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: shared-pvc

# Problem:
# - All 3 replicas use same PVC
# - Data conflicts!
# - Not suitable for databases
```

**Solution: StatefulSet with volumeClaimTemplates:**

```yaml
# ✅ Each replica gets own PVC
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: secret
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  
  # Volume Claim Template
  # Creates separate PVC for each pod!
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 20Gi
```

**What gets created:**

```bash
# Pods created in order
kubectl get pods

NAME         READY   STATUS    RESTARTS   AGE
postgres-0   1/1     Running   0          1m
postgres-1   1/1     Running   0          50s
postgres-2   1/1     Running   0          40s

# Separate PVC for each pod
kubectl get pvc

NAME                        STATUS   VOLUME    CAPACITY   ACCESS MODES
postgres-storage-postgres-0   Bound    pv-001    20Gi       RWO
postgres-storage-postgres-1   Bound    pv-002    20Gi       RWO
postgres-storage-postgres-2   Bound    pv-003    20Gi       RWO

# Each PVC bound to different PV
kubectl get pv

NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM
pv-001   20Gi       RWO            Delete           Bound    default/postgres-storage-postgres-0
pv-002   20Gi       RWO            Delete           Bound    default/postgres-storage-postgres-1
pv-003   20Gi       RWO            Delete           Bound    default/postgres-storage-postgres-2
```

---

### Persistent Identity

**StatefulSet provides stable, unique identities:**

```
Pod Names:     postgres-0, postgres-1, postgres-2
Network IDs:   postgres-0.postgres.default.svc.cluster.local
Storage:       postgres-storage-postgres-0, postgres-storage-postgres-1, ...

Characteristics:
✅ Predictable names (not random like Deployment)
✅ Stable DNS names
✅ Persistent storage tied to pod identity
✅ Survives pod restarts, rescheduling
```

#### Example: PostgreSQL Primary-Replica

```yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  labels:
    app: postgres
spec:
  ports:
  - port: 5432
    name: postgres
  clusterIP: None  # Headless service
  selector:
    app: postgres

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  serviceName: postgres
  replicas: 3
  selector:
    matchLabels:
      app: postgres
  
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        
        # Determine if primary or replica
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        
        command:
        - bash
        - -c
        - |
          if [[ "$POD_NAME" == "postgres-0" ]]; then
            echo "Starting as PRIMARY"
            # Primary configuration
          else
            echo "Starting as REPLICA"
            # Replica configuration, connect to postgres-0
          fi
          docker-entrypoint.sh postgres
        
        ports:
        - containerPort: 5432
          name: postgres
        
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 50Gi
```

**DNS Resolution:**

```bash
# Individual pods accessible via DNS
postgres-0.postgres.default.svc.cluster.local  # Primary
postgres-1.postgres.default.svc.cluster.local  # Replica 1
postgres-2.postgres.default.svc.cluster.local  # Replica 2

# Application connects to primary:
psql -h postgres-0.postgres.default.svc.cluster.local -U postgres

# Or use read replicas:
psql -h postgres-1.postgres.default.svc.cluster.local -U postgres
```

---

### Ordered Deployment/Scaling

**StatefulSet deploys and scales pods in order:**

#### Deployment Order

```
Creating StatefulSet with 3 replicas:

Time: t=0
- Create postgres-0
- Wait for postgres-0 to be Running and Ready

Time: t=30s (after postgres-0 ready)
- Create postgres-1
- Wait for postgres-1 to be Running and Ready

Time: t=60s (after postgres-1 ready)
- Create postgres-2
- Wait for postgres-2 to be Running and Ready

Time: t=90s
- All pods running
- Total time: 90s (sequential, not parallel!)
```

```bash
# Watch deployment
kubectl get pods -w

NAME         READY   STATUS              RESTARTS   AGE
postgres-0   0/1     ContainerCreating   0          0s
postgres-0   1/1     Running             0          30s
postgres-1   0/1     ContainerCreating   0          0s
postgres-1   1/1     Running             0          30s
postgres-2   0/1     ContainerCreating   0          0s
postgres-2   1/1     Running             0          30s
```

#### Scaling Order

**Scale up (3 → 5):**

```bash
kubectl scale statefulset postgres --replicas=5

# Creates in order:
# postgres-3 (wait until ready)
# postgres-4 (wait until ready)
```

**Scale down (5 → 3):**

```bash
kubectl scale statefulset postgres --replicas=3

# Deletes in reverse order:
# postgres-4 (wait until terminated)
# postgres-3 (wait until terminated)
# ⚠️ PVCs NOT deleted! (data preserved)
```

**Why ordered?**

```
Databases need ordered startup:
1. Primary starts first (postgres-0)
2. Replicas start, connect to primary
3. Replicas sync data before becoming ready

Ordered shutdown:
1. Remove replica (postgres-2)
2. Wait for graceful shutdown
3. Remove next replica (postgres-1)
4. Ensures data consistency
```

#### Update Strategy

```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
```

**Rolling update (reverse order):**

```
Update StatefulSet image:

Time: t=0
- Update postgres-2 (highest ordinal first)
- Wait for postgres-2 to be Running and Ready

Time: t=30s
- Update postgres-1
- Wait for postgres-1 to be Running and Ready

Time: t=60s
- Update postgres-0
- Wait for postgres-0 to be Running and Ready

Time: t=90s
- All pods updated
```

**Canary testing with partition:**

```yaml
spec:
  updateStrategy:
    rollingUpdate:
      partition: 2  # Only update pods >= 2

# Result:
# postgres-2: Updated (new version)
# postgres-1: Not updated (old version)
# postgres-0: Not updated (old version)

# Test postgres-2, if good:
kubectl patch statefulset postgres -p '{"spec":{"updateStrategy":{"rollingUpdate":{"partition":0}}}}'
# Now updates postgres-1, postgres-0
```

---

### StatefulSet Storage Management

#### Deleting StatefulSet

```bash
# Delete StatefulSet (keeps PVCs!)
kubectl delete statefulset postgres

# Pods deleted in reverse order:
# postgres-2 → postgres-1 → postgres-0

# But PVCs remain:
kubectl get pvc
NAME                        STATUS   VOLUME   CAPACITY
postgres-storage-postgres-0   Bound    pv-001   50Gi
postgres-storage-postgres-1   Bound    pv-002   50Gi
postgres-storage-postgres-2   Bound    pv-003   50Gi

# Recreate StatefulSet:
kubectl apply -f statefulset.yaml
# Pods reuse existing PVCs!
# Data preserved!
```

#### Manually Delete PVCs

```bash
# Delete specific replica's data
kubectl delete pvc postgres-storage-postgres-2

# Delete all PVCs (careful!)
kubectl delete pvc -l app=postgres
```

#### PVC Retention Policy (Kubernetes 1.23+)

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain  # Keep PVCs when StatefulSet deleted
    whenScaled: Delete   # Delete PVCs when scaled down
```

**Policies:**

```
Retain: Keep PVCs (default, data safe)
Delete: Delete PVCs (data lost)
```

---

## Complete Example: WordPress with MySQL

```yaml
# MySQL StatefulSet with persistent storage
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  ports:
  - port: 3306
  clusterIP: None
  selector:
    app: mysql

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
  
  volumeClaimTemplates:
  - metadata:
      name: mysql-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 20Gi

---
# WordPress Deployment with persistent uploads
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-uploads
spec:
  accessModes:
  - ReadWriteMany  # Multiple WordPress pods share uploads
  storageClassName: nfs
  resources:
    requests:
      storage: 10Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 3
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:6.4
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-0.mysql.default.svc.cluster.local
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-uploads
          mountPath: /var/www/html/wp-content/uploads
      
      volumes:
      - name: wordpress-uploads
        persistentVolumeClaim:
          claimName: wordpress-uploads

---
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
  selector:
    app: wordpress
```

---

## Best Practices

### General

**✅ DO:**

- Use PV/PVC for production workloads
- Enable volume expansion (`allowVolumeExpansion: true`)
- Use `volumeBindingMode: WaitForFirstConsumer` for topology
- Set resource limits on pods using storage
- Backup important data regularly
- Test restore procedures

**❌ DON'T:**

- Use hostPath in production
- Use emptyDir for important data
- Share RWO volumes between pods on different nodes
- Forget to delete unused PVCs (cost!)
- Commit secrets to Git

### StatefulSets

**✅ DO:**

- Use StatefulSets for databases, queues, stateful apps
- Use headless service with StatefulSets
- Implement proper health checks
- Use init containers for setup
- Test failover scenarios

**❌ DON'T:**

- Use Deployments for stateful apps
- Delete PVCs accidentally (data loss!)
- Skip pod disruption budgets
- Forget about ordered shutdown

### Storage Classes

**✅ DO:**

- Create multiple StorageClasses (fast, standard, slow)
- Set appropriate reclaim policies
- Use cloud provider's CSI drivers
- Document storage characteristics

**❌ DON'T:**

- Use only default StorageClass
- Use Retain for temporary data
- Forget to clean up released PVs

---

## Summary

### Volume Types Quick Reference

|Type|Lifecycle|Shared|Persistent|Use Case|
|---|---|---|---|---|
|**emptyDir**|Pod|Within pod|No|Cache, scratch|
|**hostPath**|Node|No|Node-tied|System pods, testing|
|**PVC/PV**|Cluster|Depends on access mode|Yes|Production data|

### Access Modes

|Mode|Abbreviation|Description|Example|
|---|---|---|---|
|ReadWriteOnce|RWO|Single node read-write|Database|
|ReadOnlyMany|ROX|Multiple nodes read-only|Config|
|ReadWriteMany|RWX|Multiple nodes read-write|Shared files|
|ReadWriteOncePod|RWOP|Single pod only|Exclusive access|

### Key Takeaways

**Volumes:**

- emptyDir: Temporary, fast, pod-scoped
- hostPath: Node storage, dangerous, avoid in production
- PV/PVC: Persistent, production-ready, decoupled from pods

**Storage Classes:**

- Dynamic provisioning (automatic PV creation)
- Different backends (EBS, GCE PD, NFS, Ceph)
- Access modes determine sharing capability
- Volume expansion for growing data

**StatefulSets:**

- Stable pod identities (predictable names)
- Persistent storage per pod (volumeClaimTemplates)
- Ordered deployment and scaling
- Perfect for databases, queues, clustered apps
