# CI/CD Best Practices

## 1. Pipeline Design

### Fast Feedback (Fail Fast)

**Goal:** Detect and report failures as quickly as possible.

```
Traditional Pipeline (Slow Feedback):
┌──────────────────────────────────────────┐
│  Build (10 min)                          │
│      ↓                                   │
│  Test (20 min)                           │
│      ↓                                   │
│  Security Scan (15 min)                  │
│      ↓                                   │
│  Deploy (5 min)                          │
│                                          │
│  Total: 50 minutes to discover failure   │
└──────────────────────────────────────────┘

Optimized Pipeline (Fast Feedback):
┌──────────────────────────────────────────┐
│  Lint + Quick Tests (2 min) → FAIL FAST │
│      ↓ (only if passed)                  │
│  Build (10 min)                          │
│      ↓                                   │
│  Parallel:                               │
│  ├─ Unit Tests (5 min)                   │
│  ├─ Integration Tests (10 min)          │
│  └─ Security Scan (5 min)               │
│      ↓                                   │
│  Deploy (5 min)                          │
│                                          │
│  Total: 2-20 minutes depending on stage  │
└──────────────────────────────────────────┘
```

**Implementation:**

```yaml
# GitLab CI - Fail Fast Example
stages:
  - validate      # Fast checks first
  - build
  - test
  - security
  - deploy

# Stage 1: Quick validation (fail fast)
lint:
  stage: validate
  script:
    - npm run lint
    - mvn checkstyle:check
  # Fails in 30 seconds if code quality issues

code-format:
  stage: validate
  script:
    - mvn spotless:check
  # Fails in 10 seconds if formatting wrong

quick-tests:
  stage: validate
  script:
    - mvn test -Dtest=*UnitTest
  # Run only unit tests, fail in 2 minutes

# Stage 2: Build (only if validation passed)
build:
  stage: build
  script:
    - mvn clean package -DskipTests
  artifacts:
    paths:
      - target/*.jar
    expire_in: 1 hour

# Stage 3: Comprehensive tests (parallel)
unit-tests:
  stage: test
  script:
    - mvn test

integration-tests:
  stage: test
  script:
    - mvn verify -DskipUnitTests

# Stage 4: Security (parallel with tests)
security-scan:
  stage: test  # Same stage = parallel
  script:
    - trivy fs .
```

**GitHub Actions - Fail Fast:**

```yaml
name: CI with Fast Feedback

on: [push, pull_request]

jobs:
  # Job 1: Quick validation (runs first)
  quick-checks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Lint code
        run: npm run lint
      
      - name: Check formatting
        run: npm run format:check
      
      - name: Quick unit tests
        run: npm run test:unit:quick
  
  # Job 2: Build (only if quick-checks pass)
  build:
    needs: quick-checks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Build
        run: mvn clean package -DskipTests
  
  # Job 3: Comprehensive tests (parallel)
  test:
    needs: build
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue all tests even if one fails
      matrix:
        test-suite: [unit, integration, e2e]
    steps:
      - uses: actions/checkout@v4
      - name: Run ${{ matrix.test-suite }} tests
        run: npm run test:${{ matrix.test-suite }}
```

**Best Practices:**

```
1. Run fastest checks first
   ✓ Linting: 10-30 seconds
   ✓ Code formatting: 10-30 seconds
   ✓ Quick unit tests: 1-2 minutes
   ✓ Build: 5-10 minutes
   ✓ Full test suite: 10-30 minutes

2. Fail immediately on first error
   ✓ Don't wait for slow tests if lint fails
   ✓ Use fail-fast: true for critical checks
   ✓ Set short timeouts

3. Provide clear error messages
   ✓ Show which test failed
   ✓ Include logs and artifacts
   ✓ Link to relevant documentation

4. Notify developers quickly
   ✓ Slack/email notifications
   ✓ PR comments with failure details
   ✓ Build status badges
```

---

### Parallel Execution

**Maximize throughput by running independent tasks concurrently.**

```yaml
# GitLab CI - Parallel Execution
stages:
  - build
  - test
  - deploy

build:
  stage: build
  script:
    - mvn clean package
  artifacts:
    paths:
      - target/*.jar

# All test jobs run in parallel (same stage)
unit-tests:
  stage: test
  script:
    - mvn test

integration-tests:
  stage: test
  script:
    - mvn verify -DskipUnitTests

security-scan:
  stage: test
  script:
    - trivy fs .

code-quality:
  stage: test
  script:
    - sonar-scanner

# Execution time:
# Sequential: 10 + 15 = 25 minutes
# Parallel: max(10, 15) = 15 minutes
```

**Jenkins - Parallel Stages:**

```groovy
pipeline {
    agent any
    
    stages {
        stage('Build') {
            steps {
                sh 'mvn clean package'
            }
        }
        
        stage('Test') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        sh 'mvn test'
                    }
                }
                
                stage('Integration Tests') {
                    steps {
                        sh 'mvn verify -DskipUnitTests'
                    }
                }
                
                stage('Security Scan') {
                    steps {
                        sh 'trivy fs .'
                    }
                }
                
                stage('Code Quality') {
                    steps {
                        sh 'sonar-scanner'
                    }
                }
            }
        }
        
        stage('Deploy') {
            steps {
                sh './deploy.sh'
            }
        }
    }
}
```

**GitHub Actions - Matrix Strategy:**

```yaml
name: Parallel Testing

on: push

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        test-type: [unit, integration, e2e]
        java-version: [11, 17, 21]
      # Creates 9 jobs (3 test types × 3 Java versions)
      max-parallel: 9  # Run all simultaneously
      fail-fast: false  # Continue even if one fails
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up JDK ${{ matrix.java-version }}
        uses: actions/setup-java@v3
        with:
          java-version: ${{ matrix.java-version }}
      
      - name: Run ${{ matrix.test-type }} tests
        run: mvn verify -Dtest.type=${{ matrix.test-type }}
```

---

### Caching Dependencies

**Reduce build time by caching downloaded dependencies.**

**Maven Cache (GitLab CI):**

```yaml
variables:
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"

cache:
  key: ${CI_COMMIT_REF_SLUG}
  paths:
    - .m2/repository

build:
  script:
    - mvn clean package
  # First run: 10 minutes (download deps)
  # Subsequent runs: 2 minutes (use cache)
```

**Gradle Cache (GitHub Actions):**

```yaml
jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: 'gradle'  # Automatic Gradle caching
      
      - name: Build
        run: ./gradlew build
```

**Docker Layer Caching:**

```yaml
# GitHub Actions
- name: Set up Docker Buildx
  uses: docker/setup-buildx-action@v3

- name: Build and push
  uses: docker/build-push-action@v5
  with:
    context: .
    push: true
    tags: myapp:latest
    cache-from: type=registry,ref=myapp:buildcache
    cache-to: type=registry,ref=myapp:buildcache,mode=max
```

**NPM/Yarn Cache:**

```yaml
# GitHub Actions
- name: Cache node modules
  uses: actions/cache@v3
  with:
    path: ~/.npm
    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
    restore-keys: |
      ${{ runner.os }}-node-
```

**Cache Best Practices:**

```
1. Cache stable dependencies
   ✓ Maven: .m2/repository
   ✓ Gradle: .gradle/caches
   ✓ NPM: node_modules
   ✓ Docker: layer cache

2. Use appropriate cache keys
   ✓ Include branch name
   ✓ Hash lock files
   ✓ Version by build tool

3. Set cache expiration
   ✓ GitLab: Default 1 week
   ✓ GitHub: Default 7 days
   ✓ Clear cache on major updates

4. Monitor cache effectiveness
   ✓ Track cache hit rate
   ✓ Measure time saved
   ✓ Optimize cache size
```

---

### Artifact Management

**Store and manage build outputs efficiently.**

```yaml
# GitLab CI - Artifacts
build:
  stage: build
  script:
    - mvn clean package
  artifacts:
    name: "myapp-$CI_COMMIT_SHORT_SHA"
    paths:
      - target/*.jar
      - target/site/jacoco
    reports:
      junit: target/surefire-reports/TEST-*.xml
      coverage_report:
        coverage_format: cobertura
        path: target/site/jacoco/jacoco.xml
    expire_in: 30 days  # Cleanup old artifacts

test:
  stage: test
  dependencies:
    - build  # Download artifacts from build job
  script:
    - java -jar target/*.jar --test

deploy:
  stage: deploy
  dependencies:
    - build
  script:
    - scp target/*.jar server:/app/
```

**Artifact Repository (Nexus/Artifactory):**

```yaml
# Maven deploy to Nexus
deploy:
  stage: deploy
  script:
    - mvn deploy -DskipTests
  only:
    - main
    - tags

# settings.xml
<servers>
  <server>
    <id>nexus</id>
    <username>${env.NEXUS_USERNAME}</username>
    <password>${env.NEXUS_PASSWORD}</password>
  </server>
</servers>
```

**Container Registry:**

```yaml
# Push to registry
docker-build:
  stage: package
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:latest
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest
```

**Artifact Best Practices:**

```
1. Version everything
   ✓ Use semantic versioning
   ✓ Include commit SHA
   ✓ Tag releases

2. Set retention policies
   ✓ Keep releases: Forever
   ✓ Keep snapshots: 30 days
   ✓ Keep PR builds: 7 days

3. Secure artifacts
   ✓ Use private registries
   ✓ Scan for vulnerabilities
   ✓ Sign artifacts

4. Track dependencies
   ✓ Generate SBOM (Software Bill of Materials)
   ✓ Monitor for CVEs
   ✓ Update regularly
```

---

## 2. Testing Strategy

### Test Pyramid

```
         /\           E2E Tests
        /  \          (Few, Slow, Brittle)
       /────\         - User journeys
      /      \        - Critical paths only
     /        \       - 5-10% of tests
    /──────────\      
   /            \     Integration Tests
  /              \    (Some, Moderate)
 /────────────────\   - Component interactions
/                  \  - Database, APIs
/──────────────────\  - 20-30% of tests
                      
──────────────────    Unit Tests
                      (Many, Fast, Reliable)
                      - Individual functions
                      - Business logic
                      - 60-70% of tests
```

---

### Unit Tests (Fast, Always Run)

**Characteristics:**

- ✓ Fast (< 0.1s per test)
- ✓ Isolated (no external dependencies)
- ✓ Deterministic (same input → same output)
- ✓ Run on every commit
- ✓ 60-70% of test suite

```java
// Unit Test Example
@Test
public void shouldCalculateDiscountCorrectly() {
    // Given
    Order order = new Order(100.0);
    DiscountService service = new DiscountService();
    
    // When
    double discount = service.calculateDiscount(order, "SAVE10");
    
    // Then
    assertEquals(10.0, discount, 0.01);
}
```

**Pipeline Integration:**

```yaml
# Always run unit tests
unit-tests:
  stage: test
  script:
    - mvn test
  coverage: '/Total.*?([0-9]{1,3})%/'
  artifacts:
    reports:
      junit: target/surefire-reports/TEST-*.xml
      coverage_report:
        coverage_format: cobertura
        path: target/site/jacoco/jacoco.xml
```

---

### Integration Tests (Slower, Critical Paths)

**Characteristics:**

- ✓ Moderate speed (1-10s per test)
- ✓ Test component interactions
- ✓ Use real dependencies (DB, APIs)
- ✓ Run on main branch / before release
- ✓ 20-30% of test suite

```java
// Integration Test Example
@SpringBootTest
@AutoConfigureTestDatabase
public class OrderServiceIntegrationTest {
    
    @Autowired
    private OrderService orderService;
    
    @Autowired
    private OrderRepository orderRepository;
    
    @Test
    public void shouldPersistOrderToDatabase() {
        // Given
        Order order = new Order("customer-123", 100.0);
        
        // When
        Order saved = orderService.createOrder(order);
        
        // Then
        Order found = orderRepository.findById(saved.getId()).orElseThrow();
        assertEquals("customer-123", found.getCustomerId());
    }
}
```

**Pipeline Integration:**

```yaml
integration-tests:
  stage: test
  services:
    - postgres:15
    - redis:7
  variables:
    DATABASE_URL: postgresql://postgres:5432/testdb
    REDIS_URL: redis://redis:6379
  script:
    - mvn verify -DskipUnitTests
  only:
    - main
    - merge_requests
```

---

### E2E Tests (Slowest, Smoke Tests Only)

**Characteristics:**

- ✓ Slow (10-60s per test)
- ✓ Test complete user workflows
- ✓ Use real browsers/applications
- ✓ Run before production deployment
- ✓ 5-10% of test suite (smoke tests only!)

```javascript
// E2E Test Example (Playwright)
test('user can complete checkout', async ({ page }) => {
  // Navigate to product page
  await page.goto('https://example.com/products/1');
  
  // Add to cart
  await page.click('#add-to-cart');
  
  // Go to checkout
  await page.click('#checkout');
  
  // Fill form
  await page.fill('#email', 'test@example.com');
  await page.fill('#card', '4242424242424242');
  
  // Submit
  await page.click('#submit');
  
  // Verify success
  await expect(page.locator('#confirmation')).toBeVisible();
});
```

**Pipeline Integration:**

```yaml
e2e-tests:
  stage: test
  image: mcr.microsoft.com/playwright:latest
  script:
    # Start application
    - docker-compose up -d
    - sleep 30  # Wait for app to start
    
    # Run E2E tests
    - npm run test:e2e
  only:
    - main
  when: manual  # Run manually before production
```

---

### Test Coverage Thresholds

```yaml
# Maven - JaCoCo Configuration
<plugin>
  <groupId>org.jacoco</groupId>
  <artifactId>jacoco-maven-plugin</artifactId>
  <executions>
    <execution>
      <id>check</id>
      <goals>
        <goal>check</goal>
      </goals>
      <configuration>
        <rules>
          <rule>
            <element>PACKAGE</element>
            <limits>
              <limit>
                <counter>LINE</counter>
                <value>COVEREDRATIO</value>
                <minimum>0.80</minimum>  <!-- 80% minimum -->
              </limit>
              <limit>
                <counter>BRANCH</counter>
                <value>COVEREDRATIO</value>
                <minimum>0.70</minimum>  <!-- 70% branch coverage -->
              </limit>
            </limits>
          </rule>
        </rules>
      </configuration>
    </execution>
  </executions>
</plugin>
```

**Coverage Targets:**

```
Recommended thresholds:
- Unit test coverage: 80%+
- Critical business logic: 90%+
- Integration test coverage: 60%+
- Overall project coverage: 75%+

Don't aim for 100%:
- Diminishing returns after 80%
- Focus on critical code paths
- Test behavior, not implementation
```

---

## 3. Security in CI/CD

### Secret Management

**Bad Practice (Never do this!):**

```yaml
# ❌ NEVER commit secrets to code
deploy:
  script:
    - kubectl apply -f deployment.yaml
  environment:
    DATABASE_PASSWORD: "mypassword123"  # EXPOSED!
    API_KEY: "sk-abc123xyz"  # EXPOSED!
```

**Good Practice - GitLab CI Variables:**

```yaml
# Settings > CI/CD > Variables
# Add: DATABASE_PASSWORD (masked, protected)
# Add: API_KEY (masked, protected)

deploy:
  script:
    - echo "Password length: ${#DATABASE_PASSWORD}"  # Show length only
    - kubectl create secret generic app-secrets \
        --from-literal=password=$DATABASE_PASSWORD \
        --from-literal=api-key=$API_KEY
  only:
    - main  # Protected branch
```

**HashiCorp Vault Integration:**

```yaml
# GitLab CI with Vault
deploy:
  image: vault:latest
  before_script:
    # Authenticate with Vault
    - export VAULT_ADDR=https://vault.example.com
    - export VAULT_TOKEN=$(vault write -field=token auth/jwt/login role=gitlab-ci jwt=$CI_JOB_JWT)
  script:
    # Fetch secrets from Vault
    - export DB_PASSWORD=$(vault kv get -field=password secret/database)
    - export API_KEY=$(vault kv get -field=key secret/api)
    
    # Use secrets
    - kubectl create secret generic app-secrets \
        --from-literal=password=$DB_PASSWORD \
        --from-literal=api-key=$API_KEY
```

**AWS Secrets Manager:**

```yaml
# GitHub Actions with AWS Secrets Manager
- name: Configure AWS credentials
  uses: aws-actions/configure-aws-credentials@v2
  with:
    role-to-assume: arn:aws:iam::123456789012:role/GitHubActions
    aws-region: us-east-1

- name: Get secrets
  run: |
    DB_PASSWORD=$(aws secretsmanager get-secret-value \
      --secret-id production/database/password \
      --query SecretString --output text)
    
    echo "::add-mask::$DB_PASSWORD"  # Mask in logs
    echo "DB_PASSWORD=$DB_PASSWORD" >> $GITHUB_ENV
```

**Best Practices:**

```
1. Never commit secrets to code
   ✓ Use .gitignore for config files
   ✓ Scan commits for secrets (git-secrets)
   ✓ Rotate exposed secrets immediately

2. Use CI/CD secret management
   ✓ GitLab CI/CD Variables (masked, protected)
   ✓ GitHub Secrets (encrypted at rest)
   ✓ Jenkins Credentials Plugin

3. Use external secret managers
   ✓ HashiCorp Vault
   ✓ AWS Secrets Manager
   ✓ Azure Key Vault
   ✓ Google Secret Manager

4. Implement least privilege
   ✓ One secret per service
   ✓ Rotate secrets regularly
   ✓ Audit secret access
   ✓ Expire unused secrets
```

---

### Image Scanning (Trivy, Snyk)

**Trivy Scanner:**

```yaml
# Scan Docker image for vulnerabilities
trivy-scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    # Scan filesystem
    - trivy fs --severity HIGH,CRITICAL .
    
    # Scan Docker image
    - trivy image --severity HIGH,CRITICAL myapp:latest
    
    # Generate report
    - trivy image --format json --output trivy-report.json myapp:latest
  artifacts:
    reports:
      container_scanning: trivy-report.json
  allow_failure: false  # Fail build if vulnerabilities found
```

**Snyk Scanner:**

```yaml
# Snyk security scan
snyk-scan:
  stage: security
  image: snyk/snyk:docker
  script:
    # Test dependencies
    - snyk test --severity-threshold=high
    
    # Test Docker image
    - snyk container test myapp:latest --severity-threshold=high
    
    # Monitor in Snyk dashboard
    - snyk monitor
  only:
    - main
```

**GitHub Actions - Trivy:**

```yaml
- name: Run Trivy scanner
  uses: aquasecurity/trivy-action@master
  with:
    image-ref: 'myapp:latest'
    format: 'sarif'
    output: 'trivy-results.sarif'
    severity: 'CRITICAL,HIGH'

- name: Upload to GitHub Security
  uses: github/codeql-action/upload-sarif@v2
  with:
    sarif_file: 'trivy-results.sarif'
```

---

### Dependency Vulnerability Scanning

**OWASP Dependency-Check:**

```yaml
# Maven plugin
<plugin>
  <groupId>org.owasp</groupId>
  <artifactId>dependency-check-maven</artifactId>
  <version>8.4.0</version>
  <configuration>
    <failBuildOnCVSS>7</failBuildOnCVSS>  <!-- Fail on high severity -->
  </configuration>
</plugin>
```

```yaml
# Pipeline
dependency-check:
  stage: security
  script:
    - mvn dependency-check:check
  artifacts:
    reports:
      dependency_scanning: target/dependency-check-report.json
```

**Dependabot (GitHub):**

```yaml
# .github/dependabot.yml
version: 2
updates:
  - package-ecosystem: "maven"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    reviewers:
      - "security-team"
    labels:
      - "dependencies"
      - "security"
```

**Renovate Bot:**

```json
// renovate.json
{
  "extends": ["config:base"],
  "packageRules": [
    {
      "matchUpdateTypes": ["major"],
      "dependencyDashboardApproval": true
    },
    {
      "matchPackagePatterns": ["^org.springframework"],
      "groupName": "Spring Framework"
    }
  ],
  "vulnerabilityAlerts": {
    "enabled": true
  }
}
```

---

### SAST/DAST Tools

**SAST (Static Application Security Testing):**

```yaml
# SonarQube SAST
sonarqube-sast:
  stage: security
  script:
    - mvn sonar:sonar \
        -Dsonar.projectKey=myproject \
        -Dsonar.host.url=$SONAR_HOST \
        -Dsonar.login=$SONAR_TOKEN \
        -Dsonar.qualitygate.wait=true
  only:
    - main
```

**Semgrep:**

```yaml
# .gitlab-ci.yml
semgrep-sast:
  stage: security
  image: returntocorp/semgrep:latest
  script:
    - semgrep --config=auto --json --output=semgrep-report.json .
  artifacts:
    reports:
      sast: semgrep-report.json
```

**DAST (Dynamic Application Security Testing):**

```yaml
# OWASP ZAP
zap-dast:
  stage: security
  image: owasp/zap2docker-stable
  script:
    # Start application
    - docker-compose up -d
    - sleep 30
    
    # Run ZAP scan
    - zap-baseline.py -t https://staging.example.com \
        -r zap-report.html
  artifacts:
    paths:
      - zap-report.html
  only:
    - main
```

---

## 4. Deployment Strategies

### Blue-Green Deployment

**Implementation:**

```yaml
# GitLab CI - Blue-Green
deploy-green:
  stage: deploy
  script:
    # Deploy new version (green)
    - kubectl apply -f deployment-green.yaml
    
    # Wait for green to be ready
    - kubectl wait --for=condition=available \
        deployment/app-green --timeout=5m
    
    # Run smoke tests
    - ./smoke-tests.sh https://green.staging.example.com
    
    # Switch traffic (update service selector)
    - kubectl patch service app \
        -p '{"spec":{"selector":{"version":"green"}}}'
    
    # Keep blue for rollback
    - echo "Blue deployment kept for rollback"
  environment:
    name: production
  when: manual
```

**Kubernetes Manifests:**

```yaml
# deployment-blue.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: blue
  template:
    metadata:
      labels:
        app: myapp
        version: blue
    spec:
      containers:
      - name: app
        image: myapp:v1.0.0

---
# deployment-green.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      version: green
  template:
    metadata:
      labels:
        app: myapp
        version: green
    spec:
      containers:
      - name: app
        image: myapp:v2.0.0

---
# service.yaml (initially points to blue)
apiVersion: v1
kind: Service
metadata:
  name: app
spec:
  selector:
    app: myapp
    version: blue  # Switch to green after validation
  ports:
  - port: 80
    targetPort: 8080
```

---

### Canary Deployment

**Gradual rollout to subset of users.**

```yaml
# GitLab CI - Canary
deploy-canary:
  stage: deploy
  script:
    # Deploy canary (10% traffic)
    - kubectl apply -f deployment-canary.yaml
    - kubectl scale deployment app-canary --replicas=1
    - kubectl scale deployment app-stable --replicas=9
    
    # Monitor metrics for 10 minutes
    - ./monitor-canary.sh --duration=10m
    
    # If successful, increase canary
    - kubectl scale deployment app-canary --replicas=5
    - kubectl scale deployment app-stable --replicas=5
    
    # Monitor again
    - ./monitor-canary.sh --duration=10m
    
    # Full rollout
    - kubectl scale deployment app-canary --replicas=10
    - kubectl scale deployment app-stable --replicas=0
  environment:
    name: production
```

**With Istio (Traffic Splitting):**

```yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: app
spec:
  hosts:
  - app.example.com
  http:
  - match:
    - headers:
        user-type:
          exact: beta
    route:
    - destination:
        host: app
        subset: canary
      weight: 100
  - route:
    - destination:
        host: app
        subset: stable
      weight: 90
    - destination:
        host: app
        subset: canary
      weight: 10  # 10% canary traffic
```

---

### Rolling Updates

**Default Kubernetes strategy:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
spec:
  replicas: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2        # Max 2 extra pods
      maxUnavailable: 1  # Max 1 pod down
  template:
    spec:
      containers:
      - name: app
        image: myapp:v2.0.0
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 10

# Rolling update process:
# 1. Create 2 new pods (maxSurge)
# 2. Wait for ready
# 3. Terminate 1 old pod
# 4. Repeat until complete
```

---

### Feature Flags

**Decouple deployment from release.**

```java
// LaunchDarkly example
import com.launchdarkly.sdk.server.*;

LDClient ldClient = new LDClient("sdk-key");

// Check feature flag
boolean newFeature = ldClient.boolVariation(
    "new-checkout-flow",
    user,
    false  // Default value
);

if (newFeature) {
    // New checkout flow
    return newCheckoutService.process(order);
} else {
    // Old checkout flow
    return oldCheckoutService.process(order);
}
```

**Configuration:**

```yaml
# Deploy with feature flag OFF
deploy:
  script:
    - kubectl set image deployment/app app=myapp:v2.0.0
    - echo "New code deployed, feature flag OFF"
  environment:
    name: production

# Enable feature flag gradually
enable-feature:
  stage: release
  script:
    # Enable for 10% of users
    - curl -X PATCH https://app.launchdarkly.com/api/v2/flags/myproject/new-feature \
        -H "Authorization: $LD_API_KEY" \
        -d '{"variations": [{"weight": 90000}, {"weight": 10000}]}'
  when: manual
```

---

### Rollback Procedures

**Automated Rollback:**

```yaml
# GitLab CI - Auto Rollback
deploy:
  stage: deploy
  script:
    # Deploy new version
    - kubectl set image deployment/app app=myapp:$CI_COMMIT_SHA
    
    # Wait and check health
    - kubectl rollout status deployment/app --timeout=5m
    
    # Run smoke tests
    - ./smoke-tests.sh
  after_script:
    # Rollback on failure
    - |
      if [ $CI_JOB_STATUS == 'failed' ]; then
        echo "Deployment failed, rolling back..."
        kubectl rollout undo deployment/app
        kubectl rollout status deployment/app
      fi
```

**Manual Rollback:**

```bash
# Kubernetes rollback
kubectl rollout history deployment/app
kubectl rollout undo deployment/app  # To previous
kubectl rollout undo deployment/app --to-revision=3  # To specific

# Helm rollback
helm list
helm rollback myapp  # To previous
helm rollback myapp 2  # To specific revision

# ArgoCD rollback
argocd app history myapp
argocd app rollback myapp 5  # To specific revision
```

**Rollback Decision Tree:**

```
Deploy New Version
    ↓
Health Check (30s)
    ↓ FAIL → Automatic Rollback
    ↓ PASS
Smoke Tests (2min)
    ↓ FAIL → Automatic Rollback
    ↓ PASS
Monitor Metrics (10min)
    - Error rate < 1%
    - Latency < 500ms
    - CPU < 80%
    ↓ FAIL → Manual Rollback Decision
    ↓ PASS
Deployment Complete
    - Keep monitoring
    - Alert on degradation
```

---

## Summary

### Pipeline Design

- ✅ **Fail Fast**: Run quick checks first (lint, format, quick tests)
- ✅ **Parallel Execution**: Run independent tasks concurrently
- ✅ **Caching**: Cache dependencies (Maven, Gradle, Docker layers)
- ✅ **Artifacts**: Version, store, and manage build outputs

### Testing Strategy

- ✅ **Test Pyramid**: 70% unit, 20% integration, 10% E2E
- ✅ **Unit Tests**: Fast (<0.1s), isolated, always run
- ✅ **Integration Tests**: Moderate speed, critical paths
- ✅ **E2E Tests**: Slow (10-60s), smoke tests only
- ✅ **Coverage**: 80% unit, 60% integration, 75% overall

### Security in CI/CD

- ✅ **Secrets**: Never commit, use Vault/Secrets Manager
- ✅ **Image Scanning**: Trivy, Snyk for vulnerabilities
- ✅ **Dependency Scanning**: OWASP, Dependabot, Renovate
- ✅ **SAST/DAST**: SonarQube, Semgrep, ZAP

### Deployment Strategies

- ✅ **Blue-Green**: Instant switchover, easy rollback
- ✅ **Canary**: Gradual rollout (10% → 50% → 100%)
- ✅ **Rolling**: Sequential replacement, zero downtime
- ✅ **Feature Flags**: Decouple deployment from release
- ✅ **Rollback**: Automated on health check failure

### Key Metrics

- Build time: < 10 minutes
- Test coverage: > 80%
- Deployment frequency: Multiple times daily
- Lead time: < 1 hour
- Change failure rate: < 15%
- MTTR (Mean Time To Recovery): < 1 hour
