
## Overview

Docker networking and volumes are fundamental concepts for:

- **Networking**: How containers communicate with each other and the outside world
- **Volumes**: How data persists beyond container lifecycle

---

## 1. Network Types

Docker provides several network drivers, each suited for different use cases.

### Bridge (Default)

The **bridge** network is the default network driver. Containers on the same bridge network can communicate with each other.

#### How Bridge Networks Work

```
┌─────────────────────────────────────────────────┐
│                  Host Machine                    │
│                                                  │
│  ┌────────────────────────────────────────────┐ │
│  │         docker0 bridge (172.17.0.1)        │ │
│  └────────────────────────────────────────────┘ │
│         │                    │                   │
│    ┌────┴────┐         ┌────┴────┐             │
│    │Container│         │Container│             │
│    │   A     │         │   B     │             │
│    │172.17.0.2│       │172.17.0.3│             │
│    └─────────┘         └─────────┘             │
│                                                  │
└─────────────────────────────────────────────────┘
```

#### Default Bridge Network

```bash
# Run container on default bridge
docker run -d --name web1 nginx

# Inspect default bridge
docker network inspect bridge

# Containers on default bridge:
# - Get automatic IP addresses
# - Can communicate via IP
# - CANNOT communicate via container name
# - Exposed ports accessible from host
```

**Limitations of default bridge:**

- No automatic DNS resolution (must use IP addresses)
- All containers share same network
- Not recommended for production

#### Custom Bridge Network (RECOMMENDED)

```bash
# Create custom bridge network
docker network create myapp-network

# Run containers on custom network
docker run -d --name web --network myapp-network nginx
docker run -d --name api --network myapp-network myapi

# Now containers can communicate by name!
# From api container: curl http://web:80
```

**Benefits of custom bridge:**

- ✅ Automatic DNS resolution (use container names)
- ✅ Better isolation (separate network per app)
- ✅ On-the-fly container connection/disconnection
- ✅ Configurable (custom subnet, gateway, etc.)

#### Custom Bridge Network - Complete Example

```bash
# Create network with custom settings
docker network create \
  --driver bridge \
  --subnet 192.168.100.0/24 \
  --gateway 192.168.100.1 \
  --opt "com.docker.network.bridge.name=my-bridge" \
  myapp-network

# Run containers
docker run -d \
  --name postgres \
  --network myapp-network \
  -e POSTGRES_PASSWORD=secret \
  postgres:15

docker run -d \
  --name app \
  --network myapp-network \
  -e DATABASE_URL=postgresql://postgres:5432/mydb \
  myapp:latest

# App can reach database at: postgres:5432
# Automatic DNS resolution!

# View network details
docker network inspect myapp-network

# Add existing container to network
docker network connect myapp-network redis

# Remove container from network
docker network disconnect myapp-network redis

# Clean up
docker network rm myapp-network
```

#### Bridge Network with Docker Compose

```yaml
version: '3.8'

services:
  web:
    image: nginx
    networks:
      - frontend
      - backend

  api:
    build: .
    networks:
      - backend

  database:
    image: postgres:15
    networks:
      - backend
    # Database is NOT accessible from frontend network

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
```

---

### Host Network

The **host** network removes network isolation between container and host. Container shares host's network stack.

#### How Host Network Works

```
┌─────────────────────────────────────────────────┐
│                  Host Machine                    │
│              IP: 192.168.1.100                   │
│                                                  │
│  ┌────────────────────────────────────────────┐ │
│  │    Container (no network isolation)        │ │
│  │    Uses host's network stack directly     │ │
│  │    Container port 80 = Host port 80       │ │
│  └────────────────────────────────────────────┘ │
│                                                  │
└─────────────────────────────────────────────────┘
```

#### Using Host Network

```bash
# Run container with host network
docker run -d --network host nginx

# Container binds directly to host's port 80
# No port mapping needed!
# Access at: http://localhost:80

# Check listening ports on host
netstat -tulpn | grep 80
# Shows nginx listening on host port 80
```

**Characteristics:**

- Container uses host's IP address
- No port mapping needed (`-p` flag ignored)
- Best performance (no NAT overhead)
- Potential port conflicts
- Less isolation

**Use Cases:**

```bash
# 1. Performance-critical applications
docker run --network host redis:7

# 2. Applications needing many ports
docker run --network host kafka:latest

# 3. Network debugging tools
docker run --network host nicolaka/netshoot

# 4. Development/testing (quick setup)
docker run --network host myapp:dev
```

**⚠️ Limitations:**

- Only works on Linux (not macOS/Windows Docker Desktop)
- Port conflicts if multiple containers use same port
- Less secure (no network isolation)
- Cannot use with Docker Swarm

#### Host Network Example - Monitoring

```bash
# Prometheus on host network (needs many ports)
docker run -d \
  --network host \
  --name prometheus \
  -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \
  prom/prometheus

# Access at: http://localhost:9090
# No port mapping needed
```

---

### Overlay Network (Swarm)

The **overlay** network connects containers across multiple Docker hosts. Used in Docker Swarm or Kubernetes.

#### How Overlay Network Works

```
┌─────────────────┐         ┌─────────────────┐
│   Host 1        │         │   Host 2        │
│  (10.0.0.10)    │         │  (10.0.0.20)    │
│                 │         │                 │
│  ┌───────────┐  │         │  ┌───────────┐  │
│  │Container A│  │         │  │Container B│  │
│  │10.0.1.2   │  │         │  │10.0.1.3   │  │
│  └───────────┘  │         │  └───────────┘  │
│        │        │         │        │        │
│  ┌─────▼──────────────────────────▼─────┐  │
│  │     Overlay Network (10.0.1.0/24)    │  │
│  │    VXLAN tunnel between hosts        │  │
│  └──────────────────────────────────────┘  │
└─────────────────┘         └─────────────────┘
```

Container A and B can communicate as if on same network, even though they're on different hosts!

#### Creating Overlay Network

```bash
# Initialize Swarm (required for overlay)
docker swarm init

# Create overlay network
docker network create \
  --driver overlay \
  --attachable \
  myoverlay

# Deploy service using overlay
docker service create \
  --name web \
  --network myoverlay \
  --replicas 3 \
  nginx

# Service containers can communicate across hosts
```

#### Overlay Network with Docker Compose (Swarm mode)

```yaml
version: '3.8'

services:
  web:
    image: nginx
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
    networks:
      - webnet

  api:
    image: myapi
    deploy:
      replicas: 5
    networks:
      - webnet

networks:
  webnet:
    driver: overlay
    attachable: true
```

**Deploy to Swarm:**

```bash
docker stack deploy -c docker-compose.yml myapp
```

#### Overlay Network Features

**Encryption:**

```bash
# Create encrypted overlay network
docker network create \
  --driver overlay \
  --opt encrypted \
  secure-network
```

**Custom Subnet:**

```bash
docker network create \
  --driver overlay \
  --subnet 10.20.0.0/16 \
  --gateway 10.20.0.1 \
  myoverlay
```

**Multi-host Communication Example:**

```bash
# Host 1
docker swarm init --advertise-addr 192.168.1.10

# Host 2 (join swarm)
docker swarm join --token <token> 192.168.1.10:2377

# Create overlay network (on manager)
docker network create --driver overlay --attachable mynet

# Run containers on different hosts
docker run -d --name web1 --network mynet nginx  # Host 1
docker run -d --name web2 --network mynet nginx  # Host 2

# They can communicate!
docker exec web1 ping web2  # Works!
```

---

### Custom Networks

Custom networks provide isolation and control over container networking.

#### Network Drivers Comparison

|Driver|Use Case|Multi-Host|DNS|Isolation|
|---|---|---|---|---|
|**bridge**|Single host, most common|No|Yes (custom)|Good|
|**host**|Performance-critical|No|No|None|
|**overlay**|Multi-host (Swarm)|Yes|Yes|Good|
|**macvlan**|Legacy apps needing own IP|No|No|Physical|
|**none**|Complete isolation|No|No|Complete|

#### Macvlan Network

Gives container its own MAC address, appearing as physical device on network.

```bash
# Create macvlan network
docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  macvlan-net

# Run container with own IP on physical network
docker run -d \
  --network macvlan-net \
  --ip 192.168.1.100 \
  nginx

# Container accessible at 192.168.1.100 on physical network
```

**Use cases:**

- Legacy applications expecting direct network access
- Network monitoring tools
- Applications needing to appear as physical devices

#### None Network

Complete network isolation. No network interface except loopback.

```bash
# Run container with no network
docker run -d --network none alpine sleep 3600

# Container has only loopback interface
docker exec <container> ip addr
# Output: lo (127.0.0.1) only
```

**Use cases:**

- Maximum isolation
- Batch processing jobs
- Security-sensitive workloads

#### Network Management Commands

```bash
# List networks
docker network ls

# Inspect network
docker network inspect mynetwork

# Create network
docker network create mynetwork

# Remove network
docker network rm mynetwork

# Remove unused networks
docker network prune

# Connect container to network
docker network connect mynetwork mycontainer

# Disconnect container from network
docker network disconnect mynetwork mycontainer

# View container's networks
docker inspect mycontainer | grep -A 20 Networks
```

---

## 2. Container Communication

### DNS Resolution

Docker provides automatic DNS resolution for containers on custom networks.

#### How Docker DNS Works

```
┌──────────────────────────────────────────────┐
│           Docker Internal DNS                 │
│         (Embedded DNS Server)                 │
│                                               │
│  Container Name → IP Address Mapping         │
│  ─────────────────────────────────────       │
│  web     → 172.18.0.2                        │
│  api     → 172.18.0.3                        │
│  db      → 172.18.0.4                        │
│  redis   → 172.18.0.5                        │
└──────────────────────────────────────────────┘
```

#### DNS on Default Bridge (❌ Not Available)

```bash
# Default bridge network
docker run -d --name web1 nginx
docker run -d --name web2 nginx

# Cannot communicate by name!
docker exec web1 ping web2
# Result: ping: web2: Name or service not known
```

#### DNS on Custom Bridge (✅ Works!)

```bash
# Custom network
docker network create mynet

docker run -d --name web1 --network mynet nginx
docker run -d --name web2 --network mynet nginx

# Can communicate by name!
docker exec web1 ping web2
# Result: PING web2 (172.18.0.3): 56 data bytes
```

#### Service Discovery Example

```yaml
version: '3.8'

services:
  frontend:
    image: react-app
    environment:
      - API_URL=http://backend:8080
    # Uses DNS to resolve 'backend'

  backend:
    image: spring-boot
    environment:
      - DATABASE_HOST=database
      - REDIS_HOST=redis
    # Uses DNS to resolve 'database' and 'redis'

  database:
    image: postgres:15

  redis:
    image: redis:7

networks:
  default:
    driver: bridge
```

**How it works:**

1. Frontend makes request to `http://backend:8080`
2. Docker's embedded DNS resolves `backend` to container IP
3. Request routed to backend container
4. Backend connects to `database:5432`
5. DNS resolves `database` to database container IP

#### Network Aliases

Give containers multiple DNS names:

```bash
# Create network
docker network create mynet

# Run with aliases
docker run -d \
  --name db \
  --network mynet \
  --network-alias database \
  --network-alias postgres \
  postgres:15

# All these work:
docker exec app ping db
docker exec app ping database
docker exec app ping postgres
```

**Docker Compose aliases:**

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    networks:
      backend:
        aliases:
          - postgres
          - db
          - primary-db

  app:
    build: .
    environment:
      # Any of these work:
      - DB_HOST=database
      - DB_HOST=postgres
      - DB_HOST=primary-db
    networks:
      - backend

networks:
  backend:
```

#### Round-Robin DNS (Load Balancing)

```bash
# Run multiple containers with same alias
docker network create mynet

docker run -d --network mynet --network-alias web nginx
docker run -d --network mynet --network-alias web nginx
docker run -d --network mynet --network-alias web nginx

# DNS returns all IPs in round-robin fashion
docker run --rm --network mynet alpine nslookup web

# Requests to 'web' distributed across all three containers
```

#### DNS Configuration

```bash
# Custom DNS servers
docker run -d \
  --dns 8.8.8.8 \
  --dns 8.8.4.4 \
  nginx

# DNS search domains
docker run -d \
  --dns-search example.com \
  --dns-search company.local \
  nginx

# Custom hosts entries
docker run -d \
  --add-host api.local:192.168.1.100 \
  --add-host db.local:192.168.1.101 \
  nginx
```

**Docker Compose DNS:**

```yaml
version: '3.8'

services:
  app:
    image: myapp
    dns:
      - 8.8.8.8
      - 8.8.4.4
    dns_search:
      - example.com
    extra_hosts:
      - "api.local:192.168.1.100"
      - "db.local:192.168.1.101"
```

---

### Port Mapping

Port mapping exposes container ports to the host and external networks.

#### How Port Mapping Works

```
External Request → Host Port → Container Port
     (Internet)     (8080)        (80)

┌──────────────────────────────────────────┐
│             Host (192.168.1.10)          │
│                                          │
│  Port 8080 ──────────┐                  │
│                      │                  │
│  ┌───────────────────▼────────────────┐ │
│  │     Container                      │ │
│  │     Port 80 (nginx)                │ │
│  └────────────────────────────────────┘ │
│                                          │
└──────────────────────────────────────────┘

Access: http://192.168.1.10:8080 → Container port 80
```

#### Basic Port Mapping

```bash
# Map single port
docker run -d -p 8080:80 nginx
# Host port 8080 → Container port 80
# Access: http://localhost:8080

# Map multiple ports
docker run -d \
  -p 8080:80 \
  -p 8443:443 \
  nginx

# Random host port
docker run -d -p 80 nginx
# Docker assigns random available port
# Find with: docker port <container>

# Bind to specific interface
docker run -d -p 127.0.0.1:8080:80 nginx
# Only accessible from localhost

# Bind to all interfaces (default)
docker run -d -p 0.0.0.0:8080:80 nginx
# Accessible from all network interfaces
```

#### Port Mapping Formats

```bash
# Format: [host-ip:]host-port:container-port[/protocol]

# Basic
-p 8080:80

# With IP
-p 127.0.0.1:8080:80

# With protocol
-p 8080:80/tcp
-p 8080:80/udp

# Range of ports
-p 8000-8010:8000-8010

# Multiple ports
-p 80:80 -p 443:443 -p 3000:3000
```

#### Port Mapping with Docker Compose

```yaml
version: '3.8'

services:
  web:
    image: nginx
    ports:
      # Short syntax
      - "8080:80"
      - "8443:443"
      
      # Long syntax
      - target: 80
        published: 8080
        protocol: tcp
        mode: host
      
      # Bind to specific IP
      - "127.0.0.1:8081:80"
      
      # Random host port
      - "80"

  api:
    build: .
    ports:
      - "8000-8010:8000-8010"  # Range
```

#### Expose vs Ports

```dockerfile
# Dockerfile
EXPOSE 80
# Documents that container listens on port 80
# Does NOT publish port automatically!

# docker-compose.yml
services:
  web:
    build: .
    expose:
      - "80"
    # Makes port accessible to linked services
    # NOT accessible from host

  app:
    build: .
    ports:
      - "8080:80"
    # Accessible from host and other services
```

#### View Port Mappings

```bash
# List port mappings
docker port <container>

# Output:
# 80/tcp -> 0.0.0.0:8080
# 443/tcp -> 0.0.0.0:8443

# Inspect container
docker inspect <container> | grep -A 10 "Ports"

# List all container ports
docker ps --format "table {{.Names}}\t{{.Ports}}"
```

#### Common Port Mapping Patterns

**Spring Boot Application:**

```bash
docker run -d \
  -p 8080:8080 \      # Application
  -p 5005:5005 \      # Debug port
  myapp:latest
```

**Full Stack Application:**

```yaml
version: '3.8'

services:
  frontend:
    image: react-app
    ports:
      - "3000:3000"   # React dev server

  backend:
    image: spring-boot
    ports:
      - "8080:8080"   # REST API

  database:
    image: postgres
    ports:
      - "5432:5432"   # PostgreSQL

  redis:
    image: redis
    ports:
      - "6379:6379"   # Redis

  adminer:
    image: adminer
    ports:
      - "8081:8080"   # Database admin UI
```

---

### Network Isolation

Isolate containers for security and organization.

#### Network Segmentation

```yaml
version: '3.8'

services:
  # Public-facing services
  nginx:
    image: nginx
    networks:
      - public
      - frontend
    ports:
      - "80:80"
      - "443:443"

  # Frontend services
  web:
    image: react-app
    networks:
      - frontend
    # No direct external access

  # Backend services
  api:
    image: spring-boot
    networks:
      - frontend
      - backend
    # Accessible from frontend, connects to backend

  # Database (most isolated)
  database:
    image: postgres
    networks:
      - backend
    # Only accessible from backend network

networks:
  public:
    driver: bridge
  frontend:
    driver: bridge
    internal: true  # No external access
  backend:
    driver: bridge
    internal: true  # No external access
```

**Network layout:**

```
Internet
   ↓
[nginx] ← Public network
   ↓
[web] ← Frontend network (internal)
   ↓
[api] ← Connects both frontend & backend
   ↓
[database] ← Backend network (internal)
```

#### Internal Networks

```bash
# Create internal network (no external access)
docker network create \
  --internal \
  --driver bridge \
  isolated-net

# Containers on this network:
# - Can communicate with each other
# - Cannot reach internet
# - Cannot be reached from internet

docker run -d \
  --name secure-db \
  --network isolated-net \
  postgres:15
```

#### Firewall Rules with iptables

Docker automatically creates iptables rules. You can add custom rules:

```bash
# Block traffic between containers
iptables -I DOCKER-USER -s 172.18.0.2 -d 172.18.0.3 -j DROP

# Allow only specific ports
iptables -I DOCKER-USER -p tcp --dport 5432 -j ACCEPT
iptables -I DOCKER-USER -p tcp --dport 6379 -j ACCEPT
iptables -I DOCKER-USER -j DROP
```

#### Security Groups Pattern

```yaml
version: '3.8'

services:
  # DMZ - Public access
  loadbalancer:
    image: nginx
    networks:
      - dmz
      - app-tier
    ports:
      - "80:80"

  # App Tier - Private
  webapp:
    image: myapp
    networks:
      - app-tier
      - data-tier

  # Data Tier - Most restricted
  database:
    image: postgres
    networks:
      - data-tier
    # No external access at all

  cache:
    image: redis
    networks:
      - data-tier

networks:
  dmz:
    driver: bridge
  app-tier:
    driver: bridge
    internal: true
  data-tier:
    driver: bridge
    internal: true
```

#### Complete Isolation Example

```yaml
version: '3.8'

services:
  # Public services
  reverse-proxy:
    image: traefik:v2.10
    networks:
      - public
      - backend-net
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

  # Application layer
  app-1:
    image: myapp:latest
    networks:
      - backend-net
      - data-net-1
    deploy:
      replicas: 3

  app-2:
    image: otherapp:latest
    networks:
      - backend-net
      - data-net-2
    deploy:
      replicas: 2

  # Data layer - isolated per application
  db-1:
    image: postgres:15
    networks:
      - data-net-1
    environment:
      POSTGRES_PASSWORD: secret1

  db-2:
    image: postgres:15
    networks:
      - data-net-2
    environment:
      POSTGRES_PASSWORD: secret2

networks:
  public:
    driver: bridge
  backend-net:
    driver: bridge
    internal: true
  data-net-1:
    driver: bridge
    internal: true
  data-net-2:
    driver: bridge
    internal: true

# Result:
# - app-1 can ONLY access db-1
# - app-2 can ONLY access db-2
# - Databases cannot reach each other
# - No database has internet access
```

---

## 3. Volumes

Volumes persist data beyond container lifecycle.

### Named Volumes vs Bind Mounts

#### Named Volumes (RECOMMENDED for Production)

```bash
# Create named volume
docker volume create mydata

# Use named volume
docker run -d \
  -v mydata:/var/lib/postgresql/data \
  postgres:15

# List volumes
docker volume ls

# Inspect volume
docker volume inspect mydata

# Output:
# {
#     "Driver": "local",
#     "Mountpoint": "/var/lib/docker/volumes/mydata/_data",
#     "Name": "mydata"
# }
```

**Characteristics:**

- ✅ Managed by Docker
- ✅ Portable across systems
- ✅ Can use volume drivers (cloud storage, etc.)
- ✅ Easy backup/restore
- ✅ Better performance on Mac/Windows
- ✅ Can be shared between containers
- ❌ Not directly accessible from host

**Docker Compose with named volumes:**

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
```

#### Bind Mounts (RECOMMENDED for Development)

```bash
# Bind mount host directory
docker run -d \
  -v /path/on/host:/path/in/container \
  nginx

# Relative path (current directory)
docker run -d \
  -v $(pwd)/data:/app/data \
  myapp

# Read-only bind mount
docker run -d \
  -v $(pwd)/config:/app/config:ro \
  myapp
```

**Characteristics:**

- ✅ Direct access to host files
- ✅ Perfect for development (live code reload)
- ✅ Easy to edit from host
- ✅ Can mount single files
- ❌ Host path dependent (not portable)
- ❌ Slower on Mac/Windows
- ❌ Security concerns (container can modify host files)

**Docker Compose with bind mounts:**

```yaml
version: '3.8'

services:
  app:
    build: .
    volumes:
      # Bind mount source code (development)
      - ./src:/app/src
      - ./target:/app/target
      
      # Named volume for dependencies
      - maven_cache:/root/.m2

volumes:
  maven_cache:
```

#### Comparison Table

|Feature|Named Volume|Bind Mount|
|---|---|---|
|**Management**|Docker-managed|User-managed|
|**Location**|Docker area|Anywhere on host|
|**Portability**|High|Low|
|**Performance**|Excellent|Good (slower on Mac/Windows)|
|**Use case**|Production data|Development|
|**Backup**|Easy|Manual|
|**Access from host**|Hard|Easy|
|**Syntax**|`myvolume:/path`|`./host/path:/container/path`|

#### tmpfs Mounts (In-Memory)

```bash
# Create tmpfs mount (stored in RAM)
docker run -d \
  --tmpfs /app/cache:rw,size=1g \
  myapp

# Data is:
# - Very fast (RAM speed)
# - Lost when container stops
# - Not shared between containers
```

**Docker Compose tmpfs:**

```yaml
version: '3.8'

services:
  app:
    image: myapp
    tmpfs:
      - /tmp
      - /app/cache
    # Or with options:
    tmpfs:
      - type: tmpfs
        target: /app/cache
        tmpfs:
          size: 1000000000  # 1GB
```

**Use cases:**

- Temporary cache
- Session data
- Scratch space
- Security (sensitive data in RAM only)

---

### Volume Drivers

Volume drivers allow you to store data in different backends.

#### Local Driver (Default)

```bash
# Create volume with local driver
docker volume create --driver local myvolume

# With options
docker volume create \
  --driver local \
  --opt type=none \
  --opt device=/path/on/host \
  --opt o=bind \
  myvolume
```

#### NFS Driver

```bash
# Mount NFS share as volume
docker volume create \
  --driver local \
  --opt type=nfs \
  --opt o=addr=192.168.1.100,rw \
  --opt device=:/path/to/share \
  nfs-volume

# Use volume
docker run -d \
  -v nfs-volume:/data \
  myapp
```

**Docker Compose NFS:**

```yaml
version: '3.8'

services:
  app:
    image: myapp
    volumes:
      - nfs-data:/data

volumes:
  nfs-data:
    driver: local
    driver_opts:
      type: nfs
      o: addr=192.168.1.100,rw,nolock
      device: ":/path/to/share"
```

#### Cloud Storage Drivers

**AWS EFS:**

```bash
# Install EFS driver
docker plugin install --alias efs rexray/efs \
  EFS_REGION=us-east-1

# Create volume
docker volume create \
  --driver efs \
  --opt filesystem-id=fs-12345678 \
  my-efs-volume
```

**Azure File Storage:**

```bash
docker volume create \
  --driver azure_file \
  --opt share=myshare \
  --opt storageaccount=mystorageaccount \
  azure-volume
```

**Google Cloud Filestore:**

```bash
docker volume create \
  --driver gcsfuse \
  --opt bucket=my-bucket \
  gcs-volume
```

#### Third-Party Volume Plugins

**REX-Ray (Multi-cloud):**

```bash
# Install REX-Ray
docker plugin install rexray/s3fs \
  S3FS_REGION=us-east-1 \
  S3FS_OPTIONS="allow_other,iam_role=auto"

# Create S3-backed volume
docker volume create \
  --driver rexray/s3fs \
  --opt bucket=my-bucket \
  s3-volume
```

**Flocker (Portable volumes):**

```bash
# Create Flocker volume
docker volume create \
  --driver flocker \
  --opt size=20GB \
  flocker-volume
```

#### Volume Driver Use Cases

```yaml
version: '3.8'

services:
  # Development - local volume
  dev-db:
    image: postgres:15
    profiles: ["dev"]
    volumes:
      - dev_data:/var/lib/postgresql/data

  # Production - cloud storage
  prod-db:
    image: postgres:15
    profiles: ["prod"]
    volumes:
      - prod_data:/var/lib/postgresql/data

  # Shared storage - NFS
  shared-app:
    image: myapp
    volumes:
      - shared_uploads:/app/uploads

volumes:
  dev_data:
    driver: local

  prod_data:
    driver: rexray/ebs
    driver_opts:
      size: 100
      volumetype: gp3

  shared_uploads:
    driver: local
    driver_opts:
      type: nfs
      o: addr=nfs.example.com,rw
      device: ":/uploads"
```

---

### Data Persistence Strategies

Different strategies for different use cases.

#### Strategy 1: Named Volumes (Most Common)

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: secret
    volumes:
      - postgres_data:/var/lib/postgresql/data
    # Data persists even after 'docker compose down'

volumes:
  postgres_data:
    driver: local
```

**Characteristics:**

- Data survives container recreation
- Data lost only with `docker compose down -v`
- Easy to backup/restore
- Managed by Docker

#### Strategy 2: External Volumes

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
    external: true
    name: production_postgres_data
```

**Create external volume separately:**

```bash
# Create volume manually
docker volume create production_postgres_data

# Start services
docker compose up -d

# Volume not deleted with 'docker compose down'
```

**Benefits:**

- Shared across multiple compose stacks
- Independent lifecycle
- Never accidentally deleted

#### Strategy 3: Bind Mounts (Development)

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    # Data stored in ./data/postgres on host
```

**Benefits:**

- Easy backup (just copy directory)
- Direct access from host
- Version control possible (with caution!)

**Drawbacks:**

- Path-dependent
- Permission issues
- Slower on Mac/Windows

#### Strategy 4: Volume Containers (Legacy)

```bash
# Create data-only container
docker create \
  --name datastore \
  -v /var/lib/postgresql/data \
  postgres:15

# Use volumes from data container
docker run -d \
  --volumes-from datastore \
  --name db1 \
  postgres:15
```

**Not recommended** - use named volumes instead!

#### Strategy 5: Cloud Storage

```yaml
version: '3.8'

services:
  app:
    image: myapp
    volumes:
      - s3_uploads:/app/uploads

volumes:
  s3_uploads:
    driver: rexray/s3fs
    driver_opts:
      bucket: my-production-uploads
      region: us-east-1
```

**Benefits:**

- Scalable storage
- Built-in redundancy
- No server storage limits
- Cross-region availability

#### Strategy 6: Database-Specific Persistence

**PostgreSQL with WAL archiving:**

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: secret
      # Enable WAL archiving
      POSTGRES_INITDB_ARGS: "-c wal_level=replica"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - postgres_wal:/var/lib/postgresql/wal
      - ./backups:/backups
    command: >
      postgres
      -c wal_level=replica
      -c archive_mode=on
      -c archive_command='cp %p /backups/%f'

volumes:
  postgres_data:
  postgres_wal:
```

**MySQL with binary logs:**

```yaml
version: '3.8'

services:
  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: secret
    volumes:
      - mysql_data:/var/lib/mysql
      - mysql_logs:/var/log/mysql
    command: >
      --log-bin=/var/log/mysql/mysql-bin
      --server-id=1

volumes:
  mysql_data:
  mysql_logs:
```

#### Strategy 7: Multi-Stage Persistence

```yaml
version: '3.8'

services:
  app:
    image: myapp
    volumes:
      # Hot data - SSD volume
      - hot_data:/app/cache
      
      # Warm data - standard volume
      - warm_data:/app/data
      
      # Cold data - S3 volume
      - cold_data:/app/archive

volumes:
  hot_data:
    driver: local
    driver_opts:
      type: none
      device: /mnt/ssd/hot_data
      o: bind

  warm_data:
    driver: local

  cold_data:
    driver: rexray/s3fs
    driver_opts:
      bucket: archive-bucket
```

---

### Backup and Restore

#### Backup Named Volumes

**Method 1: Using tar**

```bash
# Backup volume
docker run --rm \
  -v myvolume:/data \
  -v $(pwd):/backup \
  alpine \
  tar czf /backup/myvolume-backup.tar.gz -C /data .

# Restore volume
docker run --rm \
  -v myvolume:/data \
  -v $(pwd):/backup \
  alpine \
  sh -c "cd /data && tar xzf /backup/myvolume-backup.tar.gz"
```

**Method 2: Using docker cp**

```bash
# Start temporary container with volume
docker run -d --name temp -v myvolume:/data alpine sleep 3600

# Copy data from volume to host
docker cp temp:/data ./backup

# Copy data from host to volume
docker cp ./backup/. temp:/data

# Cleanup
docker stop temp && docker rm temp
```

**Method 3: Backup script**

```bash
#!/bin/bash
# backup-volume.sh

VOLUME_NAME=$1
BACKUP_DIR=${2:-./backups}
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/${VOLUME_NAME}_${TIMESTAMP}.tar.gz"

mkdir -p "$BACKUP_DIR"

docker run --rm \
  -v "$VOLUME_NAME":/source \
  -v "$BACKUP_DIR":/backup \
  alpine \
  tar czf "/backup/$(basename $BACKUP_FILE)" -C /source .

echo "Backup completed: $BACKUP_FILE"
```

**Usage:**

```bash
chmod +x backup-volume.sh
./backup-volume.sh postgres_data ./backups
```

#### Automated Backups with Cron

```bash
# Add to crontab
crontab -e

# Daily backup at 2 AM
0 2 * * * /path/to/backup-volume.sh postgres_data /backups

# Keep only last 7 days
0 3 * * * find /backups -name "postgres_data_*.tar.gz" -mtime +7 -delete
```

#### Backup with Docker Compose

```yaml
version: '3.8'

services:
  database:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data

  backup:
    image: postgres:15
    depends_on:
      - database
    volumes:
      - postgres_data:/data:ro
      - ./backups:/backups
    command: >
      sh -c "
        while true; do
          sleep 86400;
          pg_dump -h database -U postgres mydb | gzip > /backups/backup_$$(date +%Y%m%d_%H%M%S).sql.gz;
        done
      "
    environment:
      PGPASSWORD: secret

volumes:
  postgres_data:
```

#### Backup to S3

```bash
#!/bin/bash
# backup-to-s3.sh

VOLUME_NAME=$1
S3_BUCKET=$2
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Create backup
docker run --rm \
  -v "$VOLUME_NAME":/data \
  -v $(pwd):/backup \
  alpine \
  tar czf /backup/temp-backup.tar.gz -C /data .

# Upload to S3
aws s3 cp temp-backup.tar.gz \
  s3://$S3_BUCKET/${VOLUME_NAME}_${TIMESTAMP}.tar.gz

# Cleanup
rm temp-backup.tar.gz

echo "Backup uploaded to S3"
```

#### Database-Specific Backups

**PostgreSQL:**

```bash
# Backup
docker exec postgres pg_dump -U postgres mydb > backup.sql

# Compressed backup
docker exec postgres pg_dump -U postgres mydb | gzip > backup.sql.gz

# Restore
cat backup.sql | docker exec -i postgres psql -U postgres mydb

# Or compressed
gunzip < backup.sql.gz | docker exec -i postgres psql -U postgres mydb
```

**MySQL:**

```bash
# Backup
docker exec mysql mysqldump -u root -ppassword mydb > backup.sql

# Restore
cat backup.sql | docker exec -i mysql mysql -u root -ppassword mydb
```

**MongoDB:**

```bash
# Backup
docker exec mongo mongodump --out=/backup

# Copy from container
docker cp mongo:/backup ./mongo-backup

# Restore
docker cp ./mongo-backup mongo:/backup
docker exec mongo mongorestore /backup
```

#### Complete Backup Solution

```yaml
version: '3.8'

services:
  # Application
  app:
    image: myapp
    volumes:
      - app_data:/app/data

  # Database
  database:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: secret
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Backup service
  backup:
    image: alpine
    volumes:
      - app_data:/source/app:ro
      - postgres_data:/source/postgres:ro
      - ./backups:/backups
      - ./scripts:/scripts:ro
    command: >
      sh -c "
        apk add --no-cache aws-cli postgresql-client &&
        /scripts/backup.sh
      "
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      S3_BUCKET: ${S3_BUCKET}

volumes:
  app_data:
  postgres_data:
```

**Backup script** (`scripts/backup.sh`):

```bash
#!/bin/sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Backup volumes
tar czf /backups/app_${TIMESTAMP}.tar.gz -C /source/app .
tar czf /backups/postgres_${TIMESTAMP}.tar.gz -C /source/postgres .

# Upload to S3
aws s3 cp /backups/app_${TIMESTAMP}.tar.gz s3://${S3_BUCKET}/
aws s3 cp /backups/postgres_${TIMESTAMP}.tar.gz s3://${S3_BUCKET}/

# Keep only last 7 days locally
find /backups -name "*.tar.gz" -mtime +7 -delete

echo "Backup completed: ${TIMESTAMP}"
```

#### Restore Strategies

**1. Volume restoration:**

```bash
# Create new volume
docker volume create postgres_data_new

# Restore backup to new volume
docker run --rm \
  -v postgres_data_new:/data \
  -v $(pwd)/backups:/backup \
  alpine \
  tar xzf /backup/postgres_data_20241114.tar.gz -C /data

# Test with temporary container
docker run --rm -v postgres_data_new:/data alpine ls -la /data

# If OK, replace old volume:
docker compose down
docker volume rm postgres_data
docker volume create postgres_data
# Restore to postgres_data
docker compose up -d
```

**2. Point-in-time recovery:**

```bash
# List available backups
ls -lh backups/

# Choose specific backup
BACKUP_FILE="postgres_data_20241114_020000.tar.gz"

# Restore
docker run --rm \
  -v postgres_data:/data \
  -v $(pwd)/backups:/backup \
  alpine \
  sh -c "rm -rf /data/* && tar xzf /backup/$BACKUP_FILE -C /data"
```

**3. Disaster recovery:**

```bash
# Pull backups from S3
aws s3 sync s3://my-backups/postgres ./restore/

# Restore latest
LATEST=$(ls -t restore/postgres_*.tar.gz | head -1)
docker run --rm \
  -v postgres_data:/data \
  -v $(pwd)/restore:/backup \
  alpine \
  tar xzf /backup/$(basename $LATEST) -C /data
```

---

### Network Types Quick Reference

|Type|Use Case|Multi-Host|DNS|Command|
|---|---|---|---|---|
|**bridge** (default)|Single host|No|No|`docker run <image>`|
|**bridge** (custom)|Single host|No|Yes|`docker network create mynet`|
|**host**|Performance|No|No|`docker run --network host`|
|**overlay**|Swarm/Multi-host|Yes|Yes|`docker network create --driver overlay`|
|**macvlan**|Physical network|No|No|`docker network create -d macvlan`|
|**none**|Isolation|No|No|`docker run --network none`|

### Volume Types Quick Reference

|Type|Management|Performance|Use Case|Syntax|
|---|---|---|---|---|
|**Named volume**|Docker|Excellent|Production|`myvolume:/path`|
|**Bind mount**|User|Good|Development|`./host:/path`|
|**tmpfs**|Docker|Fastest|Temporary|`--tmpfs /path`|

### Key Takeaways

**Networking:**

- Use custom bridge networks (not default) for DNS resolution
- Use overlay networks for multi-host setups
- Implement network segmentation for security
- Understand port mapping (host:container)
- Leverage service discovery with DNS

**Volumes:**

- Prefer named volumes for production
- Use bind mounts for development
- Implement regular backups
- Choose appropriate volume drivers
- Understand data persistence strategies

**Best Practices:**

- Always use custom networks (not default bridge)
- Enable health checks for service dependencies
- Implement proper network isolation
- Regular automated backups
- Document your network topology
- Use external volumes for critical data
- Test restore procedures regularly
