
## Overview

A **Docker Registry** is a storage and distribution system for Docker images. It allows you to:

- Store and distribute Docker images
- Control access (public/private)
- Scan for vulnerabilities
- Track image versions
- Enable CI/CD pipelines

**Registry vs Repository:**

- **Registry**: The service that stores images (e.g., Docker Hub, ECR)
- **Repository**: A collection of related images within a registry (e.g., `nginx`, `myapp`)
- **Tag**: A specific version of an image (e.g., `nginx:1.25`, `myapp:v1.0.0`)

**Image naming format:**

```
[registry]/[namespace]/[repository]:[tag]

Examples:
docker.io/library/nginx:latest
gcr.io/my-project/myapp:v1.0.0
registry.company.com/team/service:sha-abc123
```

---

## 1. Docker Hub

Docker Hub is the default public registry maintained by Docker, Inc.

### Public vs Private Repositories

#### Public Repositories

**Characteristics:**

- Free for unlimited public repos
- Anyone can pull images
- Good for open-source projects
- Visible in Docker Hub search

**Creating public repository:**

```bash
# 1. Build image
docker build -t username/myapp:latest .

# 2. Login to Docker Hub
docker login

# 3. Push image
docker push username/myapp:latest

# Now anyone can pull:
docker pull username/myapp:latest
```

**Docker Hub URL structure:**

```
https://hub.docker.com/r/username/myapp

Official images (no username):
https://hub.docker.com/_/nginx
https://hub.docker.com/_/postgres
```

#### Private Repositories

**Characteristics:**

- Requires authentication to pull
- Limited free private repos (1 free for personal accounts)
- Paid plans for more repos
- Access control via teams

**Creating private repository:**

```bash
# Option 1: Via Docker Hub website
# 1. Go to hub.docker.com
# 2. Create new repository
# 3. Select "Private"

# Option 2: Push makes it automatically
docker build -t username/private-app:latest .
docker login
docker push username/private-app:latest
# Set to private in Docker Hub UI

# Pull requires authentication
docker login
docker pull username/private-app:latest
```

#### Access Control

**Manage repository access:**

```
Docker Hub → Repository → Settings → Collaborators
Add users/teams with different permissions:
- Read: Pull only
- Write: Pull and push
- Admin: Full control
```

**Using access tokens (recommended over password):**

```bash
# Generate token:
# Docker Hub → Account Settings → Security → New Access Token

# Login with token
docker login -u username
# Password: <paste token>

# Or use environment variable
echo $DOCKER_TOKEN | docker login -u username --password-stdin
```

#### Docker Hub Plans Comparison

|Plan|Public Repos|Private Repos|Pull Rate|Price|
|---|---|---|---|---|
|**Free**|Unlimited|1|200/6h (auth)|$0|
|**Pro**|Unlimited|Unlimited|5000/day|$5/month|
|**Team**|Unlimited|Unlimited|5000/day|$7/user/month|
|**Business**|Unlimited|Unlimited|Unlimited|Custom|

### Automated Builds

Docker Hub can automatically build images from GitHub/Bitbucket repositories.

#### Setting Up Automated Builds

**1. Link GitHub/Bitbucket account:**

```
Docker Hub → Account Settings → Linked Accounts → Connect GitHub
```

**2. Create automated build:**

```
1. Create Repository → Build Settings
2. Link to GitHub repository
3. Configure build rules
4. Save settings
```

**3. Build configuration:**

```yaml
# Dockerfile location: /Dockerfile
# Build context: /

Build rules:
- Source: main branch → Tag: latest
- Source: /^v\d+\.\d+\.\d+$/ → Tag: {sourceref}
- Source: develop → Tag: dev

Example:
Push to main branch → Triggers build → Tagged as username/myapp:latest
Push tag v1.2.3 → Triggers build → Tagged as username/myapp:1.2.3
```

#### Advanced Build Configuration

**Build environment variables:**

```
Docker Hub → Repository → Builds → Configure Automated Builds → Environment Variables

BUILD_DATE=`date -u +"%Y-%m-%dT%H:%M:%SZ"`
VCS_REF=`git rev-parse --short HEAD`
VERSION=1.0.0
```

**Dockerfile with build args:**

```dockerfile
FROM openjdk:17-jre-slim

ARG BUILD_DATE
ARG VCS_REF
ARG VERSION

LABEL org.opencontainers.image.created=$BUILD_DATE \
      org.opencontainers.image.revision=$VCS_REF \
      org.opencontainers.image.version=$VERSION

COPY target/app.jar .
CMD ["java", "-jar", "app.jar"]
```

**Build hooks:**

```bash
# hooks/build
#!/bin/bash
docker build \
  --build-arg BUILD_DATE=$(date -u +"%Y-%m-%dT%H:%M:%SZ") \
  --build-arg VCS_REF=$(git rev-parse --short HEAD) \
  --build-arg VERSION=${DOCKER_TAG} \
  -t $IMAGE_NAME .
```

#### Webhook Integration

**Trigger builds from external events:**

```bash
# Docker Hub → Repository → Webhooks → Add webhook

Webhook URL: https://hub.docker.com/api/build/v1/source/<uuid>/trigger/

# Trigger build via curl
curl -X POST https://hub.docker.com/api/build/v1/source/<uuid>/trigger/<token>/

# Integrate with CI/CD:
# After tests pass → Trigger Docker Hub build
```

#### Multi-architecture Builds

**Build for multiple platforms:**

```bash
# Enable experimental features
export DOCKER_CLI_EXPERIMENTAL=enabled

# Create builder
docker buildx create --name mybuilder --use

# Build for multiple platforms
docker buildx build \
  --platform linux/amd64,linux/arm64,linux/arm/v7 \
  --push \
  -t username/myapp:latest .

# Pull on any platform
docker pull username/myapp:latest
# Automatically gets correct architecture
```

---

## 2. Private Registries

### Harbor

Harbor is an open-source cloud-native registry with enterprise features.

#### Harbor Features

**Core capabilities:**

- ✅ Role-based access control (RBAC)
- ✅ Vulnerability scanning (Trivy, Clair)
- ✅ Image signing and verification
- ✅ Replication across registries
- ✅ Retention policies
- ✅ Webhook notifications
- ✅ Audit logging
- ✅ Helm chart repository
- ✅ OCI artifact support

#### Installing Harbor

**Prerequisites:**

```bash
# Docker 20.10+
# Docker Compose 2.x
# At least 4GB RAM
# Domain name or IP
```

**Installation steps:**

```bash
# 1. Download Harbor
wget https://github.com/goharbor/harbor/releases/download/v2.10.0/harbor-offline-installer-v2.10.0.tgz

# 2. Extract
tar xzvf harbor-offline-installer-v2.10.0.tgz
cd harbor

# 3. Configure
cp harbor.yml.tmpl harbor.yml
vim harbor.yml

# Edit harbor.yml:
hostname: registry.company.com
http:
  port: 80
https:
  port: 443
  certificate: /path/to/cert.crt
  private_key: /path/to/cert.key

harbor_admin_password: AdminPassword123

database:
  password: DatabasePassword123
  max_idle_conns: 50
  max_open_conns: 1000

data_volume: /data

# 4. Install
sudo ./install.sh --with-trivy

# 5. Access Harbor
https://registry.company.com
Username: admin
Password: AdminPassword123
```

#### Using Harbor

**Login to Harbor:**

```bash
# Login
docker login registry.company.com
Username: admin
Password: AdminPassword123

# Or with token
docker login registry.company.com -u admin -p <token>
```

**Push image to Harbor:**

```bash
# 1. Tag image with registry prefix
docker tag myapp:latest registry.company.com/library/myapp:latest

# 2. Push
docker push registry.company.com/library/myapp:latest

# 3. Pull
docker pull registry.company.com/library/myapp:latest
```

#### Harbor Projects and Access Control

**Create project via UI:**

```
Harbor UI → Projects → New Project
- Project Name: backend-services
- Access Level: Private
- Storage Quota: 10 GB
- Vulnerability Scanning: Scan on push
```

**Role-Based Access Control:**

```
Project Members:
- Project Admin: Full control
- Master: Read/write, can't manage members
- Developer: Read/write to repository
- Guest: Read only
- Limited Guest: Pull only

Add members:
Projects → backend-services → Members → +User
```

**Robot Accounts (for CI/CD):**

```
Harbor UI → Robot Accounts → New Robot Account
- Name: ci-robot
- Expiration: 365 days
- Projects: backend-services
- Permissions: Push, Pull

# Use token in CI/CD:
echo $HARBOR_ROBOT_TOKEN | docker login registry.company.com -u robot$ci-robot --password-stdin
```

#### Harbor Replication

**Setup replication (multi-region):**

```
Harbor UI → Registries → New Endpoint
- Provider: Harbor
- Name: us-east-registry
- Endpoint URL: https://us-east.registry.company.com
- Credentials: admin / password

Replication:
- Source: registry.company.com/library/*
- Destination: us-east-registry/library/*
- Trigger: Event Based (immediate)
- Schedule: Daily at 2:00 AM
```

#### Harbor Retention Policies

**Automatic cleanup:**

```
Projects → backend-services → Policy → Add Rule

Retain images:
- Last n images: 5
- By tag: Retain tags matching: ^v\d+\.\d+\.\d+$
- Exclude tags: latest, dev, staging

Schedule: Weekly on Sunday at 3:00 AM

Dry run: Test before applying
```

#### Harbor with Docker Compose

**docker-compose.yml:**

```yaml
version: '3.8'

services:
  app:
    build: .
    image: registry.company.com/myproject/app:${VERSION}
    environment:
      - SPRING_PROFILES_ACTIVE=prod

# Login before compose up:
# docker login registry.company.com
```

**CI/CD integration:**

```yaml
# .gitlab-ci.yml
stages:
  - build
  - push

build:
  stage: build
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .

push:
  stage: push
  script:
    - echo $HARBOR_PASSWORD | docker login registry.company.com -u robot$ci-robot --password-stdin
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA registry.company.com/myproject/app:$CI_COMMIT_SHA
    - docker push registry.company.com/myproject/app:$CI_COMMIT_SHA
```

---

### AWS ECR (Elastic Container Registry)

AWS's managed Docker registry service.

#### ECR Features

**Benefits:**

- ✅ Fully managed (no maintenance)
- ✅ Integrated with AWS services (ECS, EKS, Lambda)
- ✅ IAM-based access control
- ✅ Automatic encryption
- ✅ Lifecycle policies
- ✅ Image scanning (Clair-based)
- ✅ Cross-region replication
- ✅ High availability

#### Setting Up ECR

**Create repository via AWS CLI:**

```bash
# Install AWS CLI
pip install awscli

# Configure credentials
aws configure
# Enter: Access Key ID, Secret Access Key, Region

# Create repository
aws ecr create-repository \
  --repository-name myapp \
  --image-scanning-configuration scanOnPush=true \
  --encryption-configuration encryptionType=AES256 \
  --region us-east-1

# Output:
{
  "repository": {
    "repositoryUri": "123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp"
  }
}
```

**Create repository via Terraform:**

```hcl
resource "aws_ecr_repository" "myapp" {
  name                 = "myapp"
  image_tag_mutability = "IMMUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }

  encryption_configuration {
    encryption_type = "AES256"
  }

  tags = {
    Environment = "production"
    Team        = "backend"
  }
}

output "repository_url" {
  value = aws_ecr_repository.myapp.repository_url
}
```

#### Using ECR

**Login to ECR:**

```bash
# Get login password
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin \
  123456789012.dkr.ecr.us-east-1.amazonaws.com

# Login valid for 12 hours
```

**Push image to ECR:**

```bash
# 1. Build image
docker build -t myapp:latest .

# 2. Tag with ECR repository URI
docker tag myapp:latest \
  123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp:latest

# 3. Push
docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp:latest

# 4. Pull
docker pull 123456789012.dkr.ecr.us-east-1.amazonaws.com/myapp:latest
```

#### ECR IAM Policies

**Policy for CI/CD (push/pull):**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:PutImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload"
      ],
      "Resource": "arn:aws:ecr:us-east-1:123456789012:repository/myapp"
    }
  ]
}
```

**Policy for production (pull only):**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ecr:GetAuthorizationToken"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage"
      ],
      "Resource": "arn:aws:ecr:us-east-1:123456789012:repository/myapp"
    }
  ]
}
```

#### ECR Lifecycle Policies

**Automatic image cleanup:**

```json
{
  "rules": [
    {
      "rulePriority": 1,
      "description": "Keep last 10 production images",
      "selection": {
        "tagStatus": "tagged",
        "tagPrefixList": ["v"],
        "countType": "imageCountMoreThan",
        "countNumber": 10
      },
      "action": {
        "type": "expire"
      }
    },
    {
      "rulePriority": 2,
      "description": "Remove untagged images after 7 days",
      "selection": {
        "tagStatus": "untagged",
        "countType": "sinceImagePushed",
        "countUnit": "days",
        "countNumber": 7
      },
      "action": {
        "type": "expire"
      }
    },
    {
      "rulePriority": 3,
      "description": "Keep only last 3 dev images",
      "selection": {
        "tagStatus": "tagged",
        "tagPrefixList": ["dev"],
        "countType": "imageCountMoreThan",
        "countNumber": 3
      },
      "action": {
        "type": "expire"
      }
    }
  ]
}
```

**Apply lifecycle policy:**

```bash
aws ecr put-lifecycle-policy \
  --repository-name myapp \
  --lifecycle-policy-text file://lifecycle-policy.json
```

#### ECR Cross-Region Replication

**Setup replication:**

```json
{
  "rules": [
    {
      "destinations": [
        {
          "region": "us-west-2",
          "registryId": "123456789012"
        },
        {
          "region": "eu-west-1",
          "registryId": "123456789012"
        }
      ],
      "repositoryFilters": [
        {
          "filter": "myapp",
          "filterType": "PREFIX_MATCH"
        }
      ]
    }
  ]
}
```

```bash
aws ecr put-replication-configuration \
  --replication-configuration file://replication-config.json
```

#### ECR in CI/CD Pipeline

**GitHub Actions:**

```yaml
name: Build and Push to ECR

on:
  push:
    branches: [main]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: myapp

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
```

---

### Google Container Registry (GCR) & Artifact Registry

Google Cloud's container image storage services.

#### GCR vs Artifact Registry

**GCR (Container Registry):**

- Original service (being superseded)
- Container images only
- Uses Cloud Storage backend
- Hostname: `gcr.io`

**Artifact Registry (RECOMMENDED):**

- Next-generation service
- Supports containers, Maven, npm, Python, etc.
- Better performance and features
- Regional and multi-regional
- Hostname: `<region>-docker.pkg.dev`

#### Setting Up Artifact Registry

**Create repository:**

```bash
# Install gcloud CLI
curl https://sdk.cloud.google.com | bash

# Login
gcloud auth login

# Set project
gcloud config set project my-project-id

# Create repository
gcloud artifacts repositories create myapp-repo \
  --repository-format=docker \
  --location=us-central1 \
  --description="My application images"

# Repository URL:
# us-central1-docker.pkg.dev/my-project-id/myapp-repo
```

#### Using Artifact Registry

**Authenticate Docker:**

```bash
# Configure Docker auth
gcloud auth configure-docker us-central1-docker.pkg.dev

# Or manually:
gcloud auth print-access-token | \
  docker login -u oauth2accesstoken --password-stdin \
  us-central1-docker.pkg.dev
```

**Push image:**

```bash
# Tag image
docker tag myapp:latest \
  us-central1-docker.pkg.dev/my-project-id/myapp-repo/myapp:latest

# Push
docker push us-central1-docker.pkg.dev/my-project-id/myapp-repo/myapp:latest

# Pull
docker pull us-central1-docker.pkg.dev/my-project-id/myapp-repo/myapp:latest
```

#### IAM Permissions

**Artifact Registry roles:**

```bash
# Grant push/pull access
gcloud artifacts repositories add-iam-policy-binding myapp-repo \
  --location=us-central1 \
  --member=serviceAccount:ci-cd@my-project.iam.gserviceaccount.com \
  --role=roles/artifactregistry.writer

# Grant pull-only access
gcloud artifacts repositories add-iam-policy-binding myapp-repo \
  --location=us-central1 \
  --member=serviceAccount:prod@my-project.iam.gserviceaccount.com \
  --role=roles/artifactregistry.reader
```

**Service Account for CI/CD:**

```bash
# Create service account
gcloud iam service-accounts create ci-cd-sa \
  --display-name="CI/CD Service Account"

# Grant permissions
gcloud projects add-iam-policy-binding my-project-id \
  --member=serviceAccount:ci-cd-sa@my-project-id.iam.gserviceaccount.com \
  --role=roles/artifactregistry.writer

# Create and download key
gcloud iam service-accounts keys create key.json \
  --iam-account=ci-cd-sa@my-project-id.iam.gserviceaccount.com

# Use in CI/CD
gcloud auth activate-service-account --key-file=key.json
gcloud auth configure-docker us-central1-docker.pkg.dev
```

#### Artifact Registry in Cloud Build

**cloudbuild.yaml:**

```yaml
steps:
  # Build image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp:$COMMIT_SHA'
      - '-t'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp:latest'
      - '.'

  # Push image
  - name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - '--all-tags'
      - 'us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp'

  # Deploy to GKE
  - name: 'gcr.io/cloud-builders/kubectl'
    args:
      - 'set'
      - 'image'
      - 'deployment/myapp'
      - 'myapp=us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp:$COMMIT_SHA'
    env:
      - 'CLOUDSDK_COMPUTE_ZONE=us-central1-a'
      - 'CLOUDSDK_CONTAINER_CLUSTER=my-cluster'

images:
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp:$COMMIT_SHA'
  - 'us-central1-docker.pkg.dev/$PROJECT_ID/myapp-repo/myapp:latest'
```

---

### Azure Container Registry (ACR)

Microsoft Azure's managed container registry.

#### Setting Up ACR

**Create registry via Azure CLI:**

```bash
# Login to Azure
az login

# Create resource group
az group create --name myResourceGroup --location eastus

# Create registry
az acr create \
  --resource-group myResourceGroup \
  --name myregistry \
  --sku Premium \
  --admin-enabled true

# Registry URL: myregistry.azurecr.io
```

**Pricing tiers:**

- **Basic**: 10 GB storage, 10 webhooks
- **Standard**: 100 GB storage, 100 webhooks
- **Premium**: 500 GB storage, 500 webhooks, geo-replication, content trust

#### Using ACR

**Login to ACR:**

```bash
# Login with Azure AD
az acr login --name myregistry

# Or get admin credentials
az acr credential show --name myregistry

# Login with admin credentials
docker login myregistry.azurecr.io
Username: myregistry
Password: <from above command>
```

**Push image:**

```bash
# Tag image
docker tag myapp:latest myregistry.azurecr.io/myapp:latest

# Push
docker push myregistry.azurecr.io/myapp:latest

# Pull
docker pull myregistry.azurecr.io/myapp:latest
```

#### ACR Tasks (Build in Cloud)

**Build image in ACR:**

```bash
# Quick build
az acr build \
  --registry myregistry \
  --image myapp:{{.Run.ID}} \
  --image myapp:latest \
  --file Dockerfile \
  .

# Multi-step task
az acr run \
  --registry myregistry \
  --file acr-task.yaml \
  .
```

**acr-task.yaml:**

```yaml
version: v1.1.0
steps:
  # Build
  - build: -t {{.Run.Registry}}/myapp:{{.Run.ID}} .
  
  # Test
  - cmd: {{.Run.Registry}}/myapp:{{.Run.ID}}
    env:
      - TEST_MODE=true
  
  # Tag as latest
  - cmd: docker tag {{.Run.Registry}}/myapp:{{.Run.ID}} {{.Run.Registry}}/myapp:latest
  
  # Push
  - push:
    - {{.Run.Registry}}/myapp:{{.Run.ID}}
    - {{.Run.Registry}}/myapp:latest
```

#### ACR Geo-Replication

**Enable geo-replication (Premium only):**

```bash
# Replicate to West Europe
az acr replication create \
  --registry myregistry \
  --location westeurope

# Replicate to East Asia
az acr replication create \
  --registry myregistry \
  --location eastasia

# List replications
az acr replication list --registry myregistry --output table
```

#### ACR with Azure DevOps

**azure-pipelines.yml:**

```yaml
trigger:
  - main

pool:
  vmImage: 'ubuntu-latest'

variables:
  dockerRegistryServiceConnection: 'myregistry-connection'
  imageRepository: 'myapp'
  containerRegistry: 'myregistry.azurecr.io'
  dockerfilePath: '$(Build.SourcesDirectory)/Dockerfile'
  tag: '$(Build.BuildId)'

stages:
  - stage: Build
    displayName: Build and push image
    jobs:
      - job: Build
        displayName: Build
        steps:
          - task: Docker@2
            displayName: Build and push image
            inputs:
              command: buildAndPush
              repository: $(imageRepository)
              dockerfile: $(dockerfilePath)
              containerRegistry: $(dockerRegistryServiceConnection)
              tags: |
                $(tag)
                latest
```

---

### Nexus Repository Manager

Nexus is a universal artifact repository that supports Docker and many other formats.

#### Nexus Docker Repository Types

**Three repository types:**

1. **Hosted**: Store your own images
2. **Proxy**: Cache images from Docker Hub
3. **Group**: Combine multiple repositories

#### Setting Up Nexus

**Docker Compose deployment:**

```yaml
version: '3.8'

services:
  nexus:
    image: sonatype/nexus3:latest
    container_name: nexus
    ports:
      - "8081:8081"    # Web UI
      - "8082:8082"    # Docker hosted
      - "8083:8083"    # Docker proxy
      - "8084:8084"    # Docker group
    volumes:
      - nexus-data:/nexus-data
    environment:
      - INSTALL4J_ADD_VM_PARAMS=-Xms2g -Xmx2g -XX:MaxDirectMemorySize=3g

volumes:
  nexus-data:
```

**Initial setup:**

```bash
# Start Nexus
docker compose up -d

# Get initial admin password
docker exec nexus cat /nexus-data/admin.password

# Access: http://localhost:8081
# Login: admin / <password from above>
# Change password, configure anonymous access
```

#### Configure Docker Repositories in Nexus

**1. Create Docker Hosted Repository:**

```
Settings → Repositories → Create repository → docker (hosted)

Name: docker-hosted
HTTP connector: 8082
Enable Docker V1 API: No
Deployment policy: Allow redeploy
```

**2. Create Docker Proxy Repository:**

```
Settings → Repositories → Create repository → docker (proxy)

Name: docker-proxy
Remote storage: https://registry-1.docker.io
Docker Index: Use Docker Hub
HTTP connector: 8083
```

**3. Create Docker Group Repository:**

```
Settings → Repositories → Create repository → docker (group)

Name: docker-group
HTTP connector: 8084
Member repositories:
  - docker-hosted
  - docker-proxy
```

#### Using Nexus Docker Registry

**Login to Nexus:**

```bash
# Login to hosted repo (push)
docker login localhost:8082
Username: admin
Password: admin123

# Login to group repo (pull)
docker login localhost:8084
```

**Push to Nexus:**

```bash
# Tag with Nexus registry
docker tag myapp:latest localhost:8082/myapp:latest

# Push
docker push localhost:8082/myapp:latest
```

**Pull from Nexus:**

```bash
# Pull from group (tries hosted first, then proxy)
docker pull localhost:8084/myapp:latest

# Pull from Docker Hub via Nexus proxy
docker pull localhost:8084/nginx:latest
# Nexus caches the image for future pulls
```

#### Nexus Cleanup Policies

**Automatic cleanup:**

```
Settings → Repository → Cleanup Policies → Create

Name: docker-cleanup
Format: Docker
Criteria:
  - Last downloaded before: 90 days
  - Component age: 180 days
  - Last blob updated before: 365 days

Apply to repositories: docker-hosted, docker-proxy
Schedule: Daily at 2:00 AM
```

---

### JFrog Artifactory

Artifactory is another universal repository manager with enterprise features.

#### Artifactory Docker Repository

**Repository types:**

- **Local**: Store Docker images
- **Remote**: Proxy to Docker Hub
- **Virtual**: Aggregate multiple repositories

#### Setting Up Artifactory

**Docker Compose:**

```yaml
version: '3.8'

services:
  artifactory:
    image: docker.bintray.io/jfrog/artifactory-oss:latest
    container_name: artifactory
    ports:
      - "8081:8081"    # Web UI
      - "8082:8082"    # Docker registry
    volumes:
      - artifactory-data:/var/opt/jfrog/artifactory
    environment:
      - JF_SHARED_DATABASE_TYPE=postgresql
      - JF_SHARED_DATABASE_URL=jdbc:postgresql://postgres:5432/artifactory

  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: artifactory
      POSTGRES_USER: artifactory
      POSTGRES_PASSWORD: password
    volumes:
      - postgres-data:/var/lib/postgresql/data

volumes:
  artifactory-data:
  postgres-data:
```

#### Configure Docker in Artifactory

**1. Create local repository:**

```
Administration → Repositories → Add Repositories → Local Repository
Package Type: Docker
Repository Key: docker-local
Docker API Version: V2
Registry Port: 8082
```

**2. Create remote repository:**

```
Administration → Repositories → Add Repositories → Remote Repository
Package Type: Docker
Repository Key: docker-remote
URL: https://registry-1.docker.io
```

**3. Create virtual repository:**

```
Administration → Repositories → Add Repositories → Virtual Repository
Package Type: Docker
Repository Key: docker
Repositories:
  - docker-local
  - docker-remote
Default deployment repository: docker-local
```

#### Using Artifactory

**Login:**

```bash
docker login localhost:8082
Username: admin
Password: password
```

**Push/Pull:**

```bash
# Push
docker tag myapp:latest localhost:8082/docker-local/myapp:latest
docker push localhost:8082/docker-local/myapp:latest

# Pull
docker pull localhost:8082/docker/nginx:latest
```

---

## 3. Image Security Scanning

### Why Scan Images?

**Common vulnerabilities:**

- Outdated base images with known CVEs
- Vulnerable dependencies
- Exposed secrets/credentials
- Malware
- Misconfigured containers

**When to scan:**

- ✅ During build (CI/CD)
- ✅ On push to registry
- ✅ Regular scheduled scans
- ✅ Before deployment to production

### Trivy

Trivy is a comprehensive and easy-to-use vulnerability scanner.

#### Installing Trivy

```bash
# macOS
brew install trivy

# Debian/Ubuntu
wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
sudo apt-get update
sudo apt-get install trivy

# Docker
docker run aquasec/trivy:latest
```

#### Using Trivy

**Basic image scan:**

```bash
# Scan local image
trivy image nginx:latest

# Scan remote image
trivy image myregistry.com/myapp:latest

# Scan specific severities
trivy image --severity HIGH,CRITICAL nginx:latest

# Output to JSON
trivy image -f json -o results.json nginx:latest

# Output to table (default)
trivy image nginx:latest
```

**Example output:**

```
nginx:latest (debian 12.2)
==========================
Total: 245 (UNKNOWN: 0, LOW: 142, MEDIUM: 89, HIGH: 13, CRITICAL: 1)

┌───────────────┬────────────────┬──────────┬───────────────────┬───────────────┐
│   Library     │ Vulnerability  │ Severity │ Installed Version │ Fixed Version │
├───────────────┼────────────────┼──────────┼───────────────────┼───────────────┤
│ openssl       │ CVE-2023-1234  │ CRITICAL │ 3.0.9-1           │ 3.0.10-1      │
│ curl          │ CVE-2023-5678  │ HIGH     │ 7.88.1-10         │ 7.88.1-11     │
└───────────────┴────────────────┴──────────┴───────────────────┴───────────────┘
```

**Scan filesystem:**

```bash
# Scan current directory
trivy fs .

# Scan specific path
trivy fs /path/to/project

# Scan Dockerfile
trivy config Dockerfile
```

**Scan Kubernetes manifests:**

```bash
trivy config deployment.yaml
trivy config k8s-manifests/
```

#### Trivy in CI/CD

**GitHub Actions:**

```yaml
name: Security Scan

on:
  push:
    branches: [main]
  pull_request:

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build image
        run: docker build -t myapp:${{ github.sha }} .

      - name: Run Trivy scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'myapp:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: Upload results to GitHub Security
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Fail on critical vulnerabilities
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'myapp:${{ github.sha }}'
          exit-code: '1'
          severity: 'CRITICAL'
```

**GitLab CI:**

```yaml
scan:
  stage: test
  image:
    name: aquasec/trivy:latest
    entrypoint: [""]
  script:
    - trivy image --exit-code 1 --severity CRITICAL,HIGH $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  only:
    - main
    - merge_requests
```

**Jenkins:**

```groovy
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                sh 'docker build -t myapp:${BUILD_NUMBER} .'
            }
        }
        stage('Scan') {
            steps {
                sh '''
                    docker run --rm \
                      -v /var/run/docker.sock:/var/run/docker.sock \
                      aquasec/trivy:latest \
                      image --exit-code 1 --severity CRITICAL,HIGH \
                      myapp:${BUILD_NUMBER}
                '''
            }
        }
    }
}
```

#### Trivy Configuration

**trivy.yaml:**

```yaml
# Scan settings
severity: CRITICAL,HIGH
exit-code: 1
format: json
output: trivy-results.json

# Ignore specific vulnerabilities
ignores:
  - CVE-2023-1234  # False positive
  - CVE-2023-5678  # Will fix in next sprint

# Skip files/directories
skip-files:
  - "**/*.md"
  - "docs/**"

# Custom cache directory
cache-dir: /tmp/trivy-cache

# Database
db-repository: ghcr.io/aquasecurity/trivy-db
```

**Usage with config:**

```bash
trivy image --config trivy.yaml nginx:latest
```

---

### Clair

Clair is an open-source vulnerability scanner from CoreOS/Red Hat.

#### Clair Features

**Characteristics:**

- API-driven architecture
- PostgreSQL database for vulnerabilities
- Supports multiple Linux distributions
- Used by Harbor, Quay, and others
- More complex setup than Trivy

#### Installing Clair

**Docker Compose:**

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: clair
      POSTGRES_USER: clair
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

  clair:
    image: quay.io/coreos/clair:latest
    depends_on:
      - postgres
    ports:
      - "6060:6060"  # API
      - "6061:6061"  # Health
    volumes:
      - ./clair-config.yaml:/config/config.yaml
    command: -config=/config/config.yaml

volumes:
  postgres_data:
```

**clair-config.yaml:**

```yaml
clair:
  database:
    type: pgsql
    options:
      source: "postgresql://clair:password@postgres:5432/clair?sslmode=disable"
  api:
    addr: "0.0.0.0:6060"
    healthaddr: "0.0.0.0:6061"
  updater:
    interval: 2h
```

#### Using Clair

**Scan with clairctl:**

```bash
# Install clairctl
go install github.com/jgsqware/clairctl@latest

# Scan image
clairctl analyze -l nginx:latest

# Report
clairctl report -l nginx:latest
```

#### Clair in Harbor

Clair is integrated into Harbor:

```
Harbor → Projects → myproject → Configuration
Enable: Automatically scan images on push

Harbor → Interrogation Services → Scanners
Default scanner: Trivy or Clair
```

---

### Snyk

Snyk is a commercial security platform with comprehensive scanning.

#### Snyk Features

**Capabilities:**

- ✅ Container vulnerability scanning
- ✅ Open-source dependency scanning
- ✅ License compliance
- ✅ Kubernetes manifest scanning
- ✅ Infrastructure as Code scanning
- ✅ Fix recommendations
- ✅ Continuous monitoring

#### Using Snyk

**Install Snyk CLI:**

```bash
# npm
npm install -g snyk

# Or via binary
curl -sL https://github.com/snyk/cli/releases/latest/download/snyk-linux -o snyk
chmod +x snyk
sudo mv snyk /usr/local/bin/

# Authenticate
snyk auth
```

**Scan container image:**

```bash
# Scan image
snyk container test nginx:latest

# Scan with detailed output
snyk container test nginx:latest --json

# Scan and monitor
snyk container monitor nginx:latest

# Test Dockerfile
snyk container test nginx:latest --file=Dockerfile
```

**Example output:**

```
Testing nginx:latest...

✗ High severity vulnerability found in openssl
  Description: OpenSSL CVE-2023-1234
  Info: https://snyk.io/vuln/SNYK-DEBIAN12-OPENSSL-123456
  Introduced through: openssl@3.0.9-1
  Fixed in: 3.0.10-1

Organization: my-org
Package manager: debian
Project name: docker-image|nginx
Docker image: nginx:latest
Base image: debian:12-slim

Tested 245 dependencies for known vulnerabilities
Found 14 vulnerabilities (1 critical, 3 high, 6 medium, 4 low)
```

#### Snyk in CI/CD

**GitHub Actions:**

```yaml
name: Snyk Container Scan

on: [push]

jobs:
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Build image
        run: docker build -t myapp:${{ github.sha }} .

      - name: Run Snyk
        uses: snyk/actions/docker@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          image: myapp:${{ github.sha }}
          args: --severity-threshold=high

      - name: Upload results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: snyk.sarif
```

**GitLab CI:**

```yaml
snyk_scan:
  stage: test
  image: snyk/snyk:docker
  script:
    - snyk auth $SNYK_TOKEN
    - snyk container test $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA --severity-threshold=high
  only:
    - main
```

---

### Vulnerability Management

#### Comparing Scanning Tools

|Feature|Trivy|Clair|Snyk|
|---|---|---|---|
|**Ease of use**|⭐⭐⭐⭐⭐|⭐⭐⭐|⭐⭐⭐⭐|
|**Speed**|Very fast|Medium|Fast|
|**Coverage**|Excellent|Good|Excellent|
|**Cost**|Free|Free|Free tier + paid|
|**CI/CD integration**|Excellent|Good|Excellent|
|**Fix recommendations**|Yes|Limited|Excellent|
|**License scanning**|Yes|No|Yes|
|**Kubernetes scanning**|Yes|No|Yes|

#### Vulnerability Scanning Workflow

```
┌─────────────────────────────────────────────────┐
│               Development Phase                  │
├─────────────────────────────────────────────────┤
│ 1. Developer commits code                       │
│ 2. CI builds Docker image                       │
│ 3. Trivy scans image                            │
│ 4. CRITICAL/HIGH → Build fails                  │
│ 5. LOW/MEDIUM → Warning, build continues        │
└─────────────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────┐
│               Registry Phase                     │
├─────────────────────────────────────────────────┤
│ 1. Image pushed to Harbor/ECR                   │
│ 2. Automatic scan on push                       │
│ 3. Scan results stored                          │
│ 4. Webhooks notify security team                │
└─────────────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────┐
│              Deployment Phase                    │
├─────────────────────────────────────────────────┤
│ 1. Admission controller checks scan results     │
│ 2. CRITICAL vulnerabilities → Deployment blocked│
│ 3. HIGH vulnerabilities → Requires approval     │
│ 4. Clean images → Auto-deploy                   │
└─────────────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────┐
│              Runtime Phase                       │
├─────────────────────────────────────────────────┤
│ 1. Scheduled daily scans                        │
│ 2. New vulnerabilities discovered               │
│ 3. Alerts sent to security team                 │
│ 4. Remediation planned                          │
└─────────────────────────────────────────────────┘
```

#### Security Policy Example

**policy.yaml:**

```yaml
# Image security policy
version: 1.0

scanning:
  # Scan on every build
  on_build: true
  
  # Scan on push to registry
  on_push: true
  
  # Scheduled scans
  schedule: "0 2 * * *"  # Daily at 2 AM
  
  # Fail build on these severities
  fail_on:
    - CRITICAL
    - HIGH
  
  # Allowed exceptions (with justification)
  exceptions:
    - cve: CVE-2023-1234
      reason: "False positive - not exploitable in our use case"
      expires: "2024-12-31"
      approved_by: "security-team"

deployment:
  # Block deployment of vulnerable images
  block_on:
    - CRITICAL
  
  # Require approval for these severities
  require_approval:
    - HIGH
  
  # Maximum image age
  max_age_days: 90

monitoring:
  # Alert channels
  alerts:
    - type: slack
      webhook: https://hooks.slack.com/services/xxx
    - type: email
      addresses:
        - security@company.com
  
  # Alert on new vulnerabilities
  alert_on_new: true
  
  # Alert threshold
  alert_threshold:
    critical: 1
    high: 5
```

#### Remediation Workflow

**1. Identify vulnerabilities:**

```bash
trivy image myapp:latest --format json --output scan.json
```

**2. Analyze results:**

```bash
# Parse JSON to find fixable issues
jq '.Results[].Vulnerabilities[] | select(.FixedVersion != "")' scan.json
```

**3. Update base image:**

```dockerfile
# Before
FROM ubuntu:20.04

# After (newer version with fixes)
FROM ubuntu:22.04
```

**4. Update dependencies:**

```dockerfile
# Before
RUN apt-get update && apt-get install -y \
    openssl=1.1.1f-1ubuntu2

# After
RUN apt-get update && apt-get install -y \
    openssl=3.0.2-0ubuntu1
```

**5. Rebuild and rescan:**

```bash
docker build -t myapp:latest .
trivy image myapp:latest
```

**6. Verify fixes:**

```bash
# Should show reduced vulnerabilities
trivy image myapp:latest --severity CRITICAL,HIGH
```

---

## Summary

### Registry Comparison

|Feature|Docker Hub|Harbor|AWS ECR|GCP AR|Azure ACR|Nexus|Artifactory|
|---|---|---|---|---|---|---|---|
|**Cost**|Free tier|Open source|Pay per GB|Pay per GB|Pay per GB|Free (OSS)|Free (OSS)|
|**Management**|Fully managed|Self-hosted|Fully managed|Fully managed|Fully managed|Self-hosted|Self-hosted|
|**Scanning**|Limited|Trivy/Clair|Basic|Yes|Yes|No|Yes|
|**Replication**|No|Yes|Yes|Yes|Yes|Yes|Yes|
|**RBAC**|Basic|Advanced|IAM|IAM|RBAC|Yes|Yes|
|**Best for**|Public images|Enterprise|AWS workloads|GCP workloads|Azure workloads|Multi-format|Enterprise|

### Security Scanning Best Practices

**Development:**

- ✅ Scan on every build
- ✅ Use minimal base images (Alpine, Distroless)
- ✅ Keep base images updated
- ✅ Fix CRITICAL/HIGH before deployment
- ✅ Document exceptions

**CI/CD:**

- ✅ Integrate scanning into pipeline
- ✅ Fail builds on critical vulnerabilities
- ✅ Generate reports for tracking
- ✅ Notify security team of issues

**Registry:**

- ✅ Enable scan on push
- ✅ Use private registries for sensitive images
- ✅ Implement retention policies
- ✅ Regular scheduled scans
- ✅ Monitor for new vulnerabilities

**Runtime:**

- ✅ Admission controllers to block vulnerable images
- ✅ Continuous monitoring
- ✅ Automated patching where possible
- ✅ Incident response plan

### Key Takeaways

**Registries:**

- Use Docker Hub for public, open-source projects
- Use managed registries (ECR/GCR/ACR) for cloud-native apps
- Use Harbor/Nexus/Artifactory for on-premise enterprise
- Always use private registries for production workloads

**Security:**

- Scan early and often (shift-left security)
- Trivy is excellent for quick, comprehensive scans
- Snyk provides best remediation guidance
- Automate scanning in CI/CD pipelines
- Never deploy images with critical vulnerabilities

**Operations:**

- Implement lifecycle policies to clean old images
- Use replication for high availability
- Monitor registry usage and costs
- Regular backups of registry data
- Document security policies and procedures
